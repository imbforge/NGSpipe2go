---
title: "SHINYREPS_PROJECT"
output:
  html_document:
    toc: true
    toc_float: true
    css: styles.css
---

<div class="contentbox">


# Description

Enter project description here


# Processing

Bpipe.run { 
    "%.fastq.gz" * [ FastQC + FastqScreen +
      (RUN_CUTADAPT ? Cutadapt + FastQC : dontrun.using(module:"Cutadapt")) ] + 
      "%_L*_R*_001.fastq.gz" * [
         cellranger_count + [
            bamCoverage,
            inferexperiment,
            qualimap,
            subread2rnatypes,
            dupRadar,
            geneBodyCov2
        ]
    ] +
    cellranger_aggr +
    (RUN_TRACKHUB ? trackhub_config + trackhub : dontrun.using(module:"trackhub")) +
    collectToolVersions + collectBpipeLogs + shinyReports
}


```{r setup, echo=F, result='hide', error=F, warning=F, message=F}

# source helper functions
source("sc.shinyrep.helpers.R")

# load required packages
not_yet_attached <- attach_packages(c("tidyverse", "AnnotationDbi", "batchelor", "Biobase", "BiocSingular", "bluster", "Cairo", "celldex", 
          "cluster", "corrplot", "dendextend", "DESeq2", "DropletUtils", "dynamicTreeCut", "edgeR", "GeneOverlap", "ggbeeswarm", "ggrepel", 
          "gplots", "grid", "gridExtra", "gtools", "igraph", "kableExtra", "knitr", "limma", "M3Drop", "Matrix", "parallel", 
          "pheatmap", "plotly", "pkgmaker", "RColorBrewer", "reshape2", "rmarkdown", "rtracklayer", "Rtsne","SC3", "scales", 
          "scater", "scDblFinder", "scran", "shinydashboard", "SingleR", "uwot", "VennDiagram", "vipor", "viridis"))

# load global variables
loadGlobalVars(f="shinyReports.txt")

# create folder for output files
if (!file.exists(file.path("report_files"))) {dir.create(file.path("report_files"))}

# set options
options(stringsAsFactors=FALSE)
CORES <- 2
pal   <- brewer.pal(9, "Set1")
pal_rb <- colorRampPalette(c(pal[1], "white", pal[2]))(20)
pal_y  <- colorRampPalette(c("black", "yellow"))(100)

knitr::opts_chunk$set(cache=F,
                      echo=F,
                      warning=F,
                      message=F,
                      dev='CairoPNG')

theme_set(theme_bw() + theme(axis.text=element_text(colour="grey30",size=12),
									axis.title=element_text(colour="grey30",size=14),
									plot.title=element_text(size=14,hjust=0.5),
									plot.subtitle=element_text(size=12,hjust=0.5),
									legend.text=element_text(size=12,colour="grey30"),
									legend.title=element_text(size=12,colour="grey30")))

# load list of mitochondrial genes (if not given, we will use all genes starting with "MT-")
mito.genes <- tryCatch(read.delim(file.path(SHINYREPS_PROJECT,SHINYREPS_MTGENES))[, 1], error=function(e) NA)

# check for selected pipeline modules 
runCutadapt <- SHINYREPS_RUN_CUTADAPT=="true"


## load or create specific targets file for all cells. 
switch(SHINYREPS_SEQTYPE,
    MARSseq={
    # Columns required: "sample", "barcode", "row", "col", "cells", "group", "pool"   
      targets <- read.delim(SHINYREPS_TARGET, sep=",")
      group.vars <- colnames(targets)[!colnames(targets) %in% c("sample", "barcode", "row", "col")] 
      targets[,group.vars] <- lapply(targets[,group.vars], factor)
      # pool-wise targets to annotate plots made before demultiplexing by cell barcode
      targets_pools <- targets[!duplicated(targets$pool),!colnames(targets) %in% c("sample", "barcode", "row", "col", "cells")] 
      targets4plots <- targets_pools 
      colorByFactor <- "pool" # default for pipeline overview plots
      colorByFactor2 <- if(!is.na(SHINYREPS_COLORBYFACTOR)) {SHINYREPS_COLORBYFACTOR} else {"group"} # default for downstream plots (up to 2 categories)
   }, 
    SmartSeq2={
    # Columns required: "sample", "barcode", "row", "col", "cells", "group"
      targets <- read.delim(SHINYREPS_TARGET, sep=",")
      group.vars <- colnames(targets)[!colnames(targets) %in% c("sample", "barcode", "row", "col")] 
      targets[,group.vars] <- lapply(targets[,group.vars], factor)
      targets4plots <- targets
      colorByFactor <- "cells" # default for pipeline overview plots
      colorByFactor2 <-if(!is.na(SHINYREPS_COLORBYFACTOR)) {SHINYREPS_COLORBYFACTOR} else {"group"} # default for downstream plots (up to 2 categories)
    },
    tenX={
      #Columns required: "library_id", "molecule_h5"
      targets_pools <- read.delim(SHINYREPS_TARGET, sep=",")
      matrix_dir = file.path(SHINYREPS_RES, SHINYREPS_CELLRANGERAGGR_ID, "outs/filtered_feature_bc_matrix/") 
      targets = read.delim(paste0(matrix_dir, "barcodes.tsv.gz"), header = FALSE, stringsAsFactors = FALSE)
      names(targets) = "cell_barcode"
      targets <- cbind(targets, targets_pools[as.numeric(gsub("^.*-", "", targets$cell_barcode)), 
                                              !colnames(targets_pools) %in% "molecule_h5"])
      rownames(targets) <- targets$cell_barcode
      group.vars <- colnames(targets)[!colnames(targets) %in% c("cell_barcode", "library_id")] 
      targets4plots <- targets_pools
      colorByFactor <- "sample" # default for pipeline overview plots
      colorByFactor2 <- if(!is.na(SHINYREPS_COLORBYFACTOR)) {SHINYREPS_COLORBYFACTOR} else {"sample"} # default for downstream plots (up to 2 categories)
   },
    stop(c("Don't find seqtype:", SHINYREPS_SEQTYPE))   
)


# print targets file
DT::datatable(targets, caption="targets file (sample sheet) used for analysis")

```

```{r read_annotation}
# load gene annotation provided in essential.vars.groovy
gtf <- import.gff(SHINYREPS_GTF, format="gtf", feature.type="exon")
#gtf.flat <- unlist(reduce(split(gtf, elementMetadata(gtf)$gene_id)))
#gene.lengths <- tapply(width(gtf.flat), names(gtf.flat), sum)
gene.names <- unique(as.data.frame(gtf)[, c("gene_id", "gene_name")])

```


# Quality control on bulk RNA

## FastQC of all reads 

The raw sequence reads of all samples are analysed with the popular FastQC tool (http://www.bioinformatics.babraham.ac.uk/projects/fastqc/).

1. The "Read qualities" Box-Whisker plots show the range of quality values across all base positions:
    (i) The central red line is the median value,
    (ii) The yellow box represents the inter-quartile range (25-75%),
    (iii) The upper and lower whiskers represent the 10% and 90% points,
    (iv) The blue line represents the mean quality.
The y-axis on the graph shows the Phred quality scores, which are logarithmically related to the base-calling error probabilities. The higher the score the better the base call. The background of the graph divides the y axis into very good quality calls (green), calls of reasonable quality (orange), and calls of poor quality (red). Typically, the majority of calls on all base positions fall into the green area.

2. The "Sequence bias" plots show the proportion of each base (% G, A, T and C) at each position. In a random library there would be little difference between the positions of a sequence run, so the lines in this plot should run parallel with each other. But most RNA-seq libraries show sequence imbalance in the first 10-12 read positions due to RT priming biases, which should however look fairly similar in all samples.

3. The "GC Content" plots show the GC% distribution of all reads (red lines) and the ideal normal distribution based on the data (blue lines). Typically, the red and blue lines tightly overlap and look essentially the same in all samples. An unusually shaped distribution could indicate a contaminated library.

```{r FastQC_paragraph1, echo=F, results='asis', error=F, warning=F, message=F}
##### parameters to set:
# Select samples for which you would like to include fastqc results in the report. For single cell RNA-Seq with many cells, 
# you may want to restrict the total number of plots. If you provide a regular expression in 'samplePattern' only those 
# filenames will be included which match this expression, e.g. setting samplePattern="R1" yields only those fastq 
# files containing read1 of a read pair. This is recommended e.g. for MARS-Seq, where Read2 contains barcode information only.
# For samplePattern=NULL (default) all fastq files will be included.
# If you want to exclude samples according to the given certain pattern (instead of including), set exclude=T. 
# This is e.g. for excluding the trimmed fastq files which contain "cutadapt" in the file name (see below). 
# If you set argument 'maxno', the maximum sample number will be restricted accordingly to the first 'maxno' plots. 
#
# NOTE: this should be moved as parameters into shinyreports.vars.groovy
samplePattern=SHINYREPS_SAMPLEPATTERN2 # "_R1\\."
excludePattern=T  # default FALSE
maxno=as.numeric(SHINYREPS_MAXNO)        # default NULL
##### 

cat(DEhelper.Fastqc(web=F, samplePattern=samplePattern, exclude=excludePattern, maxno=maxno), sep="\n")
# These options also apply for the helper functions following below
```


```{r cutadapt, echo=F, results='asis', error=F, warning=F, message=F, eval=runCutadapt}
##### parameters to set:
# define subset of cutadapt log files if desired (samplePattern=NULL (default) includes all files).
# define categories of targets.txt to be used for dot color in the plot (one plot per element of colorByFactor will be created). 
# The function will try to map the data from targets$sample to the cutadapt log file names (for this, the unique part of 
# targets$sample must be a substring of file name). If you have cutadapt log files from pooled fastq files but cell-wise 
# information in targets.txt as in MARS-Seq, you may provide another customized targets object in 'targetsdf'. 
# Otherwise pruned file names will be used by default.
#####

cat("## Adapter trimming with Cutadapt\n")

cat("\nThe following plot shows the amount of reads trimmed for the selected adapter sequences including polyA/polyT sequences if specified. The column 'tooshort' gives the percentage of reads removed due to a length of less than 30 bases after trimming. Additionally to the column 'trimmed' which is for all adapters combined, there are columns for every single adapter giving the percentage of reads trimmed for this adapter.\n")

DEhelper.cutadapt(colorByFactor=colorByFactor, targetsdf=targets4plots)
```

```{r FastQC_paragraph2, echo=F, results='asis', error=F, warning=F, message=F, eval=runCutadapt}
# Applying FastQC again after adapter trimming (fastq file names contain "cutadapt" as part of their name)
samplePattern=SHINYREPS_SAMPLEPATTERN2
excludePattern=F  # default FALSE
maxno=as.numeric(SHINYREPS_MAXNO) # default NULL
##
cat("### FastQC after adapter trimming\n")
cat(DEhelper.Fastqc(web=F, samplePattern=samplePattern, exclude=excludePattern, maxno=maxno), sep="\n") 
```


## Gene body coverage ##

The plots below show meta-gene profiles of 5' to 3' read coverage and can reveal biases due to RNA degradation or specific library prep protocol; polyA-selection protocols are particularly prone of producing 3' coverage bias upon RNA degradation. The grey line depicts the actual read coverage and the red line shows the overall trend.

```{r GeneBodyCoverage_paragraph, echo=F, results='asis', error=F, warning=F, message=F}
cat(DEhelper.geneBodyCov(web=F), sep="\n")
```


```{r echo=F, results='asis', error=F, warning=F, message=F}
cat("## Strand specificity\n")

cat("\nThe table below shows the fraction of reads mapped in sense or antisense to gene exons - around 0.5 for non-stranded library prep protocols and close to 0 or 1 for strand-specific RNA-seq protocols. The strandedness is calculated as the percentage of unambiguous reads that are on the strand, which is expected by the library preparation protocol.\n\n")

cat(DEhelper.strandspecificity(), sep="\n")
```


## Qualimap

Illustrate genomic origin of reads

```{r Qualimap}
DEhelper.Qualimap()
```


## RNA class representation ##

The following plot shows the fraction of reads assigned to various RNA classes. These plots help in determining if the sample prep protocol worked well and may reveal issues with rRNA contamination. 

```{r RNAtypes_paragraph, echo=F, results='asis', error=F, warning=F, message=F}
DEhelper.RNAtypes()
```


# Mapping Statistics

```{r mapping_stats, fig.height=10, fig.width=12, results='asis', message=FALSE, warning=FALSE}
##### parameters to set:
# define subset of Star log files if desired. samplePattern=NULL (default) includes all files.
# define categories of targets.txt to be used for dot color in the summary plot (one plot per element of colorByFactor will be created). 
# The function will try to map the data from targets$sample to the cutadapt log file names (for this, the unique part of 
# targets$sample must be a substring of file name). If you have cutadapt log files from pooled fastq files but cell-wise 
# information in targets.txt as in MARS-Seq, you may provide another customized targets object in 'targetsdf'. 
# Otherwise pruned file names will be used by default.
#####
if (SHINYREPS_SEQTYPE=="tenX") {
  cat("\nMapping Statistics can be found in the cellranger web summary.\n")
} else {
  
  cat("\nMapping to the reference genome & transcriptome is performed with STAR (https://github.com/alexdobin/STAR). The program version, genome assembly and software parameters are described in the table at the end of the report.

The mapping statistics below show the number and percentage of: (i) input raw reads, (ii) uniquely mapped reads, (iii) multi-mapped reads aligning equally well to multiple (up to 20) positions in the genome, (iv) reads that align to too many (>20) genome loci are discarded, (v) unmapped reads. For the multimapped reads, one random alignment among the best mapping positions is retained.\n")
  
  DEhelper.STAR(targetsdf=targets4plots, colorByFactor=colorByFactor)
}
```



```{r umicount, echo=F, results='asis', error=F, warning=F, message=F, eval= (SHINYREPS_SEQTYPE=="MARSseq")}
##### parameters to set:
# define subset of umicount log files if desired (samplePattern=NULL (default) includes all files).
# define categories of targets.txt to be used for dot color in the plot (one plot per element of colorByFactor will be created). 
# The function will try to map the data from targets$sample to the umicount log file names (for this, the unique part of 
# targets$sample must be a substring of the file name). If you have umi-tools log files from pooled fastq files but cell-wise 
# information in targets.txt as in MARS-Seq, you may provide another customized targets object in 'targetsdf'. 
# Otherwise pruned file names will be used by default.
#####

cat("## Read de-duplication and feature counting with UMI-tools

UMI-tools counts the mapped reads per feature obtained after de-duplication. UMI-tools uses the uniquely mapped reads from STAR as well as as the multimapped reads (but not those which are mapped to too many loci) as input reads.")

DEhelper.umicount(colorByFactor=colorByFactor, targetsdf=targets4plots)
```


## Competetive mapping to rRNAs and other contaminats:

A competitive mapping to the reference genome and known rRNAs (H.sapiens, M.musculus, C.elegans, D.melanogaster, D.rerio, X.tropicalis) as well as frequently occuring Mycoplasma species (M. arginini, M. fermentans, M. hominis, M. hyorhinis,M. orale, and Acholeplasma laidlawii), E.coli and Bos taurus was performed with FastQScreen (http://www.bioinformatics.babraham.ac.uk/projects/fastq_screen/).

In the following plot the "one_genome" label refers to the amount of reads mapping to that genome only regardlessly if they are multi mapping or uniquely mapping reads.
The "multiple_genome" label refers to the amount of read mapping to this genome but also to other genomes in the screening regardless if they are multi mapping on the genome or uniquely mapping to that genome.
Since the reads mapping to multiple genomes are referred to for each individual genome the total percentages of reads mapping can be larger than a 100%. 

```{r fastqscreen_paragraph, echo=F, results='asis', fig.width=10, error=F, warning=F, message=F}
plot(DEhelper.fastqscreen())
     
```



## UCSC Genome Browser tracks ##

The browser tracks are generated using bamCoverage tool in [deepTools](https://deeptools.readthedocs.io/en/develop/) and they are normalized using the Counts Per Million mapped reads (CPM) method. Both unique and multi-mapped reads are included in the generation of browser coverage tracks.

To view the tracks in the UCSC Genome Browser, navigate to the [UCSC Genome Browser Track Hub page](https://genome-euro.ucsc.edu/cgi-bin/hgHubConnect) and enter the following line into the `URL` field:

```{r Trackhub, echo=F, results='asis', error=F, warning=F, message=F}
cat(DEhelper.Trackhub(), sep="\n")
```


## Library complexity assessment ##

Measuring the fraction of duplicated reads is commonly used to control for excessive PCR over-amplification during library preparation and/or for suboptimal amount of input material. However, in RNA-seq duplicated reads may also arise naturally due to the highly expressed (e.g. housekeeping) genes, which makes the overall read duplication rate useless. The IMB-developed tool dupRadar (http://bioconductor.org/packages/release/bioc/html/dupRadar.html) gives an informative insight into the duplication problem by graphically relating the gene expression levels and the observed duplication rates. Thus, problematic samples can be easily identified by visual inspection.

```{r Dupradar_paragraph, echo=F, results='asis', error=F, warning=F, message=F}
cat(DEhelper.dupRadar(web=F), sep="\n")
```




# Low-level analysis of single-cell RNAseq data

The analysis workflow is based on the Bioconductor packages *scater* and *scran* as well as the Bioconductor workflows by Lun ATL, McCarthy DJ, & Marioni JC *A step-by-step workflow for low-level analysis of single-cell RNA-seq data.* F1000Res. 2016 Aug 31 [revised 2016 Oct 31];5:2122 and Amezquita RA, Lun ATL et al. *Orchestrating Single-Cell Analysis with Bioconductor* Nat Methods. 2020 Feb;17(2):137-145.

```{r loading_counts, echo=F, message=FALSE, error=TRUE, warning=TRUE}

## load counts for MARSseq design (pooled count files) 
switch(SHINYREPS_SEQTYPE,
    MARSseq={
        # load counts
        f <- list.files(paste0(SHINYREPS_UMICOUNT), pattern="\\.tsv$", full.names=TRUE)
        counts <- mclapply(f, read.delim, head=T, row.names=1, mc.cores=CORES)
        names(counts) <- basename(f)

        # pool-wise replacement of barcodes by sample names. A pool consists of those cells which are amplified together 
        # in one pool (i.e. all reads have identical pool barcode) but which all have different unique cell barcodes.
        # The cell barcodes are re-used in other pools. For assignment of cell barcodes given in 'targets.txt' to the 
        # pooled count data, the pool ID given in 'targets.txt' must be a substring of the respective count data filename
        # (no matter at which position).
        for (i in unique(targets$pool)) {
            if(sum(grepl(i, names(counts), ignore.case = T)) !=1) {stop("\ncannot unambiguously assign pool ID given in targets.txt to count filename")}
            current_pool <- targets[targets$pool ==i, ]
            countname <- grep(i, names(counts), ignore.case = T, value=T )
            colnames(counts[[countname]]) <- current_pool[match(colnames(counts[[countname]]), current_pool$barcode), "sample"]
        }
        
        # merge pools into one dataset with union of genelists
        custom.merge <- function(x,y) { # custom merge function for Reduce
            z <- merge(x,y, all=T, by="row.names")
            rownames(z) <- z$Row.names
            return(z[,-1])
        }
        counts <- Reduce(custom.merge, counts)
        
        counts[is.na(counts)] <- 0 # set NAs in the dataset to zero counts
        targets <- targets[targets$sample %in% colnames(counts),] # adjust targets in case of cells have been lost during processing
        if (!all(colnames(counts) %in% targets$sample)) {stop("column names of count matrix and entries in targets$sample do not match!")}
        counts <- counts[,match(targets$sample, colnames(counts))] # sort counts according to target
        # table(colnames(counts) == targets$sample) # all TRUE
    },
    SmartSeq2={
      # load counts
        f <- list.files(paste0(SHINYREPS_SUBREAD), pattern="\\.readcounts\\.tsv$", full.names=TRUE) # don't select raw_readcounts.tsv
        f <- f[!grepl("Undetermined", f)] # don't use count file with unmapped reads if present
        
        counts <- mclapply(f, read.delim, header=F, row.names=1, comment.char = "#")
        stopifnot(all(sapply(counts, function(x) all(rownames(x) == rownames(counts[[1]])))))
        counts <- do.call(cbind, counts)
        colnames(counts) <- basename(f)

      # align with targets.txt
        if(any(duplicated(targets$sample))) { # check for duplicated entries in targets.txt
          stop("Duplicated entries in targets.txt: ", paste(targets$sample[duplicated(targets$sample)], collapse=", "))
        }
     
        logindex_targetsfile <- sapply(targets$sample, grep,  colnames(counts)) # grep targets in counts
        targets <- targets[sapply(logindex_targetsfile, length) ==1, ] # remove targets entries not (uniquely) found in counts

       if(!identical(sort(unname(unlist(logindex_targetsfile))), 1:length(colnames(counts)))) { 
         # check if all counts samples are contained in targets.txt
         stop("Sample names not included in targets.txt: ", paste(colnames(counts)[-logindex_targetsfile], collapse=", "))
       }
       counts <- counts[,unlist(logindex_targetsfile)] # cols in same order as rows in targets   
       colnames(counts) <- targets$sample
    },
    tenX={
      # matrix_dir already defined above 
      barcode.path <- paste0(matrix_dir, "barcodes.tsv.gz")
      features.path <- paste0(matrix_dir, "features.tsv.gz")
      matrix.path <- paste0(matrix_dir, "matrix.mtx.gz")
      counts <- readMM(file = matrix.path)
      colnames(counts) = targets$cell_barcode
      feature.names = read.delim(features.path, header = FALSE, stringsAsFactors = FALSE)
      rownames(counts) = feature.names$V1
    },
    stop(c("Don't find seqtype:", SHINYREPS_SEQTYPE))   
)


#### create SingleCellExperiment object
sce <- SingleCellExperiment(assays=list(counts=as.matrix(counts)), colData = targets)

# add gene symbols from annotation file to SingleCellExperiment object
rowData(sce)$ENSEMBL <- rownames(sce)
rowData(sce)$SYMBOL  <- gtf$gene_name[match(rownames(sce), gtf$gene_id)]
rowData(sce)$CHR  <- as.character(seqnames(gtf))[match(rownames(sce), gtf$gene_id)]

# get rid of NAs and duplicated names (missing SYMBOLS are replaced with ENSEMBL IDs)
new.names <- rowData(sce)$SYMBOL
missing.name <- is.na(rowData(sce)$SYMBOL)
new.names[missing.name] <- rowData(sce)$ENSEMBL[missing.name]

# duplicated names are extended with ENSEMBL IDs
dup.name <- new.names %in% new.names[duplicated(new.names)]
new.names[dup.name] <- paste0(new.names, "_", rowData(sce)$ENSEMBL)[dup.name]
rowData(sce)$SYMBOL <- new.names

# add gene length information (not needed)
# rowData(sce)$genelength <- gene.lengths[ match(rownames(sce),names(gene.lengths)) ]

```


```{r droplets_emptyDrops, echo=F, message=FALSE, results='asis', eval=F}

## ATTENTION: CellRanger version 3 automatically performs cell calling using an algorithm similar to emptyDrops().
# Since we load the FILTERED count matrix, we can go straight to computing other QC metrics and do
# not need to run this code chunk.
# However, sometimes it may be desirable to load the UNFILTERED matrix and apply emptyDrops() ourselves, 
# on occasions where more detailed inspection or control of the cell-calling statistics is desired.

cat("## Detect empty droplets\n\n")

cat("\nAn unique aspect of droplet-based data is that we have no prior knowledge about whether a particular library (i.e., cell barcode) corresponds to cell-containing or empty droplets. Thus, we need to call cells from empty droplets based on the observed expression profiles. This is not entirely straightforward as empty droplets can contain ambient (i.e., extracellular) RNA that can be captured and sequenced, resulting in non-zero counts for libraries that do not contain any cell.\n")

cat("\nknee plot: Total UMI count for each barcode in the PBMC dataset, plotted against its rank (in decreasing order of total counts). The inferred locations of the inflection and knee points are also shown.")

bcrank <- barcodeRanks(counts(sce))
# Only showing unique points for plotting speed.
uniq <- !duplicated(bcrank$rank)
plot(bcrank$rank[uniq], bcrank$total[uniq], log="xy",
    xlab="Rank", ylab="Total UMI count", cex.lab=1.2)

abline(h=metadata(bcrank)$inflection, col="darkgreen", lty=2)
abline(h=metadata(bcrank)$knee, col="dodgerblue", lty=2)

legend("bottomleft", legend=c("Inflection", "Knee"), 
        col=c("darkgreen", "dodgerblue"), lty=2, cex=1.2)

cat("\nTesting for empty droplets\n")
set.seed(100)
e.out <- emptyDrops(counts(sce))
summary(e.out$FDR <= 0.001)
table(Sig=e.out$FDR <= 0.001, Limited=e.out$Limited)

cat("\nemptyDrops() assumes that barcodes with low total UMI counts are empty droplets. Thus, the null hypothesis should be true for all of these barcodes. We can check whether the hypothesis testing procedure holds its size by examining the distribution of p-values for low-total barcodes with test.ambient=TRUE\n")

set.seed(100)
limit <- 100   
all.out <- emptyDrops(counts(sce), lower=limit, test.ambient=TRUE)
hist(all.out$PValue[all.out$Total <= limit & all.out$Total > 0],
    xlab="P-value", main="", col="grey80") 

cat("\nIdeally, the distribution should be close to uniform. Large peaks near zero indicate that barcodes with total counts below lower are not all ambient in origin. This can be resolved by decreasing lower further to ensure that barcodes corresponding to droplets with very small cells are not used to estimate the ambient profile. Once we are satisfied with the performance of emptyDrops(), we subset our SingleCellExperiment 
# object to retain only the detected cells.\n")

sce <- sce[,which(e.out$FDR <= 0.001)]

# While emptyDrops() will distinguish cells from empty droplets, it makes no statement about the quality of the cells.
# Filtering on the mitochondrial proportion provides is still necessary.
# Since emptyDrops() already removes cells with very low library sizes or (by association) low numbers of expressed genes, 
# further filtering on these metrics is not strictly necessary. It may still be desirable to filter on both of these metrics
# to remove non-empty droplets containing cell fragments or stripped nuclei that were not caught by the mitochondrial filter. 
# However, this should be weighed against the risk of losing genuine cell types.

# For routine analyses, there is usually no need to remove the ambient contamination from each library.

```



## Quality control of cells and RNA sequenced

To assess if the sequenced libraries are usable and the RNA captured represents a meaningful fraction of the RNA present in the cell, we are focussing on the following factors:

* **Library size:** for cells with small library size the RNA was not efficiently captured.
* **Number of expressed genes:** few expressed genes suggest that a diverse transcript population was not captured.
* **Proportion of reads mapping to mitochondrial genes:** high proportion mean increased apoptosis and/or loss of cytoplasmatic RNA from lysed cells.

To remove outliers, these criteria may be filtered either for relative thresholds using the median absolute deviation (MAD) or by setting absolute threshold after inspecting the quality control plots.

```{r quality_plots_of_cells, results='asis'}
##### parameters to set:
# define set of categories used to annotate cells in the plots (e.g.: annoFactors <- c("group", "pool"))
annoFactors <- colorByFactor2
#####

# define mitochondrial genes as control features
if (!is.na(mito.genes)) {
 is.mito  <- row.names(sce) %in% mito.genes # use predefined list with mitochondrial genes
 cat("\n", sum(is.mito), "mitochondrial genes identified by pre-defined mitochondrial gene list.\n")
 } else {
 is.mito <- rowData(sce)$CHR %in% c("chrM", "chrMT", "M", "MT")   
 cat("\n", sum(is.mito), "genes identified on mitochrondrial chromosome.\n")
 #is.mito <- grepl("^MT-", rowData(sce)$SYMBOL, ignore.case = TRUE) # use all genes starting with "MT-"
 #cat("\n", sum(is.mito), "mitochondrial genes identified by gene name starting with 'MT-'.\n")
 }

# calculate QC metrics
sce <- addPerCellQC(sce, percent_top =2, subsets=list(Mito=is.mito))  


qc.frame <- colData(sce)
qc.frame$Lib.size <- qc.frame$sum/1e3 # library size in thousands
qc.frame <- as.data.frame(qc.frame)

# generate plots for Lib.size, total_features, pct_amount_Mt for each selected category 
# histograms
qc.plots <- lapply(c("Lib.size", "detected", "subsets_Mito_percent"), function(to.plot){ 
      if(to.plot=="Lib.size"){
        xlabel="Lib. sizes in thousands"
         }else{
           xlabel=to.plot}
  
      if(to.plot=="subsets_Mito_percent"){
        madOrient="high"
      }else{
        madOrient="low"}

  plot.list <- lapply(annoFactors, function(separation){
      p <- ggplot(qc.frame, aes_string(to.plot, fill=separation))+ 
            geom_histogram(position="identity", alpha=0.5, bins=100, col="grey80") +
            geom_vline(aes(xintercept = median(na.omit(qc.frame[,to.plot])), col="median"), linetype=2)  +
            geom_vline(aes(xintercept = MAD(na.omit(qc.frame[,to.plot]), as.numeric(SHINYREPS_NMADS))[[madOrient]], col=madOrient), linetype=2) +
            #scale_color_manual(name = "Statistics", values = c(median = "blue", high="red")) +
            scale_fill_brewer(palette = "Set1") +
            #scale_x_log10() +
            xlab(xlabel) +
            ylab(paste0("# cells"))
          plot(p)
          return(p)
  })
  return(plot.list)
})

# violinplots
qc.plots.violin <- lapply(c("Lib.size", "detected", "subsets_Mito_percent"), function(to.plot){ 
  if(to.plot=="Lib.size"){
    ylabel <- "Lib. sizes in thousands"
  }else{
    ylabel <- to.plot
  }
  
  plot.list <- lapply(annoFactors, function(separation){
    p <- ggplot(qc.frame, aes_string(separation,to.plot,color=separation))+
      geom_violin() +
      geom_quasirandom() +
      scale_fill_hue(l=40, c=40) +
      ylab(ylabel) +
      xlab(separation) +
      theme(axis.text.x=element_text(angle=45, vjust=1, hjust=1))
    plot(p)
    return(p)
  })
  return(plot.list)
})

```



**Top 2% biggest libraries (based on mapped reads on features)**

```{r top_1perc_cells}
# Output the cells with a library size in the top 2%
highest.lib.size <- colData(sce)[sce$sum > quantile(sce$sum, 0.98),
              c("sum",
               "detected", 
               "subsets_Mito_percent",
               group.vars)]
highest.lib.size <- highest.lib.size[order(highest.lib.size$sum, decreasing = T),]

DT::datatable(as.data.frame(highest.lib.size), caption="2% cells with the highest library size")
```


```{r total_counts_per_plate_position, fig.width=8, results='asis', eval= (SHINYREPS_SEQTYPE %in% c("MARSseq", "SmartSeq2"))}
##### parameters to set:
# define one category to be used for symbol shape in the plots.
annoFactors <- colorByFactor2
#####

cat("## Count distribution per plate position

The plots below visualize the count data distribution (not log-transformed) via plate position for the categories 'total_features_by_counts_endogenous', 'total_counts' and 'pct_counts_Mt' indicated by spot size. 0-cell and 10-cell controls are indicated in blue and orange, respectively. Symbol shape is defined by custom grouping. For each category the plates are plotted row-wise with plate number in increasing order.\n")

sce$plate_position <- paste0(sce$row, sce$col) # column "plate_position" needed for plotPlatePosition
for (size in c("Lib.size", "detected", "subsets_Mito_percent")) {
  cat(paste("Plotting", size, "as spot size\n"))
  plates <- list()
  for (p in as.character(sort(unique(sce$plate)))) {
        plates[[p]] <- scater::plotPlatePosition(sce[, sce$plate==p], colour_by="cells",size_by=size, shape_by=annoFactors[1],
                            by_exprs_values = "counts", theme_size = 10,  point_alpha = 0.6, point_size = 3, add_legend = F)  
  } # plotPlatePosition uses by_exprs_values = "logcounts" by default. But if not available, uses "counts" instead

  multiplot(plotlist=plates, layout=matrix(c(1:ceiling(length(plates))), ncol=2, byrow=TRUE))
}

```


### Correlation plots for different features

```{r corr_plots}
##### parameters to set:
# define one category to be used for symbol color in the plots.
annoFactors <- colorByFactor2
#####

if(!is.null(annoFactors)) {annoFactors <- rlang::ensym(annoFactors)} 

lib.size.scatter <- ggplot(qc.frame, aes(x=detected, 
                                        y=Lib.size,
                                        color=!!annoFactors)) +
                             geom_point() +
                             scale_color_hue(l=40, c=60) +
                             ylab("Lib. sizes in thousands") +
                             xlab("Number of expressed genes")

mit.perc.scatter <- lib.size.scatter + aes(y=subsets_Mito_percent) + ylab("% mitochondrial reads")
grid.arrange(lib.size.scatter,  mit.perc.scatter)

```

## Filtering out low quality cells

Filter criteria (see below) are selected according to quality control plots.

Summary of cells that don't pass the QC:

```{r dropout_cells, results='asis', fig.height=8, fig.width=10}
##### parameter to set:
# define filtering criteria based on previous QC plots
type_of_threshold = SHINYREPS_TYPE_OF_THRESHOLD # either "absolute" or "relative" (i.e. using MAD)
threshold_total_counts_min = as.numeric(SHINYREPS_THRESHOLD_TOTAL_COUNTS_MIN)
threshold_total_counts_max = as.numeric(SHINYREPS_THRESHOLD_TOTAL_COUNTS_MAX)
threshold_total_detected = as.numeric(SHINYREPS_THRESHOLD_TOTAL_FEATURES_DETECTED)
threshold_pct_counts_Mt = as.numeric(SHINYREPS_THRESHOLD_PCT_COUNTS_MT)
NMADS = as.numeric(SHINYREPS_NMADS) # number of absolute deviations from median. Only relevant if type_of_threshold = "relative".
apply_QCfilter_by_factor = if(gtools::invalid(SHINYREPS_APPLY_QCFILTER_BY_FACTOR) || toupper(SHINYREPS_APPLY_QCFILTER_BY_FACTOR) == "NULL") {
  NULL} else {colData(sce)[,SHINYREPS_APPLY_QCFILTER_BY_FACTOR]}  # apply relative threshold separated by factor
# define QC metrics to be used for PCA
qcmetrics = c("sum", "detected", "subsets_Mito_percent")
# define one category to be used for symbol shape in the plot.
annoFactors <- colorByFactor2
#####

# if(!exists("sce.beforeFilt")) {sce.beforeFilt <- sce}

if(type_of_threshold=="relative") {
  # Before we apply relative QC filters we remove all the cells which hardly have any counts to avoid bias for relative thresholds
  min_readcount <- 100
  count.drop <- sce$sum < min_readcount
  cat("We drop ", sum(count.drop), " cells, because they have less than", min_readcount, "reads counted.\n") 
  sce <- sce[,!count.drop]
}

# apply thresholds
qc.drop <-  data.frame(row.names = colnames(sce)) # initialize

if(type_of_threshold=="absolute") {
  qc.drop$libsize <- (sce$sum < threshold_total_counts_min) | (sce$sum > min(threshold_total_counts_max, max(sce$sum), na.rm=T))
  qc.drop$detected <- sce$detected < threshold_total_detected
  qc.drop$mito <- sce$subsets_Mito_percent > threshold_pct_counts_Mt
  qc.drop$pass <- !apply(qc.drop, 1, any)
  qc.drop <- qc.drop[match(colnames(sce), rownames(qc.drop)), ]  # sort to match the 'sce' cell order

  if(!gtools::invalid(threshold_total_counts_max)) {
    threshold_total_counts_max <- paste("or >", threshold_total_counts_max)
        } else {threshold_total_counts_max <- NULL}   # prepare string for output
  
  qcfailed <- data.frame(criterion=c(paste("total counts <", threshold_total_counts_min, threshold_total_counts_max),
                                     paste("total features <", threshold_total_detected),
                                     paste("% mitochondrial counts >", threshold_pct_counts_Mt),
                                     paste("remaining")), 
                         cell_count=c(sum(qc.drop$libsize), sum(qc.drop$detected), sum(qc.drop$mito), sum(qc.drop$pass)))
  kbl(qcfailed, caption="QC filtering") %>% kable_styling()
  
  } else {
    if(type_of_threshold=="relative") {
     qc.drop$libsize <- isOutlier(sce$sum, nmads=NMADS, type="lower", log=TRUE, batch=apply_QCfilter_by_factor)
     qc.drop$detected <- isOutlier(sce$detected, nmads=NMADS, type="lower", log=TRUE, batch=apply_QCfilter_by_factor) 
     qc.drop$mito <- isOutlier(sce$subsets_Mito_percent, nmads=NMADS, type="higher", batch=apply_QCfilter_by_factor)
     qc.drop$pass <- !apply(qc.drop, 1, any)
     qc.drop <- qc.drop[match(colnames(sce), rownames(qc.drop)), ]  # sort to match the 'sce' cell order
     
     qcfailed <- data.frame(criterion=c(paste("total counts <", NMADS, "MAD"),
                                     paste("total features <", NMADS, "MAD"),
                                     paste("% mitochondrial counts >", NMADS, "MAD"),
                                     paste("remaining")), 
                         cell_count=c(sum(qc.drop$libsize), sum(qc.drop$detected), sum(qc.drop$mito), sum(qc.drop$pass)))
     kbl(qcfailed, caption="QC filtering") %>% kable_styling()

  } else {stop("\ntype_of_threshold must be either 'absolute' or 'relative'")}
}


cat("\nVioline plots with applied thresholds indicated\n")
qc.drop.mod <- qc.drop[,!colnames(qc.drop) %in% c("pass")]
qc.drop.mod <- sapply(qc.drop.mod, function(x) ifelse(x==TRUE, "removed", "kept"))

# plot violin plots with indicated thresholds
qc.plots.violin.thresholds <- lapply(qcmetrics, function(to.plot){ 
  color_threshold <- switch(to.plot, "sum" = "libsize", "detected" = "detected", "subsets_Mito_percent" = "mito")
  plotColData(sce, x=annoFactors[1], y=to.plot, colour_by=I(qc.drop.mod[,color_threshold])) + 
    theme(legend.position = "top", legend.text = element_text(size = 14))  
})

  gridExtra::grid.arrange(grobs=qc.plots.violin.thresholds, ncol=3)


# plot QC metrics as PCA
cat("\nThe quality metrics are summarized in a PCA\n")
plotPCAfromQCmetrics(sce, metrics=qcmetrics, anno=annoFactors, qc.drop=qc.drop)


```




Removed samples/cells:

```{r remove_low_qual_cells_and_control_wells, results='asis'}
##### parameters to set:
# define up to 2 categories to be used for output tables.
annoFactors <- colorByFactor2
#####

# output overview of remaining cells and table of removed cells
j <- c("sum", "detected", "subsets_Mito_percent", annoFactors) 
x <- cbind(qc.drop, colData(sce)[match(rownames(qc.drop), rownames(colData(sce))), j])
# kbl(as.data.frame(apply(qc.drop, 2, sum)), col.names="cells", caption="")
x$subsets_Mito_percent   <- round(x$subsets_Mito_percent, digits=2) 
x <- as.data.frame(x)
DT::datatable(x[!x$pass, ])

# filter out cells failing QC
if(length(qc.drop$pass) == ncol(sce)) { # if block is executed multiple times
    sce <- sce[, qc.drop$pass]
}

# overview of remaining cells after QC filtering
colnames_table <- if(length(annoFactors)==1) {c(annoFactors, "cell count")} else {NA}
kbl(table(colData(sce)[, annoFactors, drop=F]), col.names=colnames_table, caption="Amount of cells remaining after QC filtering") %>% kable_styling()

# filter out control wells with 0 or 10 cells
if("cells" %in% colnames(colData(sce))) {
sce <- sce[, colData(sce)$cells == "1c"]
sce$wells <- gsub("_S[0-9]+$", "", gsub("_1c", "", colnames(sce)))

# overview of remaining cells after removing control wells 
kbl(table(colData(sce)[, annoFactors, drop=F]), col.names=colnames_table, caption="Amount of cells remaining after filtering for control wells (0c/10c)") %>% kable_styling() 
}
```


### Replotting the PCA after filtering

```{r pca2, fig.height=8, fig.width=10}
##### parameters to set:
# define up to 2 categories to be indicated in the PCA plot by color and shape.
annoFactors <- colorByFactor2
#####

try(plotPCAfromQCmetrics(sce, qcmetrics, anno=annoFactors))
```



## Top most highly expressed genes

The plot below shows the most highly expressed genes (based on un-normalized mean counts). This should generally be dominated by constitutively expressed transcripts, such as those for ribosomal or mitochondrial proteins. All mitochondrial genes are marked as "Feature control". The color represents the total number of expressed genes in the respective sample/cell.

```{r highly_expr_genes, fig.height=6, fig.width=6}
# use tmp.sce for plotting with different rownames
tmp.sce <- sce
rownames(tmp.sce) <- rowData(sce)$SYMBOL
fontsize <- theme(axis.text=element_text(size=6), axis.title=element_text(size=10))
plotHighestExprs(tmp.sce, n=50) + fontsize
rm(tmp.sce)
```


## Filtering out low abundance genes

Average counts per gene should correlate with number of cells expressing it (see plot below). 
Low abundance genes are likely to be dominated by drop-out events (Poisson noise in different cells). They do not contain enough information for statistical inference, and may compromise accuracy of continuous approximations when fitting the data (edgeR biological coefficient of variation (BCV) estimation). Here, we filter out genes with no expression in 95% of the cells.

```{r remove_low_abundance_genes, results='asis'}
rowData(sce)$ave.count <- calculateAverage(sce) 
expressed.cells <- nexprs(sce, byrow=TRUE)   # number of cells expressing the gene
smoothScatter(log10(rowData(sce)$ave.count), expressed.cells,
              xlab=expression("Log10 average count"), ylab= "Number of expressing cells")
# is.ercc <- isSpike(sce, type="ERCC")
# points(log10(ave.counts[is.ercc]), numcells[is.ercc], col="red", pch=16, cex=0.5)

# filtering for low abundance genes
genes2keep <- expressed.cells > ceiling(.05 * ncol(sce))
cat(sum(genes2keep), "of", length(genes2keep), "genes remain after filtering for low abundance genes.\n")
sce <- sce[genes2keep, ]

# # remove feature controls
# cat("Remove", sum(rowData(sce)$is_feature_control), "feature controls from dataset.\n")
# sce <- sce[!rowData(sce)$is_feature_control,]

```



# Normalization of cell-specific biases
Normalization is required to eliminate these cell-specific differences in capture efficiency, prior to downstream quantitative analyses.

## Normalization for library size

Size factors can be calculated with DESeq2 or edgeR, but these methods do not work well with single-cell data due to the dominance of low and zero counts. To overcome this, we use the method from Lun et al. (2016) implemented in the *cran* package, which pools counts from many cells to estimate the size factors and to finally deconvolute them to cell-specific factors. Finally, normalized log2-expression values are computed for each endogenous gene using the appropriate size factors.

```{r size_factors_and_normalize, warning=FALSE, message=FALSE, results='asis'}
##### parameters to set:
# define up to 2 categories to be indicated in the plot by color and shape.
annoFactors <- colorByFactor2
#####

cat("\nSummary size factors by deconvolution")
set.seed(100)
qclust <- quickCluster(sce) 
sce <- computeSumFactors(sce, clusters=qclust)
#lib.sf <- librarySizeFactors(sce)
summary(sizeFactors(sce))

# cat("\nSummary size factors for ERCC spike-ins\n")
# sce <- computeSpikeFactors(sce, spikes="ERCC") 
# summary(sizeFactors(sce, "ERCC"))

# plot size factors 
dotcol <- annoFactors[1]
dotcol <- rlang::ensym(dotcol)
if(length(annoFactors)>=2) { 
  dotshape <- annoFactors[2]
  dotshape <- rlang::ensym(dotshape)
} else {
  dotshape <- NULL
}

to.plot <- data.frame(sizeFactors=sizeFactors(sce),
                 total_counts=sce$sum/1e6,
                 colData(sce)[,annoFactors, drop=F])
ggplot(to.plot, aes(x=sizeFactors, 
                    y=total_counts,
                    color=!!dotcol, shape=!!dotshape))+
  geom_point()+ ylab("Library size (millions)") + scale_color_hue(l=40, c=40)

# normalise
sce <- scater::logNormCounts(sce) # adds normalized logcounts matrix
```



```{r plot_violin_top50, fig.width=12, fig.height=10, results='asis'}
##### parameters to set:
# define up to 2 categories to be indicated in the plot by color and shape.
annoFactors <- colorByFactor2
#####

cat("\n")
cat("### Normalized log2-expression of top50 genes with highest average expression\n")

# use tmp.sce to change rownames to SYMBOL for the plot
tmp.sce <- sce
rownames(tmp.sce) <- rowData(sce)$SYMBOL

# re-calculate averages, using newly determined size factors
ave.counts.new <- calculateAverage(tmp.sce)
rowData(tmp.sce)$ave.count.new <- ave.counts.new

top.sce.aver.size <- head(rowData(tmp.sce)[order(rowData(tmp.sce)$ave.count.new,decreasing=TRUE),],50)

dotcolour <- annoFactors[1]
if(length(annoFactors)>=2) { 
  dotshape <- annoFactors[2]
} else {
  dotshape <- NULL
}

scater::plotExpression(tmp.sce, features=top.sce.aver.size$SYMBOL[1:25], colour_by=dotcolour, shape_by = dotshape) + ggtitle("highest avg. expression 1:25")
scater::plotExpression(tmp.sce, features=top.sce.aver.size$SYMBOL[26:50], colour_by=dotcolour, shape_by = dotshape) + ggtitle("highest avg. expression 26:50")
```


## Checking for confounding factors 

### Classification and normalization of cell cycle phase

We use the prediction method described by Scialdone et al. (2015) to classify cells into cell cycle phases based on the gene expression data. Pre-trained classifiers are available in scran for human and mouse data. Cells are classified as being in G1 phase if the G1 score is above 0.5 and greater than the G2/M score, in G2/M phase if the G2/M score is above 0.5 and greater than the G1 score, and in S phase if neither score is above 0.5.

Remark for adjustment: in routine scRNA-seq analyses it is not recommended to adjust for cell cycle. Compared to other differences between cell types it is a minor factor of variation and any attempt at removal would also need to assume that the cell cycle effect is orthogonal to other biological processes. For example, regression would potentially remove interesting signal if cell cycle activity varied across clusters or conditions. Cell cycle adjustment may be performed on an as-needed basis in populations with clear cell cycle effects.

```{r find_cell_cycle, results='asis', fig.width=8, fig.height=8}
##### parameters to set:
# define organism (either "human" or "mouse")
org <- SHINYREPS_ORG
# define one category to be used for symbol color in the plot.
annoFactors <- colorByFactor2
#####

if(!exists("sce.no.ccp")) {sce.no.ccp <- sce}
set.seed(100)

# load gene.pairs: "mouse_cycle_markers.rds" or "human_cycle_markers.rds"
gene.pairs <- readRDS(system.file("exdata", paste0(org, "_cycle_markers.rds"), package="scran")) # 

# determine cell cycle phase:
# in "human_cycle_markers.rds" ensembl names are without the ".number" at the very end, but 
# the rownames of our sce object include this. ---> remove .number
assignments <- cyclone(sce, pairs=gene.pairs, gene.names=sub("\\..*$","",rowData(sce)$ENSEMBL), assay.type="logcounts") 
sce$phases <- assignments$phases
cat(sum(is.na(sce$phases)), "out of", length(sce$phases),
    "cells could not be assigned to a cell cycle phase.", fill=TRUE)
if(any(!is.na(sce$phases))) {
    print(kbl(table(sce$phases), col.names=c("phases", "freq"), caption="Cell cycle phases", align = "l", format="simple") %>% kable_styling(full_width = F))
    cat("\n")
    plot(0, xlim=c(0, 1), ylim=c(0, 1), type="n", xlab="G1 score", ylab="G2/M score")
    points(assignments$scores$G1, assignments$scores$G2M, col=scales::alpha(pal[factor(colData(sce)[,annoFactors])], .5) )   # pch=16
    # abline(h=.5, v=.5, lty=2, col="red")
    arrows(0.5,0.5,1,1, length=0, col="red", lty=2)
    arrows(0,0.5,0.5,0.5, length=0, col="red", lty=2)
    arrows(0.5,0,0.5,0.5, length=0, col="red", lty=2)

    text(x=c(.25, .75, .25, .75), y=c(.75, .75, .25, .25), labels=c("G2", " ", "S", "G1"))
    legend("bottomleft", col=c(pal[1:4]), pch=16, legend=c(levels(factor(colData(sce)[,annoFactors]))))
}

## Info: adjustment for cell cycle phases (https://osca.bioconductor.org/cell-cycle-assignment.html)
  # In routine scRNA-seq analyses it is not recommended to adjust for cell cycle. Compared to other differences 
  # between cell types it is a minor factor of variation and any attempt at removal would also need to assume 
  # that the cell cycle effect is orthogonal to other biological processes. For example, regression would 
  # potentially remove interesting signal if cell cycle activity varied across clusters or conditions. 
  # Cell cycle adjustment may be performed on an as-needed basis in populations with clear cell cycle effects.
```




### Identify explanatory variables 

We check whether there are technical factors that contribute substantially to the heterogeneity of gene expression. If so, the factor may need to be regressed out to ensure that it does not inflate the variances or introduce spurious correlations.

```{r explanatorx_variables, results='asis', echo=F, error=F, warning=F, message=F}
##### parameters to set:
# define potential explanatory variables to be tested for their effect
explanatoryVariables <- c(group.vars, if("phases" %in% colnames(colData(sce))){"phases"} else {NULL})
#####

explVar <- plotExplanatoryVariables(sce, variables=explanatoryVariables, exprs_values = "logcounts") +    
    ggtitle("Explanatory variables")

cat("\nPCA plots of top 500 genes colored by potential confounder variables\n\n")  
  sce <-scater::runPCA(sce, name = "PCA")
  explVarScatter <- lapply(explanatoryVariables, function(x) {
                    scater::plotPCASCE(sce, ncomponents = 4, point_size=1, point_alpha=0.3, colour_by=x) + 
                      ggtitle(paste("PCA plots colored by", x))
                    })

  explVar2plot <- c(list(explVar), explVarScatter)
  
```

```{r plot_explanatorx_variables, results='asis', echo=F, error=F, warning=F, message=F, fig.height=6*length(explVar2plot)}
  gridExtra::grid.arrange(grobs=explVar2plot, ncol=1)
```



```{r limma_blocking_confounder, results='asis', eval=T}
# use limma for adjustment of cell cycle phase and confounder variable
library(limma)
var2adjust <- NULL
adjust_ccp <- F # adjust for cell cycle phase 
######

if(adjust_ccp | !is.null(var2adjust)) {
  
cat("## Adjust for confounder variables\n\n")


if(adjust_ccp) {
cat("We can account for possible cell cycle effect on downstream analysis, using the G1 and G2M assignment scores as a continuous blocking factor to estimate the variance. This is more graduated than using a strict assignment of each cell to a specific phase, as the magnitude of the score considers the uncertainty of the assignment. The phase covariates in the design matrix will absorb any phase-related effects on expression such that they will not affect estimation of the effects of other experimental factors. Any additionial batch variable can be included via the 'batch'-argument of the 'removeBatchEffect' function.")
    
    
    # filter out cells with undetermined cell cycle
     scoresFilteredNA <- assignments$scores[!is.na(assignments$phases), ] # NA entries are not used
     cat(sum(is.na(sce$phases)), "cells not assigned to a cell cycle phase are removed from the dataset.\n")
     sce <- sce[,!is.na(sce$phases)] 
    
    sce.no_block <- sce
    sce.no_block$G1score <- sce$G1score <- scoresFilteredNA$G1
    sce.no_block$G2Mscore <- sce$G2Mscore <- scoresFilteredNA$G2M
    
    # adjust for cell cycle phase by modelling G scores
    design <- model.matrix(~ G1 + G2M, scoresFilteredNA)
}


  if(!is.null(var2adjust)) { # includes batch effect
    
        if(adjust_ccp) {# includes cell cycle phase
          
            cat("\nVariable to adjust for (additional to cell cycle phase):", var2adjust)
            set.seed(100)
            assay(sce, "corrected") <- removeBatchEffect(logcounts(sce), batch=colData(sce)[, var2adjust], covariates=design[,-1]) 
            
            # create final_var2adjust (factor composed from var2adjust and optionally cell cycle phase)
            sce$final_var2adjust <- as.factor(paste(colData(sce)[,var2adjust], sce$phases, sep="_"))
            
             } else {
                  cat("\nNo adjustment for cell cycle phase applied.\n")
                  cat("\nVariable to adjust for:", var2adjust)
                set.seed(100)
                assay(sce, "corrected") <- removeBatchEffect(logcounts(sce), batch=colData(sce)[, var2adjust]) 
                
                # create final_var2adjust 
                sce$final_var2adjust <- as.factor(colData(sce)[,var2adjust])
            }
    
  } else { # no batch var
    cat("\nAdjustment for cell cycle scores only (no additional batch variable)")
    set.seed(100)
    assay(sce, "corrected") <- removeBatchEffect(logcounts(sce), covariates=design[,-1]) # scores as covariates
    
    # create final_var2adjust 
    sce$final_var2adjust <- as.factor(sce$phases)
  }


## switch assay channel for downstream processing
assay(sce, "logcounts_not_adjusted") <- assay(sce, "logcounts")
assay(sce, "logcounts") <- assay(sce, "corrected")
assay(sce, "corrected") <- NULL

# plotting
if(adjust_ccp) {
    cat("\nPCA plot of top 500 genes colored by G1 and G2M score\n")
    set.seed(100)
    out1 <- plotPCASCE(sce.no_block, ncomponents=2, rerun=T, # use_dimred="PCA", 
             colour_by="G1score", size_by="G2Mscore") + ggtitle("Before ccp adjustment")
    # After blocking on the phase scores
    set.seed(100)
    out2 <- plotPCASCE(sce, ncomponents=2, rerun=T, # use_dimred="PCA", 
              colour_by="G1score", size_by="G2Mscore") + ggtitle("After ccp adjustment")
    gridExtra::grid.arrange(out1, out2, ncol=2)
  }
} else {
    cat("\nno adjustment applied")
}
```


```{r batchelor_blocking_confounder, results='asis', echo=F, error=F, warning=F, message=F, eval=F}
library(batchelor)

##### parameters to set:
var2adjust <- NULL
adjust_ccp <- F # adjust for cell cycle phase (default = FALSE)
#####

cat("Counfounder variables like batch effects can be adjusted using the mutual nearest neighbors method (MNN). The effect from cell cycle phase can be removed from the dataset (if necessary) via linear regression treating each phase as a separate batch. If both a confounder variable and cell cycle shall be adjusted for, a new categorical variable is composed from both factors and used for MNN correction.")

## Info: Batchelor functions for batch correction
# (http://bioconductor.org/packages/devel/bioc/vignettes/batchelor/inst/doc/correction.html#3_mutual_nearest_neighbors)
# correctExperiments() # Apply a correction to multiple SingleCellExperiment objects, while also combining the 
# assay data and column metadata for easy use. batchCorrect does the correction inside this function
# batchCorrect() # A common interface for single-cell batch correction methods
# fastMNN() # For scRNA-seq data, fastMNN() tends to be both faster and better at achieving a satisfactory merge than mnnCorrect()
# mnnCorrect() # The original method described by Haghverdi et al. (2018), is mainly provided here for posteritys sake
#     The MNN-corrected values can be used for further correction with mnnCorrect().
#     This is useful in nested experimental designs involving multiple batches within each of multiple studies. 
#     Users should set cos.norm.in=FALSE and cos.norm.out=FALSE when supplying mnnCorrect() with MNN-corrected values. 
#     This ensures that the cosine normalization is only applied once, during the first round of MNN correction.
#     (https://bioc.ism.ac.jp/packages/3.7/workflows/vignettes/simpleSingleCell/inst/doc/work-5-mnn.html).
#     MNN-corrected values are generally not suitable for differential expression (DE) analyses.
# rescaleBatches() # conceptually equivalent to running removeBatchEffect() from limma with no covariates other than the batch. 
#     While this method is fast and simple, it makes the strong assumption that the population composition of each batch is the same. 
#     This is usually not the case for scRNA-seq experiments in real systems that exhibit biological variation. 
#     Thus, rescaleBatches() is best suited for merging technical replicates of the same sample, e.g., that have 
#     been sequenced separately.
# regressBatches() # Alternative to rescaleBatches(), a more direct linear regression of the batch effect. 
#     This does not preserve sparsity but uses a different set of tricks to avoid explicitly creating a dense matrix, 
#     specifically by using the ResidualMatrix class from the BiocSingular package.
# multiBatchNorm() # Differences in sequencing depth between batches are an obvious cause for batch-to-batch differences. 
#     These can be removed by multiBatchNorm(), which downscales all batches to match the coverage of the least-sequenced batch. 


# store unmodified logcounts channel for downstream processing
 assay(sce, "logcounts_not_adjusted") <- assay(sce, "logcounts")

if(adjust_ccp) {
 # filter out cells with undetermined cell cycle
 cat(sum(is.na(sce$phases)), "cells not assigned to a cell cycle phase are removed from the dataset.\n")
 sce <- sce[,!is.na(sce$phases)] 
}
 
 
if(is.null(var2adjust) & adjust_ccp) {
  
  # adjust cell cycle phase as batch effect (linear regression)
  cat("\nadjusting for cell cycle phase using linear regression.")
  sce$final_var2adjust <- as.factor(sce$phases)
  set.seed(100)
  sce.adjust <- batchelor::rescaleBatches(sce, batch=sce$final_var2adjust, assay.type = "logcounts")
  assay(sce, "logcounts") <- assay(sce.adjust, "corrected")

  } else {
  
  if(!is.null(var2adjust)) {
    
      # filter out cells with missing value for var2adjust
      cat(sum(is.na(colData(sce)[,var2adjust])), "cells with missing value for", var2adjust, "are removed from the dataset.\n")
      sce <- sce[,!is.na(colData(sce)[,var2adjust])] 
    
      # create final_var2adjust (factor composed from var2adjust and optionally cell cycle phase)
      sce$final_var2adjust <- if(adjust_ccp) {paste(colData(sce)[,var2adjust], sce$phases, sep="_")} else {colData(sce)[,var2adjust]}
      sce$final_var2adjust <- as.factor(sce$final_var2adjust)
      
      # adjust for final_var2adjust as batch effect (mutual nearest neighbours, MNN)
      cat("\nadjusting for", var2adjust, if(adjust_ccp) {"and cell cycle phase"}, "using MNN")
      set.seed(100)
      sce.adjust <- batchelor::fastMNN(sce, batch=sce$final_var2adjust, assay.type = "logcounts")
      reducedDim(sce, "corrected") <- reducedDim(sce.adjust, "corrected") # will be compared with PCA slot
      assay(sce, "logcounts") <- assay(sce.adjust, "reconstructed") # class LowRankMatrix (not compatible with trendVar) 
      
  } else {
    cat("\nno adjustment applied")
  }
}

```


```{r explanatory_variables_2, echo=F, error=F, warning=F, message=F, fig.width=8, fig.height=6*length(explVar2plot), results='asis', eval=(adjust_ccp | !is.null(var2adjust))}


if(!is.null(var2adjust)) {
  cat("\nRe-plot explanatory variables after adjustment\n\n")
  explVarAdj <- plotExplanatoryVariables(sce, variables=explanatoryVariables, exprs_values = "logcounts") +
                ggtitle("Explanatory variables after adjustment")
}

cat("\nPCA plots of top 500 genes colored by potential confounder variables after correction\n\n")
  explVarScaterAdj <- lapply(explanatoryVariables, function(x) {
                    sce <-scater::runPCA(sce, name = "PCA")
                    scater::plotPCASCE(sce, ncomponents = 4, point_size=1, colour_by=x) +
                          ggtitle(paste("PCA plots after adjustment colored by", x))
                    })

  explVar2plotAdj <- c(list(explVarAdj), explVarScaterAdj)
  gridExtra::grid.arrange(grobs=explVar2plotAdj, ncol=1)
```



# Identifying highly variable genes (HVGs)

The per-gene variation is quantified by computing the variance of the log-normalized expression values for each gene across all cells in the population (A. T. L. Lun, McCarthy, and Marioni 2016). This has an advantage in that the feature selection is based on the same log-values that are used for later downstream steps. In particular, genes with the largest variances in log-values will contribute the most to the Euclidean distances between cells. By using log-values here, we ensure that our quantitative definition of heterogeneity is consistent throughout the entire analysis.

Calculation of the per-gene variance is simple but feature selection requires modelling of the mean-variance relationship. The log-transformation does not achieve perfect variance stabilization, which means that the variance of a gene is driven more by its abundance than its underlying biological heterogeneity. To account for this effect, we use the modelGeneVar() function to fit a trend to the variance with respect to abundance across all genes (see scatter plot below).

```{r trendVar, echo=F, error=F, warning=F, message=F}
# estimate highly variable genes
decVar <- modelGeneVar(sce)
var.fit <- metadata(decVar)

plot(var.fit$mean, var.fit$var, xlab="Mean of log-expression",
    ylab="Variance of log-expression")
curve(var.fit$trend(x), col="dodgerblue", add=TRUE, lwd=2)
```


At any given abundance, we assume that the expression profiles of most genes are dominated by random technical noise. Under this assumption, our trend represents an estimate of the technical noise as a function of abundance. We then break down the total variance of each gene into the technical component, i.e., the fitted value of the trend at that genes abundance; and the biological component, defined as the difference between the total variance and the technical component. This biological component represents the interesting variation for each gene and can be used as the metric for HVG selection.

Once we have quantified the per-gene variation, the next step is to select the subset of HVGs to use in downstream analyses. A larger subset will reduce the risk of discarding interesting biological signal by retaining more potentially relevant genes, at the cost of increasing noise from irrelevant genes that might obscure said signal. Here, we consider the top 10% of genes with the highest biological components as HVGs for downstream analysis.

```{r get_hvg, results='asis', fig.width=11, echo=F, error=F, warning=F, message=F}
annoFactors <- colorByFactor2

# Ordering by most interesting genes for inspection.
decVar_ordered <- decVar[order(decVar$bio, decreasing=TRUE),] 
hvg <- getTopHVGs(decVar, prop=0.1)
rowSubset(sce, "HVGs") <- hvg

cat(paste(length(hvg), "genes are considered as HVGs.\n"))
DT::datatable(as.data.frame(decVar_ordered)[hvg,])


# plot top 10 HVGs
if(length(annoFactors)>2) {stop("\nno more than 2 categories allowed in 'anno'.")}
dotcol <- annoFactors[1]
if(length(annoFactors)==2) { 
    dotshape <- annoFactors[2]
} else {
    dotshape <- NULL
}

if(length(hvg) == 0) {
    cat("\nNo highly variable genes found.\n")
} else {
  tmp.sce <- sce    
  rownames(tmp.sce) <- rowData(tmp.sce)$SYMBOL
  print(
          plotExpression(tmp.sce, 
                         features = rownames(tmp.sce[rowData(tmp.sce)$ENSEMBL %in% hvg[1:10], ]),
                         colour_by=dotcol,
                         shape_by=dotshape) + 
            ggtitle("Top 10 HGVs for all cells") +
            scale_color_hue(c=40, l=50, name= dotcol) +
            facet_grid(~colour_by)
        )
}

```



# Using HVGs for further data exploration

## PCA based on significantly HVGs

```{r hvg_pca, results='asis', echo=F, error=F, warning=F, message=F, fig.height=5, fig.width=10}
##### parameters to set:
# define up to 2 categories to be indicated in the plot by color and shape.
annoFactors <- colorByFactor2 
ncomponents=2  # number of principal components to obtain  
exprs_values="logcounts"
#####

cat(paste0("\nPCA is calculated either based on the highly variable genes (HVGs) or on all genes (see suffix 'allgenes').\n"))

# HVG 
 set.seed(100)
  sce <- scater::runPCA(sce, name = "PCA", ncomponents=ncomponents, subset_row=hvg, exprs_values=exprs_values)

# all genes
 set.seed(100)
  sce <-scater::runPCA(sce, name = "PCA_allgenes", ncomponents=ncomponents, ntop = nrow(sce), exprs_values=exprs_values)
  
if(length(annoFactors)>2) {stop("\nno more than 2 categories allowed in 'annoFactors'.")}
  dotcol <- annoFactors[1]
  if(length(annoFactors)==2) { 
    dotshape <- annoFactors[2]} else {
      dotshape <- NULL}

# plot PCA
  gridExtra::grid.arrange(grobs=lapply(grep("PCA", reducedDimNames(sce), value=T), function(d) {
 
      scater::plotReducedDim(sce, dimred = d, 
        by_exprs_values = exprs_values,
        colour_by=dotcol, shape_by = dotshape, point_size=2) +  
        ggtitle(paste("PCA plot of", if(grepl("allgenes", d)) {"all genes"} else {"HVGs"}))

  }), ncol=2)
  
```

## Identification of sub-populations with t-SNE 

Another widely used approach is the t-stochastic neighbour embedding (t-SNE) method (Van der Maaten & Hinton, 2008). t-SNE  tends  to  work  better  than  PCA  for  separating  cells  in  more  diverse  populations.  This  is  because  the former can directly capture non-linear relationships in high-dimensional space, whereas the latter must represent them (suboptimally) as linear components. However, this improvement comes at the cost of more computational effort and complexity. In particular, t-SNE is a stochastic method, so users should run the algorithm several times to ensure that the results are representative, and then set a seed to ensure that the chosen results are reproducible. It is also advisable to test different settings of the perplexity parameter as this will affect the distribution of points in the low-dimensional space.

The perplexity can be interpreted as a smooth measure of the effective number of neighbors. The performance of SNE is fairly robust to changes in the perplexity, and typical values are between 5 and 50. 

A major weakness of t-SNE is that the cost function is not convex, as a result of which several optimization parameters need to be chosen. The constructed solutions depend on these choices of optimization parameters and may be different each time t-SNE is run from an initial random configuration of map points. But the developers of t-SNE have demonstrated that the same choice of optimization parameters can be used for a variety of different visualization tasks and found that the quality of the optima does not vary much from run to run. Thus, t-SNE should not be rejected in favor of methods that lead to convex optimization problems but produce noticeably worse visualizations. A local optimum of a cost function that accurately captures what is wanted in a visualization is often preferable to the global optimum of a cost function that fails to capture important aspects of what is wanted.

```{r TSNE_plot_various_perplexities, results='asis', fig.height=10, fig.width=10, echo=F, error=F, warning=F, message=F}
##### parameters to set:
# define up to 2 categories to be indicated in the plot by color and shape.
annoFactors <- colorByFactor2
ncomponents=2  
perplexity=c(25, 50)
exprs_values="logcounts"
#####

cat(paste0("\nHere, t-SNE dimensions (with perplexity ", paste(paste0("p=",perplexity), collapse=", "), ") are calculated either based on the highly variable genes (HVGs) or on all genes (see suffix 'allgenes').\n"))

if(length(annoFactors)>2) {stop("\nno more than 2 categories allowed in 'annoFactors'.")}
  dotcol <- annoFactors[1]
  if(length(annoFactors)==2) { 
    dotshape <- annoFactors[2]} else {
      dotshape <- NULL}

  # t-SNE without using pre-existing PCA results as input
    for (p in perplexity) {
    set.seed(100) # HWE
    sce <- scater::runTSNE(sce, name = paste0("TSNE_p", p), ncomponents=ncomponents, 
                           perplexity=p, dimred = NULL, subset_row=hvg, exprs_values=exprs_values)
    set.seed(100) # all genes
    sce <- scater::runTSNE(sce, name = paste0("TSNE_p", p, "_allgenes"), ncomponents=ncomponents, 
                           perplexity=p, dimred = NULL, ntop = nrow(sce), exprs_values=exprs_values)
    }
 
  # plot TSNE
  gridExtra::grid.arrange(grobs=lapply(grep("TSNE", reducedDimNames(sce), value=T), function(d) {
 
      scater::plotReducedDim(sce, dimred = d, 
        by_exprs_values = exprs_values,
        colour_by=dotcol, shape_by = dotshape, point_size=2) +  
        ggtitle(paste("TSNE plot of", if(grepl("allgenes", d)) {"all genes"} else {"HVGs"}, "with perplexity", gsub("^.*p", "", d)))

  }), ncol=2)
  
```


## Identification of sub-populations with UMAP 

Uniform manifold approximation and projection (UMAP) is another nonlinear dimensionality reduction technique to identify sub-populations in expression data.

```{r UMAP_plot, results='asis', fig.height=5, fig.width=10, echo=F, error=F, warning=F, message=F}
##### parameters to set:
# define up to 2 categories to be indicated in the plot by color and shape.
annoFactors <- colorByFactor2
ncomponents=2  
n_neighbors = 15
exprs_values="logcounts"
#####

cat(paste0("\nUMAP dimensions (with ", paste(paste0("n_neighbors=",n_neighbors), collapse=", "), ") are calculated either based on the highly variable genes (HVGs) or on all genes (see suffix 'allgenes').\n"))


if(length(annoFactors)>2) {stop("\nno more than 2 categories allowed in 'annoFactors'.")}
  dotcol <- annoFactors[1]
  if(length(annoFactors)==2) { 
    dotshape <- annoFactors[2]} else {
      dotshape <- NULL}


# HVGs
 set.seed(100)
  sce <- scater::runUMAP(sce, name = "UMAP", ncomponents=ncomponents, n_neighbors = n_neighbors, 
                         dimred = NULL, subset_row=hvg, exprs_values=exprs_values)  
# all genes  
 set.seed(100)
  sce <- scater::runUMAP(sce, name = "UMAP_allgenes", ncomponents=ncomponents, n_neighbors = n_neighbors, 
                         dimred = NULL, ntop = nrow(sce), exprs_values=exprs_values)

 # plot UMAP 
   gridExtra::grid.arrange(grobs=lapply(grep("UMAP", reducedDimNames(sce), value=T), function(d) {
 
      scater::plotReducedDim(sce, dimred = d, 
        by_exprs_values = exprs_values,
        colour_by=dotcol, shape_by = dotshape, point_size=2) +  
        ggtitle(paste("UMAP plot of", if(grepl("allgenes", d)) {"all genes"} else {"HVGs"}))

  }), ncol=2)
 
```


# Clustering

## Defining clusters with SC3

Single-Cell Consensus Clustering (SC3) is a tool for unsupervised clustering of scRNA-seq data. SC3 achieves high accuracy and robustness by consistently integrating different clustering solutions through a consensus approach. Genes used for clustering are filtered based on gene dropout rates of the counts matrix. SC3 recommends a number of clusters to be used for clustering.

```{r sc3, results="asis", eval=TRUE}
library(SC3)
library(pkgmaker)

rowData(sce)$feature_symbol <- rowData(sce)$SYMBOL # column feature_symbol needed for SC3

  set.seed(100)
    sce <- sc3_prepare(sce, gene_filter=T, n_cores=8, rand_seed = 100) # 
  set.seed(100)
    sce <- sc3_estimate_k(sce) 
    nr_clusters <- metadata(sce)$sc3$k_estimation
    cat("Number of estimated clusters:", nr_clusters, "\n")
  set.seed(100)
    sce <- sc3_calc_dists(sce) 
  set.seed(100)
    sce <- sc3_calc_transfs(sce)
  set.seed(100)
    sce <- sc3_kmeans(sce, ks=nr_clusters) 
  set.seed(100)
    sce <- sc3_calc_consens(sce) # assigns clusters in colData(sce)
 
# metadata(sce)$sc3
# colData(sce)[ , grep("sc3_", colnames(colData(sce))), drop=F]   
# rowData(sce)[ , grep("sc3_", colnames(rowData(sce))), drop=F]

# print cluster assignment
  kbl(t(table(colData(sce)[, paste0("sc3_", nr_clusters, "_clusters")])), align = "l", format="simple", caption="cells per cluster") %>% kable_styling(full_width = F) 

# store cluster assignments 
dir_sc3 <- file.path("report_files", "cluster", "SC3")
if (!file.exists(dir_sc3)) {dir.create(dir_sc3, recursive=T) }
write.table(colData(sce)[,c(group.vars, grep("sc3_", colnames(colData(sce)), value=T))], file =file.path(dir_sc3, paste0("SC3_", nr_clusters, "_cluster.txt")), sep="\t", quote = F, row.names = F)


### plot SC3 clustering
annoFactors <- c(paste0("sc3_", nr_clusters, "_clusters"), colorByFactor2[1])  

if(length(annoFactors)>2) {stop("\nno more than 2 categories allowed in 'annoFactors'.")}
  dotcol <- annoFactors[1]
  if(length(annoFactors)==2) { 
    dotshape <- annoFactors[2]} else {
      dotshape <- NULL}

cat(paste0("\nClustering is applied on the gene list filtered by SC3 (", sum(rowData(sce)$sc3_gene_filter), " of ", nrow(sce), " genes were used for clustering). The obtained cluster assingment is illustrated with PCA, TSNE (with perplexity ", paste(paste0("p=",perplexity), collapse=", "), ") and UMAP plots (with n_neighbors=", paste(n_neighbors, collapse = ", "), "), which were calculated based on either the highly variable genes (HVGs) or on the SC3 filtered genes (see suffix 'filtgenes').\n\n"))

 sceSC3 <- sce # create temp object for dim reduction with SC3 filtered gene list.
 reducedDims(sceSC3)[grepl("allgenes", reducedDimNames(sceSC3))] <- NULL # replace "allgenes" with SC3 gene filtered list
 set.seed(100) # PCA
 sceSC3 <- scater::runPCA(sceSC3, name = "PCA_filtgenes", ncomponents=ncomponents, subset_row=rowData(sceSC3)$sc3_gene_filter, exprs_values=exprs_values)
 for(p in perplexity) { # TSNE
    set.seed(100)
    sceSC3 <- scater::runTSNE(sceSC3, name = paste0("TSNE_p", p, "_filtgenes"), ncomponents=ncomponents, 
                           perplexity=p, dimred = NULL, subset_row=rowData(sceSC3)$sc3_gene_filter, exprs_values=exprs_values)
  }
 set.seed(100) # UMAP
 sceSC3 <- scater::runUMAP(sceSC3, name = "UMAP_filtgenes", ncomponents=ncomponents, n_neighbors = n_neighbors, 
                         dimred = NULL, subset_row=rowData(sceSC3)$sc3_gene_filter, exprs_values=exprs_values)  


### plot clustering separated for PCA, TSNE and UMAP
  for (d in c("PCA", "TSNE", "UMAP")) {
        plot_Items <- grep(d, reducedDimNames(sceSC3), value=T) # use sceSC3 instead of sce
        
      # dim reduction plots
        plot_cluster <- mclapply(plot_Items, function(i) {
                  scater::plotReducedDim(sceSC3,  # use sceSC3 instead of sce
                    dimred = i,
                    colour_by=dotcol, shape_by = dotshape, point_size=1.5) +
                    scale_color_discrete(name="cluster") +
                    guides(color=guide_legend(ncol=2)) +
                    ggtitle(paste(gsub("_", " ", i), "SC3")) + theme(aspect.ratio = 1)
              }, mc.cores=1)
      
      filenamePlot <- file.path(dir_sc3, paste0("SC3_", d, "_cluster_plots.png")) 
      ggsave(filename=filenamePlot,
                      plot=gridExtra::grid.arrange(grobs=plot_cluster, ncol=2),
                      width = 200, height = 80*ceiling(length(plot_Items)/2),
                      units = c("mm"),  dpi = 300, device="png")
    
      # include plots in report    
      cat(paste0("![plot of ", basename(filenamePlot), "](", filenamePlot, ")"))
  }

 rm(sceSC3)
```



## Graph-based clustering with igraph

Graph-based clustering is a flexible and scalable technique for clustering large scRNA-seq datasets. We first build a graph where each node is a cell that is connected to its nearest neighbours in the high-dimensional space. Edges are weighted based on the similarity between the cells involved, with higher weight given to cells that are more closely related. We then apply algorithms to identify communities of cells that are more connected to cells in the same community than they are to cells of different communities. Each community represents a cluster that we can use for downstream interpretation. 

Graph construction avoids making strong assumptions about the shape of the clusters or the distribution of cells within each cluster, compared to other methods. k is the number of nearest neighbors used to construct the graph. This controls the resolution of the clustering where higher k yields a more inter-connected graph and broader clusters. Users can exploit this by experimenting with different values of k to obtain a satisfactory resolution.

Clustering is applied on the respective dimension reduction slot (i.e. PCA, TSNE or UMAP).

```{r clustering_graphbased, results='asis'}
library(igraph)
library(pheatmap)
exprs_values = "logcounts"
dotshape <- colorByFactor2[1]
numberNearestNeighbors <- c(10, 20) # number of nearest neighbors k
# available clustering strategies in igraph
cluster_algo <- c("cluster_walktrap", "cluster_louvain", "cluster_infomap", "cluster_fast_greedy", "cluster_label_prop", "cluster_leading_eigen")[1:2]

cat(paste0("\nClustering is applied on the respective dimension reduction slot, i.e. PCA, TSNE (with perplexity ", paste(paste0("p=",perplexity), collapse=", "), ") or UMAP (with n_neighbors=", paste(n_neighbors, collapse = ", "), "), which were calculated either based on the highly variable genes (HVGs) or on all genes (see suffix 'allgenes').\n"))
cat("\nGraph-based clustering algorithms applied:", cluster_algo, "\n")
cat("\nNumber of nearest neighbors k to consider during graph construction:", numberNearestNeighbors, "\n")

sce_buildgraph <- list()
sce_igraphclust <- list()
clmod <- list() # clusterModularity
cluster.gr <- list() # cluster interaction graph

for(d in reducedDimNames(sce)) {    

  for(k in numberNearestNeighbors) {
  
    set.seed(100)
    sce_buildgraph[[paste(d, paste0("k",k), sep="_")]] <- buildSNNGraph(sce, k=k, use.dimred = d) # type= rank, number, jaccard
    
    for (clu in cluster_algo) {
      
      clu <- gsub("^cluster_", "", clu)
      clusterfun <- switch(clu, 
                      "walktrap"=igraph::cluster_walktrap,
                      "louvain"= igraph::cluster_louvain,
                      "infomap"= igraph::cluster_infomap,
                      "fast_greedy"= igraph::cluster_fast_greedy,
                      "label_prop"= igraph::cluster_label_prop,
                      "leading_eigen"= igraph::cluster_leading_eigen)

      set.seed(100)
      sce_igraphclust[[paste(d, paste0("k",k), clu, sep="_")]] <- clusterfun(sce_buildgraph[[paste(d, paste0("k",k), sep="_")]])$membership
      colData(sce)[,paste("igraph", d, paste0("k",k), clu, sep="_")] <- factor(sce_igraphclust[[paste(d, paste0("k",k), clu, sep="_")]])
      
  # Assessing cluster separation
  set.seed(100)
  clmod[[paste(d, paste0("k",k), clu, sep="_")]] <- bluster::pairwiseModularity(sce_buildgraph[[paste(d, paste0("k",k), sep="_")]], sce_igraphclust[[paste(d, paste0("k",k), clu, sep="_")]], get.weights=F, as.ratio = TRUE)
  cluster.gr[[paste(d, paste0("k",k), clu, sep="_")]] <- igraph::graph_from_adjacency_matrix(clmod[[paste(d, paste0("k",k), clu, sep="_")]], mode="upper", weighted=TRUE, diag=FALSE)

    }
  }
}    

cat("\nOverview cluster assignments:\n", fill=T)
for (x in names(sce_igraphclust)) {
  print(kbl(t(table(sce_igraphclust[[x]])), align = "l", format="simple", caption=x) %>% kable_styling(full_width = F))
  cat("\n")
}


# store cluster assignments
dir_igraph <- file.path("report_files", "cluster", "igraph")
if (!file.exists(dir_igraph)) {dir.create(dir_igraph, recursive=T) }
      write.table(colData(sce)[,c(group.vars, grep("igraph", names(colData(sce)), value=T))], 
      file = file.path(dir_igraph, paste0("igraph_cluster_assignments.txt")), sep="\t", quote=F, row.names=F)



### plot clusterings (separated for PCA, TSNE and UMAP)
  for (d in c("PCA", "TSNE", "UMAP")) {
        plot_Items <- grep(d, names(sce_igraphclust), value=T)
        
      # dim reduction plots
        plot_cluster <- mclapply(plot_Items, function(i) {
                  scater::plotReducedDim(sce, 
                    dimred = sub("^([A-Z]*[_p0-9]*[_allgenes]*)_.*", "\\1", i),
                    by_exprs_values = exprs_values,
                    colour_by=paste0("igraph_", i), shape_by = dotshape, point_size=2) +
                    scale_color_discrete(name="cluster") +
                    guides(color=guide_legend(ncol=2)) + theme(aspect.ratio = 1) +
                    ggtitle(gsub("_", " ", i))
              }, mc.cores=1)
      
      filenamePlot <- file.path(dir_igraph, paste0("igraph_", d, "_cluster_plots.png"))
      ggsave(filename=filenamePlot,
                      plot=gridExtra::grid.arrange(grobs=plot_cluster, ncol=2), 
                      width = 200, height = 80*ceiling(length(plot_Items)/2), 
                      units = c("mm"),  dpi = 300, device="png")
  
  # include plots in report    
  cat(paste0("![plot of ", basename(filenamePlot), "](", filenamePlot, ")"))
  }


# heatmaps
for (d in c("PCA", "TSNE", "UMAP")) {
 plot_Items <- grep(d, names(clmod), value=T)

   cl_heatmap <-  mclapply(plot_Items, function(i) {
                      pheatmap(log2(clmod[[i]]+1), cluster_rows=FALSE, cluster_cols=FALSE,
                          color=colorRampPalette(c("white", "blue"))(100), silent=T,
                          main= paste("cl heatmap", gsub("_", " ", i)))[["gtable"]]
               }, mc.cores=1)

   ggsave(filename=file.path(dir_igraph, paste0("igraph_", d, "_cluster_heatmaps.png")),
          plot=plot(arrangeGrob(grobs=cl_heatmap, ncol=2)), 
          width = 200, height = 80*ceiling(length(plot_Items)/2), units = c("mm"),  dpi = 300, device="png")
}


# cluster graph plots
  for (d in c("PCA", "TSNE", "UMAP")) {
    plot_Items <- grep(d, names(cluster.gr), value=T)
    
      png(filename= file.path(dir_igraph, paste0("igraph_", d, "_cluster_graph.png")), units = "mm", res= 300,
          width = 200, height = 80*ceiling(length(plot_Items)/2))
       par(mfrow = c(ceiling(length(plot_Items)/2), 2))
      for (i in plot_Items) {
      set.seed(100)
      plot(cluster.gr[[i]], edge.width=igraph::E(cluster.gr[[i]])$weight*5, main=paste("cluster graph", gsub("_", " ", i)))
      }
       
  dev.off()
  par(mfrow = c(1, 1))
 }
      
```


## Hierarchical clustering

Hierarchical clustering aims to generate a dendrogram containing a hierarchy of samples. This is most commonly done by greedily agglomerating samples into clusters, then agglomerating those clusters into larger clusters, and so on until all samples belong to a single cluster. Variants of hierarchical clustering methods primarily differ in how they choose to perform the agglomerations.

The dendrogram describes the relationships between cells and subpopulations at various resolutions and in a quantitative manner based on the branch lengths. Users can cut the tree at different heights to define clusters with different granularity, where clusters defined at high resolution are guaranteed to be nested within those defined at a lower resolution. The dendrogram is also a natural representation of the data in situations where cells have descended from a relatively recent common ancestor. For larger datasets hierarchical clustering may be too slow.

Clustering is applied on the respective dimension reduction slot (i.e. PCA, TSNE or UMAP).

```{r clustering_hierarchical, warning=F, results='asis'}
library(dendextend)
library(dynamicTreeCut)
hclust_methods <- c("ward.D2")
minClusterSize <- c(10, 100)
exprs_values <- "logcounts"
dotshape <- colorByFactor2[1]

cat(paste0("\nClustering is applied on the respective dimension reduction slot, i.e. PCA, TSNE (with perplexity ", paste(paste0("p=",perplexity), collapse=", "), ") or UMAP (with n_neighbors=", paste(n_neighbors, collapse = ", "), "), which were calculated either based on the highly variable genes (HVGs) or on all genes (see suffix 'allgenes').\n"))
cat("\nHierarchical clustering algorithms applied:", hclust_methods, "\n")
cat("\nMinimum cluster size cs:", minClusterSize, "\n")

dist_sce <- list()
tree_sce <- list()
dend <- list()
clust_sce <- list()

for(d in reducedDimNames(sce)) {    
    dist_sce[[d]] <- dist(reducedDim(sce, d))

  for(h in hclust_methods) {
    set.seed(100)
    tree_sce[[paste(d, h, sep="_")]] <- hclust(dist_sce[[d]], method=h)
    
    # Making dendrogram.
    tree_sce[[paste(d, h, sep="_")]]$labels <- seq_along(tree_sce[[paste(d, h, sep="_")]]$labels)

    for (cs in minClusterSize) {
      
      dend[[paste(d, h, paste0("cs", cs), sep="_")]] <- as.dendrogram(tree_sce[[paste(d, h, sep="_")]], hang=0.1)

      # minClusterSize needs to be turned down for small datasets.
      # deepSplit controls the resolution of the partitioning.
      set.seed(100)
      clust_sce[[paste(d, h, paste0("cs", cs), sep="_")]] <- cutreeDynamic(tree_sce[[paste(d, h, sep="_")]], 
                                                         distM=as.matrix(dist_sce[[d]]),
                                                         method = "hybrid", minClusterSize=cs, deepSplit=1, verbose = 0)

      labels_colors(dend[[paste(d, h, paste0("cs", cs), sep="_")]]) <- clust_sce[[paste(d, h, paste0("cs", cs), sep="_")]][order.dendrogram(dend[[paste(d, h, paste0("cs", cs), sep="_")]])]

      colData(sce)[,paste("hclust", d, h, paste0("cs",cs), sep="_")] <- factor(as.vector(clust_sce[[paste(d, h, paste0("cs", cs), sep="_")]])) 
      # as.vector removes vector names
  
     }
  }
}    


cat("\nOverview cluster assignments:\n", fill=T)
for (x in names(clust_sce)) {
  print(kbl(t(table(clust_sce[[x]])), align = "l", format="simple", caption=x) %>% kable_styling(full_width = F))
  cat("\n")
}

# store cluster assignments
dir_hclust <- file.path("report_files", "cluster", "hclust")
if (!file.exists(dir_hclust)) {dir.create(dir_hclust, recursive=T) }
      write.table(colData(sce)[,c(group.vars, grep("hclust", names(colData(sce)), value=T))], 
      file = file.path(dir_hclust, paste0("hclust_cluster_assignments.txt")), sep="\t", quote=F, row.names=F)


### plot clusterings (separated for PCA, TSNE and UMAP)
  for (d in c("PCA", "TSNE", "UMAP")) {
    plot_Items <- grep(d, names(clust_sce), value=T)
    
      # dim reduction plots
        plot_cluster <- mclapply(plot_Items, function(i) {
                  scater::plotReducedDim(sce, 
                    dimred = sub("^([A-Z]*[_p0-9]*[_allgenes]*)_.*", "\\1", i),
                    by_exprs_values = exprs_values,
                    colour_by=paste0("hclust_", i), shape_by = dotshape, point_size=3) +
                    scale_color_discrete(name="cluster") +
                    guides(color=guide_legend(ncol=2)) +
                    ggtitle(gsub("_", " ", i)) + theme(aspect.ratio = 1)
              }, mc.cores=1)
  
        
      filenamePlot <- file.path(dir_hclust, paste0("hclust_", d, "_cluster_plots.png"))
      ggsave(filename=filenamePlot,
                      plot=gridExtra::grid.arrange(grobs=plot_cluster, ncol=2), 
                      width = 200, height = 80*ceiling(length(plot_Items)/2), 
                      units = c("mm"),  dpi = 300, device="png")
    
      # include plots in report    
      cat(paste0("![plot of ", basename(filenamePlot), "](", filenamePlot, ")"))
  }
  
## plot dendrograms
 for (d in c("PCA", "TSNE", "UMAP")) {
    plot_Items <- grep(d, names(clust_sce), value=T)
    
     png(filename= file.path(dir_hclust, paste0("hclust_", d, "_dendrograms.png")), units = "mm", res= 300,
          width = 200, height = 80*ceiling(length(plot_Items)/2))
      
      par(mfrow = c(ceiling(length(plot_Items)/2), 2))
      
      for (i in plot_Items) {
        plot(dend[[i]], main=paste("hclust dendrogram", gsub("_", " ", i)))
      }
      par(mfrow = c(1, 1))
      dev.off()
 }


```

## Clustering method selected for downstream processing

```{r select_clustering, results='asis'}
     
  selected_clustering_method <- "hclust_TSNE_p25_ward.D2_cs100"    
  name_dimred <- gsub("^.*(((TSNE)(_p[[:digit:]]+)*|(UMAP)|(PCA))(_allgenes)*).*$", "\\1",  selected_clustering_method) # get corresponding reducedDimName
  cat("Selected clustering:", selected_clustering_method, "\n\nreducedDimName:", name_dimred, "\n\n")
  
  colLabels(sce) <- factor(colData(sce)[, selected_clustering_method])
  
   plot_selected <- scater::plotReducedDim(sce, 
                      dimred = name_dimred,
                      by_exprs_values = exprs_values,
                      colour_by=selected_clustering_method, shape_by = dotshape, point_size=3) +
                      scale_color_discrete(name="cluster") +
                      guides(color=guide_legend(ncol=2)) +
                      ggtitle(gsub("_", " ", selected_clustering_method)) + theme(aspect.ratio = 1)
   
   plot_selected
   
   ggsave(plot=plot_selected, filename= file.path("report_files", "cluster", paste0(selected_clustering_method, "_selected_cluster_plot.png")),
          width = 120, height = 120, units = c("mm"),  dpi = 300)
cat("\n")
```



```{r doublet_detection_with_clusters, results='asis', eval=SHINYREPS_SEQTYPE %in% c("tenX")}
library(scDblFinder)
cat("# Doublet detection\n\n")

cat("## Doublet detection with clusters\n")

cat("\n\nThe findDoubletClusters() function from the scDblFinder package identifies clusters with expression profiles lying between two other clusters (Bach et al. 2017). We consider every possible triplet of clusters consisting of a query cluster and two putative source clusters. Under the null hypothesis that the query consists of doublets from the two sources, we compute the number of genes (num.de) that are differentially expressed in the same direction in the query cluster compared to both of the source clusters. Such genes would be unique markers for the query cluster and provide evidence against the null hypothesis. For each query cluster, the best pair of putative sources is identified based on the lowest num.de. Clusters are then ranked by num.de where those with the few unique genes are more likely to be composed of doublets.\n")

dbl.out <- findDoubletClusters(sce, clusters=colData(sce)[,selected_clustering_method])
kbl(DataFrame(cluster=rownames(dbl.out), dbl.out), caption="Doublet detection with clusters") %>% kable_styling()

```


```{r doublet_detection_by_simulation, results='asis', eval=SHINYREPS_SEQTYPE %in% c("tenX")}
annoFactors <- colorByFactor2
######################

cat("## Doublet detection by simulation\n")

cat("\nThis doublet detection strategy involves in silico simulation of doublets from the single-cell expression profiles (Dahlin et al. 2018). This is performed using the computeDoubletDensity() function from the scDblFinder package. This function simulates thousands of doublets by adding together two randomly chosen single-cell profiles, computes the density of simulated doublets in the surrounding neighborhood for each original cell, computes the density of other observed cells in the neighborhood and returns the ratio between the two densities as a doublet score for each cell.\n")


library(BiocSingular)

set.seed(100)

# Setting up the parameters for consistency with denoisePCA();
# this can be changed depending on your feature selection scheme.
dbl.dens <- computeDoubletDensity(sce, subset.row=hvg, d=ncol(reducedDim(sce)))
#summary(dbl.dens)

sce$DoubletScore <- dbl.dens

TSNE_DoubletScore <- plotReducedDim(sce, dimred=name_dimred, colour_by="DoubletScore", shape_by = annoFactors)
TSNE_DoubletScore

plotCluster_DoubletScore <- plotColData(sce, x=selected_clustering_method, y="DoubletScore", colour_by = annoFactors, shape_by = annoFactors)
plotCluster_DoubletScore
#gridExtra::grid.arrange(grobs=list(TSNE_DoubletScore, plotCluster_DoubletScore), ncol=1)

# Simply removing cells with high doublet scores will not be sufficient to eliminate real doublets from the data set. In some cases, only a subset of the cells in the putative doublet cluster actually have high scores, and removing these would still leave enough cells in that cluster to mislead downstream analyses. In fact, even defining a threshold on the doublet score is difficult as the interpretation of the score is relative. There is no general definition for a fixed threshold above which libraries are to be considered doublets.

# We recommend interpreting the computeDoubletDensity() scores in the context of cluster annotation. All cells from a cluster with a large average doublet score should be considered suspect, and close neighbors of problematic clusters should also be treated with caution.

```




# Detect marker genes for each cluster

Next we identify the genes that drive separation between clusters of the final clustering setting. These marker genes allow us to assign biological meaning to each cluster based on their functional annotation. In the most obvious case, the marker genes for each cluster are a priori associated with particular cell types, allowing us to treat the clustering as a proxy for cell type identity. We use a one-sided t-test to identify genes that are upregulated in each cluster compared to the others (test for log foldchange > 1).

Additionally, we use a cluster specific approach. Instead of identifying genes upregulated compared to any of the other clusters, we only consider genes that are up-regulated in all pairwise comparisons involving the cluster of interest. To achieve this, we use an intersection-union test where the combined p-value for each gene is the maximum of the p-values from all pairwise comparisons (apply setting pval.type="all"). A gene will only achieve a low combined p-value if it is strongly DE in all comparisons to other clusters.

For each cluster, every gene is assigned with the highest rank which from comparisons with all other clusters. E.g. the set of genes with top rank <= n contains the top n genes from each comparison. These genes which distinguish that cluster from any other cluster are visualized in a heatmap across all clusters.


```{r findmarkers_per_cluster, fig.width=11, fig.height=20, results='asis'}
DE_var2adjust <- "final_var2adjust" # it will be checked if DE_var2adjust exists in sce. Can also be NULL.
clustervar <- selected_clustering_method
groupingvar <- colorByFactor2 # still to implement: use findMarkers for differential expression analysis within clusters
assay.type <- if(adjust_ccp | !is.null(var2adjust)) {"logcounts_not_adjusted"} else {"logcounts"}
###############################################


if(nlevels(factor(colData(sce)[,clustervar])) >=2) {

  dir_marker <- file.path("report_files", "marker_per_cluster", clustervar)
  if (!file.exists(dir_marker)) {dir.create(dir_marker, recursive=T) }
  
  cat("Un-adjusted logcounts are used for marker detection. ")
  if(is.null(DE_var2adjust) || !any(colnames(colData(sce)) %in% DE_var2adjust)) {
    cat("The analysis is not adjusted for any confounding variables.\n")
    DE_var2adjust <- NULL
  } else {
      if(DE_var2adjust == "final_var2adjust") {
        cat("The analysis is adjusted for the same factors as described in 'Adjust for confounder variables'.\n\n")
      } else {
        cat("The analysis is adjusted for:", DE_var2adjust, "\n\n")
      } 
    }
    
    
  # find upregulated markers for each cluster and upregulated marker specific for each cluster
  # using unadjusted logcounts including previously applied blocking variable
  # If DE_var2adjust is NULL no blocking applied 
  markers <- findMarkers(sce, groups=colData(sce)[,clustervar], 
                         block= if (is.null(DE_var2adjust)) {NULL} else {colData(sce)[,DE_var2adjust]},  
                         assay.type = assay.type,
                         row.data=rowData(sce)[,"SYMBOL", drop=F], direction="up", lfc=1)
  markers_spec <- findMarkers(sce, groups=colData(sce)[,clustervar], 
                         block= if (is.null(DE_var2adjust)) {NULL} else {colData(sce)[,DE_var2adjust]},  
                         assay.type = assay.type,
                         row.data=rowData(sce)[,"SYMBOL", drop=F], direction="up", pval.type="all")
                         # use pval.type="some" if "all" is too stringent 
  
     list_heatmaps <- NULL
     list_heatmaps_spec <- NULL
   marker_heatmaps <- lapply(names(markers), function(cl) { 
    
         # all upregulated markers per cluster
        write.table(markers[[cl]], sep="\t", quote=F, row.names=F, 
                    file=file.path(dir_marker, paste0("gene_marker_upregulated_in_cluster_", cl, ".txt")) )
        best_set <- markers[[cl]][markers[[cl]]$Top <= 5,]
        logFCs <- as.matrix(best_set[,grepl("logFC", colnames(best_set))])
        colnames(logFCs) <- sub("logFC.", "", colnames(logFCs))
        logFCs <- logFCs[, colSums(is.na(logFCs)) != nrow(logFCs)] # remove columns with only NA (cannot be plotted in heatmap)
        
        # markers upregulated in 1 cluster only   
        write.table(markers_spec[[cl]], sep="\t", quote=F, row.names=F, 
                    file=file.path(dir_marker, paste0("gene_marker_upregulated_in_cluster_", cl, "_only.txt")))
        best_set_spec <- markers_spec[[cl]][1:nrow(best_set), ] # select same number of genes displayed as in best_set
        logFCs_spec <- as.matrix(best_set_spec[,grepl("logFC", colnames(best_set_spec))])
        colnames(logFCs_spec) <- sub("logFC.", "", colnames(logFCs_spec))
        logFCs_spec <- logFCs_spec[, colSums(is.na(logFCs_spec)) != nrow(logFCs_spec)] # remove columns with only NA 
        
        if(length(names(markers)) >=3) { # create heatmaps if at least 3 groups, i.e. at least 2 hetmap columns.
            list_heatmaps <- pheatmap(logFCs, labels_row=best_set$SYMBOL, silent=T, breaks=seq(-5, 5, length.out=101), 
                                  main=paste("Top marker upregulated in cluster", cl))[["gtable"]]
  
            list_heatmaps_spec <- pheatmap(logFCs_spec, labels_row=best_set_spec$SYMBOL, silent=T, breaks=seq(-5, 5, length.out=101), 
                                       main=paste("Top marker upregulated in cluster", cl, "only"))[["gtable"]]
        
            filenamePlot <- file.path(dir_marker, paste0("heatmap_of_upregulated_gene_marker_cluster_", cl, ".png"))
            ggsave(filename=filenamePlot,
                   plot=plot(arrangeGrob(grobs=list(list_heatmaps, list_heatmaps_spec), ncol=2)), 
                   width = 200, height = 150, units = c("mm"),  dpi = 300, device="png")
    
            # include plots in report    
            cat(paste0("![plot of ", basename(filenamePlot), "](", filenamePlot, ")"))
            cat("\n", fill = T)
        } else {cat("\nHeatmaps skipped because only 1 column to plot.")}
        
      return(list(all_up=list_heatmaps, cluster_spec=list_heatmaps_spec))
    })
  
} else {
  cat("\nNo marker genes to calculate because clustering variable has only 1 cluster\n")
}

```



```{r go_annotation, results='asis', message=F,  eval=(org %in% c("human", "mouse"))}
# for human and mouse only
cat("# GO annotation of maker genes per cluster\n")

cat("\nPerform a gene set enrichment analysis on the marker genes defining each cluster. This identifies the pathways and processes that are (relatively) active in each cluster based on upregulation of the associated genes compared to other clusters. We will use gene sets defined by the Gene Ontology (GO) project, which describe a comprehensive range of biological processes and functions. We define our subset of relevant marker genes at a FDR of 5% and apply the goana function from the limma package. This performs a hypergeometric test to identify GO terms that are overrepresented in our marker subset. We keep only biological process terms that are not overly general (<=200 genes) and which are significantly enriched (p<0.05).\n\n")
 
dir_anno_per_cluster <- file.path("report_files", "annotation_per_cluster", selected_clustering_method)
if (!file.exists(dir_anno_per_cluster)) {dir.create(dir_anno_per_cluster, recursive=T) }

  switch(org,
           human={
              library(org.Hs.eg.db)
              orgdb <- org.Hs.eg.db
              species="Hs"
           },
           mouse={
              library(org.Mm.eg.db)
              orgdb <- org.Mm.eg.db
              species="Mm"
           }
  )
    
anno.results <- list()

# Extract symbols for each GO term for individual lookup; done once.
    tab.go <- AnnotationDbi::select(orgdb, keytype="ENSEMBL", keys=gsub("\\..*$", "", rownames(sce)), columns="GOALL")
    by.go <- split(tab.go[,1], tab.go[,2])

for (i in levels(colData(sce)[,selected_clustering_method])) {
 
    cat("\nProcessing cluster", i, "\n")

    cur.markers <- markers[[i]]
    is.de <- cur.markers$FDR <= 0.05 
    # summary(is.de)
    
    if(org=="human") {rownames(cur.markers) <- gsub("\\..*$", "", rownames(cur.markers))} # remove .version of human Ensembl IDs

    entrez.ids <- mapIds(orgdb, keys=rownames(cur.markers), 
        column="ENTREZID", keytype="ENSEMBL")
    go.out <- goana(unique(entrez.ids[is.de]), species=species, 
        universe=unique(entrez.ids))
    
    # Only keeping biological process terms that are not overly general and which are significantly enriched.
    # P.DE: p-value for over-representation of the GO term in the set.
    go.out <- go.out[order(go.out$P.DE),]
    go.useful <- go.out[go.out$Ont=="BP" & go.out$N <= 200 & go.out$P.DE < 0.05,]
    
    print(kbl(go.useful[1:10,], align = "l", format="simple") %>% kable_styling(full_width = F))
    write.table(data.frame(GOID=rownames(go.useful), go.useful), 
                file =file.path(dir_anno_per_cluster, paste0("GOannotation_cluster_", i, ".txt")),
                quote = F, row.names = F,  sep="\t")
    
    anno.results[[i]] <- go.useful
    
    # Identify genes associated with an interesting term.
    # adhesion <- unique(by.go[["GO:0022408"]])
    # head(cur.markers[rownames(cur.markers) %in% adhesion,1:3], 10)
}
```



```{r cell_type_annotation, results='asis', message=F,  eval=(org %in% c("human"))}
# for human only
library(celldex)
library(SingleR)

cat("# Cell type annotation\n")

# Assigning cell labels from reference data (available for human only)
  cat("\nAssigning cell labels from built-in reference data constructed from Blueprint and ENCODE data (Martens and Stunnenberg 2013; The ENCODE Project Consortium 2012). This reference contains normalized expression values of 259 human bulk RNA-seq samples of pure stroma and immune cells. The Heatmap below shows the assignment score for each cell (column) and label (row). Scores are normalized to [0, 1] within each cell\n\n")
  logcountdata <- logcounts(sce)
  rownames(logcountdata) <- rowData(sce)$SYMBOL
  ref <- celldex::BlueprintEncodeData() # human bulk RNA-seq data from Blueprint and ENCODE
  pred <- SingleR(test=logcountdata, ref=ref, labels=ref$label.main) # alternative: ref$label.fine 
  plotScoreHeatmap(pred)

  #pred.cluster <- SingleR(test=logcountdata, ref=ref, labels=ref$label.main, clusters=colData(sce)[,selected_clustering_method])
  #plotScoreHeatmap(pred.cluster, show_colnames=T) # plot prediction per cluster instead per cell
  #table(pred$labels)
  #plotScoreDistribution(pred)
  
  cat("\n\nAssignments are compared with the clustering results to determine the identity of each cluster. The heatmap below shows the distribution of cells across labels and clusters in the dataset. Color scale is reported in the log10-number of cells for each cluster-label combination.\n\n")
  tab <- table(Assigned=pred$pruned.labels, Cluster=colData(sce)[,selected_clustering_method])
  # Adding a pseudo-count of 10 to avoid strong color jumps with just 1 cell.
  pheatmap(log2(tab+10), color=colorRampPalette(c("white", "blue"))(101))
  cat("\n")

```




```{r diff_expression, results='asis', eval=T}
DE_var2adjust <- "final_var2adjust" # it will be checked if DE_var2adjust exists in sce. Can also be NULL.
varDiffGroups <- colorByFactor2[1]
########################################

library(edgeR)

cat("# Multi-sample comparisons

Differential analyses of replicated multi-condition scRNA-seq experiments can be broadly split into two categories: differential expression (DE) and differential abundance (DA) analyses. The former tests for changes in expression between conditions for cells of the same type that are present in both conditions, while the latter tests for changes in the composition of cell types (or states, etc.) between conditions.

## Differential expression between conditions using pseudo-bulk samples

Motivations behind the use of pseudo-bulking:

- Larger counts are more amenable to standard DE analysis pipelines designed for bulk RNA-seq data. Normalization is more straightforward and certain statistical approximations are more accurate for large counts.
- Collapsing cells into samples reflects the fact that our biological replication occurs at the sample level (Lun and Marioni 2017). Each sample is represented no more than once for each condition, avoiding problems from unmodelled correlations between samples. Supplying the per-cell counts directly to a DE analysis pipeline would imply that each cell is an independent biological replicate, which is not true from an experimental perspective. 
- Variance between cells within each sample is masked, provided it does not affect variance across (replicate) samples. This avoids penalizing DEGs that are not uniformly up- or down-regulated for all cells in all samples of one condition. Masking is generally desirable as DEGs - unlike marker genes - do not need to have low within-sample variance to be interesting, e.g., if the treatment effect is consistent across replicate populations but heterogeneous on a per-cell basis. 
    
The DE analysis will be performed using quasi-likelihood (QL) methods from the edgeR package (Robinson, McCarthy, and Smyth 2010; Chen, Lun, and Smyth 2016). This uses a negative binomial generalized linear model (NB GLM) to handle overdispersed count data in experiments with limited replication. We test for differences in expression using glmQLFTest(). DEGs are defined as those with non-zero log-fold changes at a false discovery rate of 5%.
")



dir_DE_per_cluster <- file.path("report_files", "DE_per_cluster", selected_clustering_method)
if (!file.exists(dir_DE_per_cluster)) {dir.create(dir_DE_per_cluster, recursive=T) }

cat("Un-adjusted and un-normalized counts are used for differential expression analysis. Normalization is done after grouping of cells. ")
if(is.null(DE_var2adjust) || !any(colnames(colData(sce)) %in% DE_var2adjust)) {
  cat("The analysis is not adjusted for any confounding variables.\n")
  DE_var2adjust <- NULL
} else {
    if(DE_var2adjust == "final_var2adjust") {
      cat("The analysis is adjusted for the same factors as described in section 'Adjust for confounder variables'.\n")
    } else {
      cat("The analysis is adjusted for:", DE_var2adjust, "\n")
    } 
  }


# Creating pseudo-bulk samples from clusters uses (unnormalised "counts" matrix)
  summed <- aggregateAcrossCells(sce, ids=DataFrame(
                cluster=colData(sce)[,selected_clustering_method],
                DE_var2adjust=colData(sce)[, DE_var2adjust],
                diffgroups=colData(sce)[, varDiffGroups]),
                use.assay.type = "counts"
                )

 de.results <- list()

for (i in levels(colData(sce)[,selected_clustering_method])) {
   
    cat("\nProcessing cluster", i, "\n")
  
    current <- summed[,colData(summed)[,selected_clustering_method] == i]
    
    # Creating up a DGEList object for use in edgeR:
    y <- DGEList(counts(current), samples=colData(current), genes=rowData(current)$SYMBOL)
    
    # preprocessing for bulk samples (is done here for each cluster separately)
        # # remove samples with very low library sizes
        # discarded <- isOutlier(y$samples$lib.size, log=TRUE, type="lower") # 
        discarded <- current$ncells < 5
        y <- y[,!discarded]
        # remove genes that are lowly expressed
        keep <- filterByExpr(y, group=current$diffgroups, min.count = 5, min.total.count = 10, large.n = 3, min.prop = 0.5)
        y <- y[keep,]
    
    
    # correct for composition biases by normalization factors (currently using unprocessed count data)
    y <- edgeR::calcNormFactors(y)
    
    # Statistical modelling
    design <- try(
        if(!is.null(DE_var2adjust)) {
            model.matrix(~ factor(DE_var2adjust) + factor(diffgroups), y$samples)
          } else {
            model.matrix(~ factor(diffgroups), y$samples)
          },  
        silent=TRUE)
    
    if (is(design, "try-error") || 
        qr(design)$rank==nrow(design) ||
        qr(design)$rank < ncol(design)) 
    {
        # Skipping labels without contrasts or without 
        # enough residual d.f. to estimate the dispersion.
        cat("\nskip cluster", i, "\n")
        next
    }

    # print filter metrics if cluster is processed further
     cat(paste0("\nFiltering genes for low expression for cluster ", i, ". "))
     cat("Genes kept:", sum(keep), "\n")
    # cat("\nsamples discarded:", sum(discarded))

    # We estimate the negative binomial (NB) dispersions with estimateDisp(). 
    y <- estimateDisp(y, design)
    #summary(y$trended.dispersion)
    #plotBCV(y)
    
    # We also estimate the quasi-likelihood dispersions with glmQLFit() (Chen, Lun, and Smyth 2016). 
    # This fits a GLM to the counts for each gene and estimates the QL dispersion from the GLM deviance. 
    # We set robust=TRUE to avoid distortions from highly variable clusters (Phipson et al. 2016). 
    # The QL dispersion models the uncertainty and variability of the per-gene variance (Figure 14.3),
    # which is not well handled by the NB dispersions, so the two dispersion types complement each other 
    # in the final analysis.
    
    fit <- glmQLFit(y, design, robust=TRUE)
    #summary(fit$var.prior)
    #summary(fit$df.prior)
    #plotQLDisp(fit)
    
    # We test for differences in expression using glmQLFTest(). DEGs are defined as those with non-zero 
    # log-fold changes at a false discovery rate of 5%.
    
    cat("\nDifferential expression results for", varDiffGroups, "in cluster", i, "\n")
    res <- glmQLFTest(fit, coef=ncol(design))
    #summary(decideTests(res))
    print(kbl(as.data.frame(topTags(res)), align = "l", format="simple") %>% kable_styling(full_width = F)) 
    write.table(data.frame(EnsemblID=rownames(topTags(res, n=nrow(res))), topTags(res, n=nrow(res))), 
                file =file.path(dir_DE_per_cluster, paste0("DEgenes_cluster_", i, ".txt")),
                quote = F, row.names = F,  sep="\t")
    de.results[[i]] <- res

}    

cat("\nSummary for all clusters\n")    
summaries <- lapply(de.results, FUN=function(x) summary(decideTests(x))[,1])
sum.tab <- do.call(rbind, summaries)
print(kable(sum.tab, align = "l", format="simple") %>% kable_styling(full_width = F))

```




```{r store_data, echo=F, results='hide', error=F, warning=F, message=F, eval=T}

# store count data (all and by pool)
dir_countdata <- file.path("report_files", "count_data")

cat("count data matrices are stored in", dir_countdata, "\n")

countslist <- lapply(names(assays(sce)), function(x) {

  if (!file.exists(file.path(dir_countdata, x))) {dir.create(file.path(dir_countdata, x), recursive = T)}

  write.table(data.frame(gene=rownames(sce), symbol=rowData(sce)$SYMBOL, assay(sce, x)), 
              file =file.path(dir_countdata, x, paste0(x, "_all.txt")), sep="\t", quote = F, row.names = F)

  lapply(unique(colData(sce)$pool), function(y) {
    sce.subset <- sce[, colData(sce)$pool == y]
    write.table(data.frame(gene=rownames(sce.subset), symbol=rowData(sce.subset)$SYMBOL, assay(sce.subset, x)), 
                file = file.path(dir_countdata, x, paste0(x, "_", y, ".txt")), sep="\t", quote = F, row.names = F)
  })
})

# save workspace
save.image("report_files/WS.RData")

# create files for shiny app 
library(shinydashboard)
library(iSEE)

dir_shinyapp <- file.path("report_files/shinyapp")
if (!file.exists(dir_shinyapp)) {dir.create(dir_shinyapp) }
  app <- iSEE(sce)
  top.hvg <- hvg
  genes <- gene.names[gene.names$gene_id %in% rownames(sce), ]
  save(sce, genes, top.hvg, file=file.path(dir_shinyapp, "data4shiny.RData"))
  save(sce, file=file.path(dir_shinyapp, "sce.RData"))
  save(app, file=file.path(dir_shinyapp, "app.R"))
   
```


# Used tools and versions for this analysis ##

Read mapping was performed with STAR using the following parameters:

```{r STAR_parameters_paragraph, echo=F, results='asis', error=F, warning=F, message=F}
cat(DEhelper.STARparms(), sep="\n")
``` 

The following tools were used for data processing:

```{r ToolVersions_paragraph, echo=F, results='asis', error=F, warning=F, message=F}
cat(Toolhelper.ToolVersions(), sep="\n")
```

R session info:

```{r R_sessionInfo, echo=F, results='asis', error=F, warning=F, message=F}
sessionInfo()
```

</div>

