---
title: "SHINYREPS_PROJECT"
output:
  html_document:
    toc: true
    toc_float: true
    css: styles.css
---

<div class="contentbox">


# Description

Enter project description here


# Processing

Enter data processing here


```{r setup, echo=F, result='hide', error=F, warning=F, message=F}

# source helper functions
source("sc.shinyrep.helpers.R")

# load required packages
attach_packages(c("rmarkdown", "knitr", "Cairo", "gplots", "ggplot2", "plotly", "RColorBrewer", "ggrepel", "parallel",
                  "grid", "gridExtra", "rtracklayer", "scater", "scran", "ggbeeswarm", "dplyr", "tidyr", "forcats",
                  "scales", "kableExtra", "reshape2", "M3Drop", "vipor", "corrplot", "limma", "pheatmap",
                  "VennDiagram", "edgeR", "DESeq2", "GeneOverlap", "viridis", "Biobase", "batchelor", "SC3", "pkgmaker",
                  "igraph", "pheatmap", "cluster", "dendextend", "dynamicTreeCut", "shinydashboard", "umap", "SingleR"))

# load global variables
loadGlobalVars(f="shinyReports.txt")

# create folder for output files
if (!file.exists(file.path("report_files"))) {dir.create(file.path("report_files"))}

# set options
options(stringsAsFactors=FALSE)
CORES <- 2
pal   <- brewer.pal(9, "Set1")
pal_rb <- colorRampPalette(c(pal[1], "white", pal[2]))(20)
pal_y  <- colorRampPalette(c("black", "yellow"))(100)

knitr::opts_chunk$set(cache=F,
                      echo=F,
                      warning=F,
                      message=F,
                      dev='CairoPNG')

theme_set(theme_bw() + theme(axis.text=element_text(colour="grey30",size=12),
									axis.title=element_text(colour="grey30",size=14),
									plot.title=element_text(size=14,hjust=0.5),
									plot.subtitle=element_text(size=12,hjust=0.5),
									legend.text=element_text(size=12,colour="grey30"),
									legend.title=element_text(size=12,colour="grey30")))

# load list of mitochondrial genes
mito.genes <- tryCatch(read.delim(file.path(SHINYREPS_PROJECT,SHINYREPS_MTGENES))[, 1], error=function(e) NA)

## load targets file for all cells. 
targets <- read.delim(SHINYREPS_TARGET, sep=",")
group.vars <- colnames(targets)[!colnames(targets) %in% c("sample", "barcode", "row", "col", "cells")] 
targets[,group.vars] <- lapply(targets[,group.vars], factor)

required_cols <- c("sample", "plate", "row", "col", "cells", "group")
if(SHINYREPS_SEQTYPE=="MARSseq") {required_cols <- c(required_cols, "barcode", "pool")}
if(!all(required_cols) %in% colnames(targets)) {
  stop("required target columns missing: ", paste(required_cols[!required_cols %in% colnames(targets)], collapse=", "))
}

## create targets4plot (needed for pool-based libraries like MARS-Seq with 1 count file for all samples of a pool)
  if(SHINYREPS_SEQTYPE=="MARSseq") {
  targets4plot <- targets[!duplicated(targets$pool), !colnames(targets) %in% c("barcode", "row", "col", "cells"), drop=F]
  targets4plot$sample <- targets4plot$pool # for pool-based designs pool id is used for merging to count files 
  targets4plot <- sapply(targets4plot, factor)
  } else {
    targets4plot <- targets
  }

# print targets file
DT::datatable(targets, caption="targets file (sample sheet) used for analysis")

```

```{r read_annotation, cache=TRUE}
# load gene annotation provided in essential.vars.groovy
gtf <- import.gff(SHINYREPS_GTF, format="gtf", feature.type="exon")
gtf.flat <- unlist(reduce(split(gtf, elementMetadata(gtf)$gene_id)))
gene.lengths <- tapply(width(gtf.flat), names(gtf.flat), sum)
gene.names <- unique(as.data.frame(gtf)[, c("gene_id", "gene_name")])

```


# Quality control on bulk RNA

## FastQC of all reads 

The raw sequence reads of all samples are analysed with the popular FastQC tool (http://www.bioinformatics.babraham.ac.uk/projects/fastqc/).

1. The "Read qualities" Box-Whisker plots show the range of quality values across all base positions:
    (i) The central red line is the median value,
    (ii) The yellow box represents the inter-quartile range (25-75%),
    (iii) The upper and lower whiskers represent the 10% and 90% points,
    (iv) The blue line represents the mean quality.
The y-axis on the graph shows the Phred quality scores, which are logarithmically related to the base-calling error probabilities. The higher the score the better the base call. The background of the graph divides the y axis into very good quality calls (green), calls of reasonable quality (orange), and calls of poor quality (red). Typically, the majority of calls on all base positions fall into the green area.

2. The "Sequence bias" plots show the proportion of each base (% G, A, T and C) at each position. In a random library there would be little difference between the positions of a sequence run, so the lines in this plot should run parallel with each other. But most RNA-seq libraries show sequence imbalance in the first 10-12 read positions due to RT priming biases, which should however look fairly similar in all samples.

3. The "GC Content" plots show the GC% distribution of all reads (red lines) and the ideal normal distribution based on the data (blue lines). Typically, the red and blue lines tightly overlap and look essentially the same in all samples. An unusually shaped distribution could indicate a contaminated library.

```{r FastQC_paragraph1, echo=F, results='asis', error=F, warning=F, message=F}
##### parameters to set:
# Select samples for which you would like to include fastqc results in the report. For single cell RNA-Seq with many cells, 
# you may want to restrict the total number of plots. If you provide a regular expression in 'samplePattern' only those 
# filenames will be included which match this expression, e.g. setting samplePattern="R1" yields only those fastq 
# files containing read1 of a read pair. This is recomended e.g. for MARS-Seq, where Read2 contains barcode information only.
# For samplePattern=NULL (default) all fastq files will be included.
# If you want to exclude samples according to the given certain pattern (instead of including), set exclude=T. 
# This is e.g. for excluding the trimmed fastq files which contain "cutadapt" in the file name (see below). 
# If you set argument 'maxno', the maximum sample number will be restricted accordingly to the first 'maxno' plots. 
#
samplePattern=SHINYREPS_PATTERNFASTQC
maxno=SHINYREPS_MAXNO        
##### 

cat(DEhelper.Fastqc(web=F, samplePattern=samplePattern, maxno=maxno), sep="\n")
```


## Adapter trimming with Cutadapt

The following plot shows the amount of reads trimmed for the selected adapter sequences including polyA/polyT sequences if specified. The column "tooshort" gives the percentage of reads removed due to a length of less than 30 bases after trimming. Additionally to the column "trimmed" which is for all adapters combined, there are columns for every single adapter giving the percentage of reads trimmed for this adapter.

```{r cutadapt, echo=F, results='asis', error=F, warning=F, message=F}
##### parameters to set:
# define categories of targets.txt to be used for dot color in the plot (one plot per element of colorByFactor will be created). 
# The function will try to map the data from targets$sample to the cutadapt log file names (for this, the unique part of 
# targets$sample must be a substring of file name). Otherwise pruned file names will be used by default. 
# In case of pooled library design the targets object will be modified accordingly. 
colorByFactor = SHINYREPS_COLORBYFACTOR 
#####

DEhelper.cutadapt(colorByFactor=colorByFactor, targetsdf=targets4plot)
```


## FastQC after adapter trimming

Quality control plots of fastq files after adapter trimming with cutadapt.

```{r FastQC_paragraph2, echo=F, results='asis', error=F, warning=F, message=F}
# Applying FastQC again after adapter trimming (fastq file names contain "cutadapt" as part of their name)

cat(DEhelper.Fastqc(web=F, samplePattern=SHINYREPS_PATTERNFASTQC2), sep="\n") 
```


## Gene body coverage ##

The plots below show meta-gene profiles of 5' to 3' read coverage and can reveal biases due to RNA degradation or specific library prep protocol; polyA-selection protocols are particularly prone of producing 3' coverage bias upon RNA degradation. The grey line depicts the actual read coverage and the red line shows the overall trend.

```{r GeneBodyCoverage_paragraph, echo=F, results='asis', error=F, warning=F, message=F}
cat(DEhelper.geneBodyCov(web=F), sep="\n")
```

## Strand specificity ##

The table below shows the fraction of reads mapped in sense or antisense to gene exons - around 0.5 for non-stranded library prep protocols and close to 0 or 1 for strand-specific RNA-seq protocols.

```{r echo=F, results='asis', error=F, warning=F, message=F}
cat(DEhelper.strandspecificity(), sep="\n")
```


## Qualimap

Illustrate genomic origin of reads

```{r Qualimap}
DEhelper.Qualimap()
```


## RNA class representation ##

The following plot shows the fraction of reads assigned to various RNA classes. These plots help in determining if the sample prep protocol worked well and may reveal issues with rRNA contamination. 

```{r RNAtypes_paragraph, echo=F, results='asis', error=F, warning=F, message=F}
DEhelper.RNAtypes()
```



# Mapping Statistics

apping to the reference genome & transcriptome is performed with STAR (https://github.com/alexdobin/STAR). The program version, genome assembly and software parameters are described in the table at the end of the report.

The mapping statistics below show the number and percentage of: (i) input raw reads, (ii) uniquely mapped reads, (iii) multi-mapped reads aligning equally well to multiple (up to 20) positions in the genome, (iv) reads that align to too many (>20) genome loci are discarded, (v) unmapped reads. For the multimapped reads, one random alignment among the best mapping positions is retained. 

While both unique and multi-mapped reads are included in the generation of browser coverage tracks, only uniquely mapped reads are typically taken into account in the differential expression analysis in order to avoid potential false-positives. However, this standard analysis approach may also exclude some recently duplicated genes or gene-pseudogene pairs, resulting in false-negatives. To investigate the potential expression changes in such genes, a side branch of DESeq2 may be used for using all mapped reads.

The browser tracks are generated using bamCoverage tool in [deepTools](https://deeptools.readthedocs.io/en/develop/) and they are normalized using the Counts Per Million mapped reads (CPM) method. The CPM formula is: CPM  = number of reads per bin (20 bp) / number of mapped reads (in millions).

```{r mapping_stats, fig.height=10, fig.width=12, results='asis', message=FALSE, warning=FALSE}
##### parameters to set:
# define categories of targets.txt to be used for dot color in the summary plot (one plot per element of colorByFactor will be created). 
# The function will try to map the data from targets$sample to the cutadapt log file names (for this, the unique part of 
# targets$sample must be a substring of file name). Otherwise pruned file names will be used by default. 
# In case of pooled library design the targets object will be modified accordingly.  
colorByFactor = SHINYREPS_COLORBYFACTOR 
#####

DEhelper.STAR(targetsdf=targets4plot, colorByFactor=colorByFactor)
```



```{r umicount, echo=F, results='asis', error=F, warning=F, message=F, eval= (SHINYREPS_SEQTYPE=="MARSseq")}
##### parameters to set:
# define categories of targets.txt to be used for dot color in the plot (one plot per element of colorByFactor will be created). 
# The function will try to map the data from targets$sample to the umicount log file names (for this, the unique part of 
# targets$sample must be a substring of the file name). Otherwise pruned file names will be used by default. 
# In case of pooled library design the targets object will be modified accordingly. 
colorByFactor = SHINYREPS_COLORBYFACTOR 
#####

cat("## Read de-duplication and feature counting with UMI-tools

UMI-tools counts the mapped reads per feature obtained after de-duplication. UMI-tools uses the uniquely mapped reads from STAR as well as as the multimapped reads (but not those which are mapped to too many loci) as input reads.")

DEhelper.umicount(colorByFactor=colorByFactor, targetsdf=targets4plot)
```







# Low-level analysis of single-cell RNAseq data

The analysis workflow is based on the Bioconductor package *scater* and the Bioconductor workshop published in F1000 Research: Lun, A. T. L., McCarthy, D. J., & Marioni, J. C. (2016). *A step-by-step workflow for low-level analysis of single-cell RNA-seq data.* F1000Research, 5(0), 2122. http://doi.org/10.12688/f1000research.9501.1


```{r loading_counts, echo=F, message=FALSE, error=TRUE, warning=TRUE, cache=F}

## load counts for MARSseq design (pooled count files) 
switch(SHINYREPS_SEQTYPE,
    MARSseq={
        # load counts
        f <- list.files(paste0(SHINYREPS_UMICOUNT), pattern="\\.tsv$", full.names=TRUE)
        counts <- mclapply(f, read.delim, head=T, row.names=1, mc.cores=CORES)
        names(counts) <- basename(f)

        # pool-wise replacement of barcodes by sample names. A pool consists of those cells which are amplified together 
        # in one pool (i.e. all reads have identical pool barcode) but which all have different unique cell barcodes.
        # The cell barcodes are re-used in other pools. For assignment of cell barcodes given in 'targets.txt' to the 
        # pooled count data, the pool ID given in 'targets.txt' must be a substring of the respective count data filename
        # (no matter at which position).
        for (i in unique(targets$pool)) {
            if(sum(grepl(i, names(counts), ignore.case = T)) !=1) {stop("\ncannot unambiguously assign pool ID given in targets.txt to count filename")}
            current_pool <- targets[targets$pool ==i, ]
            countname <- grep(i, names(counts), ignore.case = T, value=T )
            colnames(counts[[countname]]) <- current_pool[match(colnames(counts[[countname]]), current_pool$barcode), "sample"] # replace barcode with sample name
        }
        
        # merge pools into one dataset with union of genelists
        custom.merge <- function(x,y) { # custom merge function for Reduce
            z <- merge(x,y, all=T, by="row.names")
            rownames(z) <- z$Row.names
            return(z[,-1])
        }
        counts <- Reduce(custom.merge, counts)
        
        counts[is.na(counts)] <- 0 # set NAs in the dataset to zero counts
        targets <- targets[targets$sample %in% colnames(counts),] # adjust targets in case of cells have been lost during processing
        if (!all(colnames(counts) %in% targets$sample)) {stop("column names of count matrix and entries in targets$sample do not match!")}
        counts <- counts[,match(targets$sample, colnames(counts))] # sort counts according to target
        # table(colnames(counts) == targets$sample) # all TRUE
    },
    SmartSeq2={
      # load counts
        f <- list.files(paste0(SHINYREPS_SUBREAD), pattern="\\.readcounts\\.tsv$", full.names=TRUE) # don't select raw_readcounts.tsv
        f <- f[!grepl("Undetermined", f)] # don't use count file with unmapped reads if present
        
        counts <- mclapply(f, read.delim, header=F, row.names=1, comment.char = "#")
        stopifnot(all(sapply(counts, function(x) all(rownames(x) == rownames(counts[[1]])))))
        counts <- do.call(cbind, counts)
        colnames(counts) <- basename(f)

      # align with targets.txt
        if(any(duplicated(targets$sample))) { # check for duplicated entries in targets.txt
          stop("Duplicated entries in targets.txt: ", paste(targets$sample[duplicated(targets$sample)], collapse=", "))
        }
     
        logindex_targetsfile <- sapply(targets$sample, grep,  colnames(counts)) # grep targets in counts
        targets <- targets[sapply(logindex_targetsfile, length) ==1, ] # remove targets entries not (uniquely) found in counts

       if(!identical(sort(unname(unlist(logindex_targetsfile))), 1:length(colnames(counts)))) { 
         # check if all counts samples are contained in targets.txt
         stop("Sample names not included in targets.txt: ", paste(colnames(counts)[-logindex_targetsfile], collapse=", "))
       }
       counts <- counts[,unlist(logindex_targetsfile)] # cols in same order as rows in targets   
       colnames(counts) <- targets$sample
    },
    stop(c("Don't find seqtype:", SHINYREPS_SEQTYPE))  
)


#### create SingleCellExperiment object
sce <- SingleCellExperiment(assays=list(counts=as.matrix(counts)), colData = targets)

# add gene symbols from annotation file to SingleCellExperiment object
rowData(sce)$ENSEMBL <- rownames(sce)
rowData(sce)$SYMBOL  <- gtf$gene_name[match(rownames(sce), gtf$gene_id)]

# get rid of NAs and duplicated names (missing SYMBOLS are replaced with ENSEMBL IDs)
new.names <- rowData(sce)$SYMBOL
missing.name <- is.na(rowData(sce)$SYMBOL)
new.names[missing.name] <- rowData(sce)$ENSEMBL[missing.name]

# duplicated names are extended with ENSEMBL IDs
dup.name <- new.names %in% new.names[duplicated(new.names)]
new.names[dup.name] <- paste0(new.names, "_", rowData(sce)$ENSEMBL)[dup.name]
rowData(sce)$SYMBOL <- new.names

# add gene length information
rowData(sce)$genelength <- gene.lengths[ match(rownames(sce),names(gene.lengths)) ]

```


## Quality control of cells and RNA sequenced

To assess if the sequenced libraries are usable and the RNA captured represents a meaningful fraction of the RNA present in the cell, we are focussing on the following factors:

* **Library size:** for cells with small library size the RNA was not efficiently captured.
* **Number of expressed genes:** few expressed genes suggest that a diverse transcript population was not captured.
* **Proportion of reads mapping to mitochondrial genes:** high proportion mean increased apoptosis and/or loss of cytoplasmatic RNA from lysed cells.

To remove outliers, these criteria may be filtered either for relative thresholds using the median absolute deviation (MAD) or by setting absolute threshold after inspecting the quality control plots.

```{r quality_plots_of_cells}

##### parameters to set:
# define set of categories used to annotate cells in the plots (e.g.: annoFactors <- c("group", "pool"))
# By default, all columns given in 'targets.txt' ar used except for "sample", "barcode", "row" and "col".
annoFactors <- SHINYREPS_COLORBYFACTOR 
#####

annoFactors <- unique(c(annoFactors, "cells")) # cells will be included anyway

# define mitochondrial genes as control features
is.mito  <- row.names(sce) %in% mito.genes
# calculate QC metrics
sce <- calculateQCMetrics(sce, feature_controls=list(Mt=is.mito)) # Mt=is.mito, Rb=is.ribosomal
qc.frame <- colData(sce)
qc.frame$Lib.size <- sce$total_counts/1e3 # library size in thousands
qc.frame <- as.data.frame(qc.frame)

# generate plots for Lib.size, total_features, pct_amount_Mt for each selected category 
# histograms
qc.plots <- lapply(c("Lib.size", "total_features_by_counts", "pct_counts_Mt"), function(to.plot){ 
      if(to.plot=="Lib.size"){
        xlabel="Lib. sizes in thousands"
         }else{
           xlabel=to.plot}
  
      if(to.plot=="pct_counts_Mt"){
        madOrient="high"
      }else{
        madOrient="low"}

  plot.list <- lapply(annoFactors, function(separation){
      p <- ggplot(qc.frame, aes_string(to.plot, fill=separation))+ 
            geom_histogram(position="identity", alpha=0.5, bins=100, col="grey80") +
            geom_vline(aes(xintercept = median(na.omit(qc.frame[,to.plot])), col="median"), linetype=2)  +
            geom_vline(aes(xintercept = MAD(na.omit(qc.frame[,to.plot]), as.numeric(SHINYREPS_NMADS))[[madOrient]], col=madOrient), linetype=2) +
            #scale_color_manual(name = "Statistics", values = c(median = "blue", high="red")) +
            scale_fill_brewer(palette = "Set1") +
            #scale_x_log10() +
            xlab(xlabel) +
            ylab(paste0("# cells"))
          plot(p)
          return(p)
  })
  return(plot.list)
})

# violinplots
qc.plots.violin <- lapply(c("Lib.size", "total_features_by_counts", "pct_counts_Mt"), function(to.plot){ 
  if(to.plot=="Lib.size"){
    ylabel <- "Lib. sizes in thousands"
  }else{
    ylabel <- to.plot
  }
  
  plot.list <- lapply(annoFactors, function(separation){
    p <- ggplot(qc.frame, aes_string(separation,to.plot,color=separation))+
      geom_violin() +
      geom_quasirandom() +
      scale_fill_hue(l=40, c=40) +
      ylab(ylabel) +
      xlab(separation) +
      theme(axis.text.x=element_text(angle=45, vjust=1, hjust=1))
    plot(p)
    return(p)
  })
  return(plot.list)
})

```



**Top 2% biggest libraries (based on mapped reads on features)**

```{r top_1perc_cells}

# Output the cells with a library size in the top 2%
highest.lib.size <- colData(sce)[sce$total_counts > quantile(sce$total_counts, 0.98),
              c("total_counts",
               "total_counts_endogenous",
               "total_counts_Mt",
               group.vars)]
highest.lib.size <- highest.lib.size[order(highest.lib.size$total_counts, decreasing = T),]
kable(highest.lib.size,
      format="markdown",
      caption="2% cells with the highest library size")

```

## Count distribution per plate position

The plots below visualise the count data distribution (not log-transformed) via plate position for the categories 'total_features_by_counts_endogenous', 'total_counts' and 'pct_counts_Mt' indicated by spot size. 0-cell and 10-cell controls are indicated in blue and orange, respectively. Symbol shape is defined by custom grouping. For each category the plates are plotted row-wise with plate number in increasing order.

```{r total_counts_per_plate_position, fig.width=8}
##### parameters to set:
# define one category to be used for symbol shape in the plots.
annoFactors <- SHINYREPS_COLORBYFACTOR[1]
#####

sce$plate_position <- paste0(sce$row, sce$col) # column "plate_position" needed for plotPlatePosition
for (size in c("total_counts", "total_features_by_counts_endogenous", "pct_counts_Mt")) {
  cat(paste("Plotting", size, "as spot size\n"))
  plates <- list()
  for (p in as.character(sort(unique(sce$plate)))) {
        plates[[p]] <- scater::plotPlatePosition(sce[, sce$plate==p], colour_by="cells",size_by=size, shape_by=annoFactors[1],
                            by_exprs_values = "counts", theme_size = 10,  point_alpha = 0.6, point_size = 3, add_legend = F)  
  } # plotPlatePosition uses by_exprs_values = "logcounts" by default. But if not available, uses "counts" instead

  multiplot(plotlist=plates, layout=matrix(c(1:ceiling(length(plates))), ncol=2, byrow=TRUE))
}

```


### Correlation plots for different features

```{r corr_plots}
##### parameters to set:
# define one category to be used for symbol color in the plots.
annoFactors <- SHINYREPS_COLORBYFACTOR[1]
#####

qc.data <- colData(sce)
qc.data$Lib.Size.million <- qc.data$total_counts/1e6
qc.data <- as.data.frame(qc.data)

if(!is.null(annoFactors)) {annoFactors <- rlang::ensym(annoFactors)} 

lib.size.scatter <- ggplot(qc.data, aes(x=total_features_by_counts, 
                                        y=Lib.Size.million,
                                        color=!!annoFactors)) +
                             geom_point() +
                             scale_color_hue(l=40, c=60) +
                             ylab("Library size (millions)") +
                             xlab("Number of expressed genes")

mit.perc.scatter <- lib.size.scatter + aes(y=pct_counts_Mt) + ylab("% mitochondrial reads")
grid.arrange(lib.size.scatter,  mit.perc.scatter)

```

## Filtering out low quality cells

Filter criteria (see below) are selected according to quality control plots.

Summary of cells that don't pass the QC:

```{r dropout_cells, fig.height=8, fig.width=10}
##### parameter to set:
# define filtering criteria based on previous QC plots
type_of_threshold = SHINYREPS_TYPE_OF_THRESHOLD # either "absolute" or "relative" (i.e. using MAD)
threshold_total_counts_min = as.numeric(SHINYREPS_THRESHOLD_TOTAL_COUNTS_MIN)
threshold_total_counts_max = as.numeric(SHINYREPS_THRESHOLD_TOTAL_COUNTS_MAX)
threshold_total_features_by_counts_endogenous = as.numeric(SHINYREPS_THRESHOLD_TOTAL_FEATURES_BY_COUNTS_ENDOGENOUS)
threshold_pct_counts_Mt = as.numeric(SHINYREPS_THRESHOLD_PCT_COUNTS_MT)
NMADS = as.numeric(SHINYREPS_NMADS) # number of absolute deviations from median. Only relevant if type_of_threshold = "relative".
# define QC metrics to be used for PCA
qcmetrics = c("pct_counts_in_top_100_features", "total_features_by_counts", "pct_counts_feature_control", "total_features_by_counts_feature_control", "log10_total_counts_endogenous", "log10_total_counts_feature_control")
# define one category to be used for symbol shape in the plot.
annoFactors <- SHINYREPS_COLORBYFACTOR[1]
#####

if(!exists("sce.beforefiltMincount")) {sce.beforefiltMincount <- sce}

if(type_of_threshold=="relative") {
  # Before we apply QC filters we remove all the cells which hardly have any counts to avoid bias for relative thresholds
  min_readcount <- 100
  count.drop <- sce$total_counts < min_readcount
  cat("We drop ", sum(count.drop), " cells, because they have less than", min_readcount, "reads counted.\n") 
  sce <- sce[,!count.drop]
}

# apply thresholds
qc.drop <-  data.frame(row.names = colnames(sce)) # initialise

if(SHINYREPS_TYPE_OF_THRESHOLD=="absolute") {
  qc.drop$libsize <- (sce$total_counts < threshold_total_counts_min) | (sce$total_counts > min(threshold_total_counts_max, max(sce$total_counts), na.rm=T))
  qc.drop$feature_count <- sce$total_features_by_counts < threshold_total_features_by_counts_endogenous
  qc.drop$mito <- sce$pct_counts_Mt > threshold_pct_counts_Mt
  qc.drop$pass <- !apply(qc.drop, 1, any)
  qc.drop <- qc.drop[match(colnames(sce), rownames(qc.drop)), ]  # sort to match the 'sce' cell order

  if(!is.na(threshold_total_counts_max)) {threshold_total_counts_max <- paste("or >", threshold_total_counts_max)
        } else {threshold_total_counts_max <- NULL}   # prepare string for output
  
  qcfailed <- data.frame(criterion=c(paste("total counts <", threshold_total_counts_min, threshold_total_counts_max),
                                     paste("total features <", threshold_total_features_by_counts_endogenous),
                                     paste("% mitochondrial counts >", threshold_pct_counts_Mt),
                                     paste("remaining")), 
                         cell_count=c(sum(qc.drop$libsize), sum(qc.drop$feature_count), sum(qc.drop$mito), sum(qc.drop$pass)))
  kable(qcfailed, format="markdown", caption="QC filtering") %>% kable_styling()
  
  } else {
    if(SHINYREPS_TYPE_OF_THRESHOLD=="relative") {
     qc.drop$libsize <- isOutlier(sce$total_counts, nmads=NMADS, type="lower", log=TRUE)
     qc.drop$feature_count <- isOutlier(sce$total_features_by_counts, nmads=NMADS, type="lower", log=TRUE) 
     qc.drop$mito <- isOutlier(sce$pct_counts_Mt, nmads=NMADS, type="higher")
     qc.drop$pass <- !apply(qc.drop, 1, any)
     qc.drop <- qc.drop[match(colnames(sce), rownames(qc.drop)), ]  # sort to match the 'sce' cell order
     
     qcfailed <- data.frame(criterion=c(paste("total counts <", NMADS, "MAD"),
                                     paste("total features <", NMADS, "MAD"),
                                     paste("% mitochondrial counts >", NMADS, "MAD"),
                                     paste("remaining")), 
                         cell_count=c(sum(qc.drop$libsize), sum(qc.drop$feature_count), sum(qc.drop$mito), sum(qc.drop$pass)))
     kable(qcfailed, format="markdown", caption="QC filtering") %>% kable_styling()

  } else {stop("\ntype_of_threshold must be either 'absolute' or 'relative'")}
}

# plot QC metrics as PCA
cat("The quality metrics are summarized in a PCA")
plotPCAfromQCmetrics(sce, metrics=qcmetrics, anno=annoFactors, qc.drop=qc.drop)


```


Removed samples/cells:

```{r remove_low_qual_cells_and_control_wells}
##### parameters to set:
# define up to 2 categories to be used for output tables.
SHINYREPS_COLORBYFACTOR <- SHINYREPS_COLORBYFACTOR[SHINYREPS_COLORBYFACTOR != "cells"] # column "cells" not needed anymore
annoFactors <- SHINYREPS_COLORBYFACTOR
#####


# output overview of remaining cells and table of removed cells
j <- c("total_counts", "total_features_by_counts", "pct_counts_Mt", annoFactors) 
x <- cbind(qc.drop, colData(sce)[match(rownames(qc.drop), rownames(colData(sce))), j])
# knitr::kable(as.data.frame(apply(qc.drop, 2, sum)), col.names="cells", caption="")
x$pct_counts_Mt   <- round(x$pct_counts_Mt  , digits=2) 
x <- as.data.frame(x)
DT::datatable(x[!x$pass, ])

# filter out cells failing QC
if(length(qc.drop$pass) == ncol(sce)) { # if block is executed multiple times
    sce <- sce[, qc.drop$pass]
}

# overview of remaining cells after QC filtering
colnames_table <- if(length(annoFactors)==1) {c(annoFactors, "cell count")} else {NA}
knitr::kable(table(colData(sce)[, annoFactors, drop=F]), col.names=colnames_table, caption="Amount of cells remaining after QC filtering") %>% kable_styling()

# filter out control wells with 0 or 10 cells
sce <- sce[, colData(sce)$cells == "1c"]
sce$wells <- gsub("_S[0-9]+$", "", gsub("_1c", "", colnames(sce)))

# overview of remaining cells after removing control wells 
knitr::kable(table(colData(sce)[, annoFactors, drop=F]), col.names=colnames_table, caption="Amount of cells remaining after filtering for control wells (0c/10c)") %>% kable_styling() 

```


### Replotting the PCA after filtering

```{r pca2, fig.height=8, fig.width=10}
##### parameters to set:
# define up to 2 categories to be indicated in the PCA plot by color and shape.
annoFactors <- SHINYREPS_COLORBYFACTOR
#####

try(plotPCAfromQCmetrics(sce, qcmetrics, anno=annoFactors))
```



## Top most highly expressed genes

The plot below shows the most highly expressed genes (based on un-normalized mean counts). This should generally be dominated by constitutively expressed transcripts, such as those for ribosomal or mitochondrial proteins. All mitochondrial genes are marked as "Feature control". The color represents the total number of expressed genes in the respective sample/cell.

```{r highly_expr_genes, fig.height=6, fig.width=6}
# use tmp.sce for plotting with different rownames
tmp.sce <- sce
rownames(tmp.sce) <- rowData(sce)$SYMBOL
fontsize <- theme(axis.text=element_text(size=6), axis.title=element_text(size=10))
plotHighestExprs(tmp.sce, n=50) + fontsize
rm(tmp.sce)
```


## Filtering out low abundance genes

Average counts per gene should correlate with number of cells expressing it (see plot below). 
Low abundance genes are likely to be dominated by drop-out events (Poisson noise in different cells). They do not contain enough information for statistical inference, and may compromise accuracy of continuous approximations when fitting the data (edgeR biological coefficient of variation (BCV) estimation). Here, we filter out genes with no expression in 95% of the cells.

```{r remove_low_abundance_genes}
avg.counts <- calcAverage(sce, use_size_factors=F) 
expressed.cells <- nexprs(sce, byrow=TRUE)   # number of cells expressing the gene
smoothScatter(log10(avg.counts), expressed.cells,
              xlab=expression("Log10 average count"), ylab= "Number of expressing cells")
# is.ercc <- isSpike(sce, type="ERCC")
# points(log10(ave.counts[is.ercc]), numcells[is.ercc], col="red", pch=16, cex=0.5)

# filtering for low abundance genes
genes2keep <- expressed.cells > ceiling(.05 * ncol(sce))
cat(sum(genes2keep), "of", length(genes2keep), "genes remain after filtering for low abundance genes.\n")
sce <- sce[genes2keep, ]

# remove feature controls
cat("Remove", sum(rowData(sce)$is_feature_control), "feature controls from dataset.\n")
sce <- sce[!rowData(sce)$is_feature_control,]

```



# Normalization of cell-specific biases
Normalization is required to eliminate these cell-specific differences in capture efficiency, prior to downstream quantitative analyses.

## Normalization for library size

Size factors can be calculated with DESeq2 or edgeR, but these methods do not work well with single-cell data due to the dominance of low and zero counts. To overcome this, we use the method from Lun et al. (2016) implemented in the *cran* package, which pools counts from many cells to estimate the size factors and to finally deconvolute them to cell-specific factors. Finally, normalized log2-expression values are computed for each endogenous gene using the appropriate size factors.

```{r size_factors_and_normalize, warning=FALSE, message=FALSE}
##### parameters to set:
# define up to 2 categories to be indicated in the plot by color and shape.
annoFactors <- SHINYREPS_COLORBYFACTOR
#####

high.ave <- rowData(sce)$ave.count >= 0.1

cat("\nSummary size factors")
sce <- computeSumFactors(sce, sizes=seq(21, 101, 5))
summary(sizeFactors(sce))

# cat("\nSummary size factors for ERCC spike-ins\n")
# sce <- computeSpikeFactors(sce, type="ERCC", general.use=F) ## general.use=TRUE use spike-ins for normalisation
# summary(sizeFactors(sce, "ERCC"))

# plot size factors 
dotcol <- annoFactors[1]
dotcol <- rlang::ensym(dotcol)
if(length(annoFactors)>=2) { 
  dotshape <- annoFactors[2]
  dotshape <- rlang::ensym(dotshape)
} else {
  dotshape <- NULL
}

to.plot <- data.frame(sizeFactors=sizeFactors(sce),
                 total_counts=sce$total_counts/1e6,
                 colData(sce)[,annoFactors, drop=F])
ggplot(to.plot, aes(x=sizeFactors, 
                    y=total_counts,
                    color=!!dotcol, shape=!!dotshape))+
  geom_point()+ ylab("Library size (millions)") + scale_color_hue(l=40, c=40)

# normalise
sce <- scater::normalize(sce) # adds normalized logcounts matrix
```


### Normalized log2-expression of top50 genes with highest average expression

```{r plot_violin_top50, fig.width=12, fig.height=10}
##### parameters to set:
# define up to 2 categories to be indicated in the plot by color and shape.
annoFactors <- SHINYREPS_COLORBYFACTOR
#####

# use tmp.sce to change rownames to SYMBOL for the plot
tmp.sce <- sce
rownames(tmp.sce) <- rowData(sce)$SYMBOL

# re-calculate averages, using newly determined size factors
ave.counts.new <- calcAverage(tmp.sce)
rowData(tmp.sce)$ave.count.new <- ave.counts.new

top.sce.aver.size <- head(rowData(tmp.sce)[order(rowData(tmp.sce)$ave.count.new,decreasing=TRUE),],50)

dotcol <- annoFactors[1]
if(length(annoFactors)>=2) { 
    dotshape <- annoFactors[2]
} else {
    dotshape <- NULL
}

plotExpression(tmp.sce, top.sce.aver.size$SYMBOL[1:25], colour_by=dotcol, shape_by = dotshape) + ggtitle("highest avg. expression 1:25")
plotExpression(tmp.sce, top.sce.aver.size$SYMBOL[26:50], colour_by=dotcol, shape_by = dotshape) + ggtitle("highest avg. expression 26:50")
```


## Checking for confounding factors 

### Classification and normalization of cell cycle phase

We use the prediction method described by Scialdone et al. (2015) to classify cells into cell cycle phases based on the gene expression data. Pre-trained classifiers are available in scran for human and mouse data. Cells are classified as being in G1 phase if the G1 score is above 0.5 and greater than the G2/M score; in G2/M phase
if the G2/M score is above 0.5 and greater than the G1 score; and in S phase if neither score is above 0.5. 

```{r find_cell_cycle, fig.width=8, fig.height=8}
##### parameters to set:
# define organism (either "human" or "mouse")
org <- SHINYREPS_ORG
# define one category to be used for symbol colour in the plot.
annoFactors <- SHINYREPS_COLORBYFACTOR[1]
#####

if(!exists("sce.no.ccp")) {sce.no.ccp <- sce}
set.seed(100)

# load gene.pairs: "mouse_cycle_markers.rds" or "human_cycle_markers.rds"
gene.pairs <- readRDS(system.file("exdata", paste0(org, "_cycle_markers.rds"), package="scran")) # mouse_cycle_markers.rds

# determine cell cycle phase:
# in "human_cycle_markers.rds" ensembl names are without the ".number" at the very end, but 
# the rownames of our sce object include this. ---> remove .number
assignments <- cyclone(sce, pairs=gene.pairs, gene.names=sub("\\..*$","",rowData(sce)$ENSEMBL), assay.type="logcounts") 
sce$phases <- assignments$phases
cat(sum(is.na(sce$phases)), "out of", length(sce$phases),
    "cells could not be assigned to a cell cycle phase.", fill=TRUE)
if(any(!is.na(sce$phases))) {
    cat("\ncell cycle phases")
    table(sce$phases)
    plot(0, xlim=c(0, 1), ylim=c(0, 1), type="n", xlab="G1 score", ylab="G2/M score")
    points(assignments$scores$G1, assignments$scores$G2M, col=scales::alpha(pal[factor(colData(sce)[,annoFactors])], .5) )   # pch=16
    # abline(h=.5, v=.5, lty=2, col="red")
    arrows(0.5,0.5,1,1, length=0, col="red", lty=2)
    arrows(0,0.5,0.5,0.5, length=0, col="red", lty=2)
    arrows(0.5,0,0.5,0.5, length=0, col="red", lty=2)

    text(x=c(.25, .75, .25, .75), y=c(.75, .75, .25, .25), labels=c("G2", " ", "S", "G1"))
    legend("bottomleft", col=c(pal[1:4]), pch=16, legend=c(levels(factor(colData(sce)[,annoFactors]))))
}

## Info: adjustment for cell cycle phases (https://osca.bioconductor.org/cell-cycle-assignment.html)
  # In routine scRNA-seq analyses it is not recommended to adjust for cell cycle. Compared to other differences 
  # between cell types it is a minor factor of variation and any attempt at removal would also need to assume 
  # that the cell cycle effect is orthogonal to other biological processes. For example, regression would 
  # potentially remove interesting signal if cell cycle activity varied across clusters or conditions. 
  # Cell cycle adjustment may be performed on an as-needed basis in populations with clear cell cycle effects.
```




### Identify explanatory variables 

We check whether there are technical factors that contribute substantially to the heterogeneity of gene expression. If so, the factor may need to be regressed out to ensure that it does not inflate the variances or introduce spurious correlations.

```{r explanatorx_variables, echo=F, error=F, warning=F, message=F}
##### parameters to set:
# define potential explanatory variables to be tested for their effect
explanatoryVariables <- group.vars
var2color <- group.vars
#####

plotExplanatoryVariables(sce, variables=explanatoryVariables, exprs_values = "logcounts") +    
    ggtitle("Explanatory variables")

cat("\nPCA plots of top 500 genes colored by potential confounder variables\n\n")  
lapply(var2color, function(x) {
  set.seed(100) # plotPCASCE # plotTSNE # plotUMAP
  scater::plotPCASCE(sce, rerun=T, ncomponents = 4, run_args= list(exprs_values="logcounts"), point_size=1, colour_by=x) + 
    ggtitle(paste("PCA plots colored by", x))
  })

```



```{r limma_blocking_confounder, eval=T}
# use limma for adjustment of cell cycle phase and confounder variable
library(limma)
var2adjust <- NULL
adjust_ccp <- F # adjust for cell cycle phase 
######

if(adjust_ccp || !is.null(var2adjust)) {
  
  cat("## Adjust for confounder variables")

  if(adjust_ccp) {
      cat("We can account for possible cell cycle effect on downstream analysis, using the G1 and G2M assignment scores as a continuous blocking factor to estimate the variance. This is more graduated than using a strict assignment of each cell to a specific phase, as the magnitude of the score considers the uncertainty of the assignment. The phase covariates in the design matrix will absorb any phase-related effects on expression such that they will not affect estimation of the effects of other experimental factors. Any additionial batch variable can be included via the 'batch'-argument of the 'removeBatchEffect' function.")
      
      
    # filter out cells with undetermined cell cycle
     scoresFilteredNA <- assignments$scores[!is.na(assignments$phases), ] # NA entries are not used
     cat(sum(is.na(sce$phases)), "cells not assigned to a cell cycle phase are removed from the dataset.\n")
     sce <- sce[,!is.na(sce$phases)] 
    
    sce.no_block <- sce
    sce.no_block$G1score <- sce$G1score <- scoresFilteredNA$G1
    sce.no_block$G2Mscore <- sce$G2Mscore <- scoresFilteredNA$G2M
    
    # adjust for cell cycle phase by modelling G scores
    design <- model.matrix(~ G1 + G2M, scoresFilteredNA)
}


  if(!is.null(var2adjust)) { # includes batch effect
    
        if(adjust_ccp) {# includes cell cycle phase
          
            cat("\nVariable to adjust for (additional to cell cycle phase):", var2adjust)
            set.seed(100)
            assay(sce, "corrected") <- removeBatchEffect(logcounts(sce), batch=colData(sce)[, var2adjust], covariates=design[,-1]) 
            
            # create final_var2adjust (factor composed from var2adjust and optionally cell cycle phase)
            sce$final_var2adjust <- as.factor(paste(colData(sce)[,var2adjust], sce$phases, sep="_"))
            
             } else {
                  cat("\nNo adjustment for cell cycle phase applied.")
                  cat("\nVariable to adjust for:", var2adjust)
                set.seed(100)
                assay(sce, "corrected") <- removeBatchEffect(logcounts(sce), batch=colData(sce)[, var2adjust]) 
                
                # create final_var2adjust 
                sce$final_var2adjust <- as.factor(colData(sce)[,var2adjust])
            }
    
  } else { # no batch var
    cat("\nAdjustment for cell cycle scores only (no additional batch variable)")
    set.seed(100)
    assay(sce, "corrected") <- removeBatchEffect(logcounts(sce), covariates=design[,-1]) # scores as covariates
    
    # create final_var2adjust 
    sce$final_var2adjust <- as.factor(sce$phases)
  }


  ## switch assay channel for downstream processing
  assay(sce, "logcounts_not_adjusted") <- assay(sce, "logcounts")
  assay(sce, "logcounts") <- assay(sce, "corrected")
  assay(sce, "corrected") <- NULL
  
  # plotting
  if(adjust_ccp) {
      cat("\nPCA plot of top 500 genes colored by G1 and G2M score\n")
      set.seed(100)
      out1 <- plotPCASCE(sce.no_block, ncomponents=2, rerun=T, # use_dimred="PCA", 
               colour_by="G1score", size_by="G2Mscore") + ggtitle("Before ccp adjustment")
      # After blocking on the phase scores
      set.seed(100)
      out2 <- plotPCASCE(sce, ncomponents=2, rerun=T, # use_dimred="PCA", 
                colour_by="G1score", size_by="G2Mscore") + ggtitle("After ccp adjustment")
      multiplot(out1, out2, cols=2)
  }

} else {
  final_var2adjust <- NULL
}
```


```{r batchelor_blocking_confounder, echo=F, error=F, warning=F, message=F, eval=F}
library(batchelor)

##### parameters to set:
var2adjust <- NULL
adjust_ccp <- F # adjust for cell cycle phase (default = FALSE)
#####

if(adjust_ccp || !is.null(var2adjust)) {
  
  cat("## Adjust for confounder variables")

cat("Counfounder variables like batch effects can be adjusted using the mutual nearest neighbors method (MNN). The effect from cell cycle phase can be removed from the dataset (if necessary) via linear regression treating each phase as a separate batch. If both a confounder variable and cell cycle shall be adjusted for, a new categorical variable is composed from both factors and used for MNN correction.")

## Info: Batchelor functions for batch correction
# (http://bioconductor.org/packages/devel/bioc/vignettes/batchelor/inst/doc/correction.html#3_mutual_nearest_neighbors)
# correctExperiments() # Apply a correction to multiple SingleCellExperiment objects, while also combining the 
# assay data and column metadata for easy use. batchCorrect does the correction inside this function
# batchCorrect() # A common interface for single-cell batch correction methods
# fastMNN() # For scRNA-seq data, fastMNN() tends to be both faster and better at achieving a satisfactory merge than mnnCorrect()
# mnnCorrect() # The original method described by Haghverdi et al. (2018), is mainly provided here for posterityâ€™s sake
#     The MNN-corrected values can be used for further correction with mnnCorrect().
#     This is useful in nested experimental designs involving multiple batches within each of multiple studies. 
#     Users should set cos.norm.in=FALSE and cos.norm.out=FALSE when supplying mnnCorrect() with MNN-corrected values. 
#     This ensures that the cosine normalization is only applied once, during the first round of MNN correction.
#     (https://bioc.ism.ac.jp/packages/3.7/workflows/vignettes/simpleSingleCell/inst/doc/work-5-mnn.html).
#     MNN-corrected values are generally not suitable for differential expression (DE) analyses.
# rescaleBatches() # conceptually equivalent to running removeBatchEffect() from limma with no covariates other than the batch. 
#     While this method is fast and simple, it makes the strong assumption that the population composition of each batch is the same. 
#     This is usually not the case for scRNA-seq experiments in real systems that exhibit biological variation. 
#     Thus, rescaleBatches() is best suited for merging technical replicates of the same sample, e.g., that have 
#     been sequenced separately.
# regressBatches() # Alternative to rescaleBatches(), a more direct linear regression of the batch effect. 
#     This does not preserve sparsity but uses a different set of tricks to avoid explicitly creating a dense matrix, 
#     specifically by using the ResidualMatrix class from the BiocSingular package.
# multiBatchNorm() # Differences in sequencing depth between batches are an obvious cause for batch-to-batch differences. 
#     These can be removed by multiBatchNorm(), which downscales all batches to match the coverage of the least-sequenced batch. 


# store unmodified logcounts channel for downstream processing
 assay(sce, "logcounts_not_adjusted") <- assay(sce, "logcounts")

if(adjust_ccp) {
 # filter out cells with undetermined cell cycle
 cat(sum(is.na(sce$phases)), "cells not assigned to a cell cycle phase are removed from the dataset.\n")
 sce <- sce[,!is.na(sce$phases)] 
}
 
 
if(is.null(var2adjust) & adjust_ccp) {
  
  # adjust cell cycle phase as batch effect (linear regression)
  cat("\nadjusting for cell cycle phase using linear regression.")
  sce$final_var2adjust <- as.factor(sce$phases)
  set.seed(100)
  sce.adjust <- batchelor::rescaleBatches(sce, batch=sce$final_var2adjust, assay.type = "logcounts")
  assay(sce, "logcounts") <- assay(sce.adjust, "corrected")

  } else {
  
    if(!is.null(var2adjust)) {
      
        # filter out cells with missing value for var2adjust
        cat(sum(is.na(colData(sce)[,var2adjust])), "cells with missing value for", var2adjust, "are removed from the dataset.\n")
        sce <- sce[,!is.na(colData(sce)[,var2adjust])] 
      
        # create final_var2adjust (factor composed from var2adjust and optionally cell cycle phase)
        sce$final_var2adjust <- if(adjust_ccp) {paste(colData(sce)[,var2adjust], sce$phases, sep="_")} else {colData(sce)[,var2adjust]}
        sce$final_var2adjust <- as.factor(sce$final_var2adjust)
        
        # adjust for final_var2adjust as batch effect (mutual nearest neighbours, MNN)
        cat("\nadjusting for", var2adjust, if(adjust_ccp) {"and cell cycle phase"}, "using MNN")
        set.seed(100)
        sce.adjust <- batchelor::fastMNN(sce, batch=sce$final_var2adjust, assay.type = "logcounts")
        reducedDim(sce, "corrected") <- reducedDim(sce.adjust, "corrected") # will be compared with PCA slot
        assay(sce, "logcounts") <- assay(sce.adjust, "reconstructed") # class LowRankMatrix (not compatible with trendVar) 
        
    } else {
      cat("\nno adjustment applied")
    }
  }
} else {
  final_var2adjust <- NULL
}
```


```{r explanatory_variables_2, echo=F, error=F, warning=F, message=F, fig.width=8, fig.height=6, results='asis', eval=(adjust_ccp || !is.null(var2adjust))}

if(!is.null(var2adjust)) {
  cat("\nRe-plot explanatory variables after adjustment\n")
  plotExplanatoryVariables(sce, variables=explanatoryVariables, exprs_values = "logcounts") +
    ggtitle("Explanatory variables after adjustment")
}

cat("\nPCA plots of top 500 genes colored by potential confounder variables after correction\n\n")
lapply(var2color, function(x) {
  set.seed(100) # plotPCASCE # plotTSNE # plotUMAP
  scater::plotPCASCE(sce, rerun=T, ncomponents = 4, run_args= list(exprs_values="logcounts"), point_size=1, colour_by=x) +
        ggtitle(paste("PCA plots after adjustment colored by", x))
  })

```



# Identifying highly variable genes (HVGs)

Estimation of the variance in expression for each gene, followed by decomposition of the variance into biological and technical components. The technical component would be estimated by fitting a mean-variance trend to the spike-in transcripts using the trendVar function. The biological component of the variance can then be calculated by subtracting the technical component from the total variance of each gene with the decomposeVar function.

```{r trendVar, echo=F, error=F, warning=F, message=F}
# estimate highly variable genes
decVar.list <- lapply(setNames(nm="all"), function(type){ # no subsetting of cell groups
    sce.tmp <- sce
    var.fit <- trendVar(sce.tmp, method="loess",
                        loess.args=list(span=0.3),
                        use.spikes=F)
    decVar <- decomposeVar(sce.tmp, var.fit)
    return(list(decVar=decVar, var.fit=var.fit))
})
```


The scatter plot below shows the mean log expression vs. the total variance, while the blue line shows the technical part of the variance.

```{r plot_trendVar, echo=F, error=F, warning=F, message=F}
# plot mean log expression vs. the total variance
for(to.plot in names(decVar.list)){
    decVar.tmp  <- decVar.list[[to.plot]]$decVar 
    var.fit.tmp <- decVar.list[[to.plot]]$var.fit 
    plot(decVar.tmp$mean, decVar.tmp$total, pch=16, cex=0.6, col="#00000050",
         xlab="Mean log-expression", ylab="Variance of log-expression",
         main=paste("Plot for", to.plot, "cells"))
    #cur.spike <- isSpike(sce, type="ERCC")
    #points(decVar.tmp$mean[cur.spike], decVar.tmp$total[cur.spike], col="red", pch=16)
    o <- order(decVar.tmp$mean)
    lines(decVar.tmp$mean[o], decVar.tmp$tech[o], col="dodgerblue", lwd=2)
}

```

HVGs are defined as genes with biological components that are significantly greater than zero at a false discovery rate (FDR) of 5%. We only consider a gene to be a HVG if it has a biological component greater or equal than 0.5 (assuming the log-expression values are normally distributed with a variance of 0.5, this means an average difference of 2-fold between 2 cells).

The number of HVGs found is:

```{r get_hvg, fig.width=11, echo=F, error=F, warning=F, message=F}
##### parameters to set:
# define up to 2 categories to be indicated in the plot by color and shape.
annoFactors <- SHINYREPS_COLORBYFACTOR
#####


# count number of HVGs
hvg.list <- lapply(names(decVar.list), function(x){
    decVar <- decVar.list[[x]]$decVar
    hvg <- decVar[which(decVar$FDR <= 0.05 & decVar$bio >= 0.5), ]
    hvg <- hvg[order(hvg$bio, decreasing=TRUE), ]
    cat("Number of HVGs for", x, "cells:", nrow(hvg), "\n")
    cat("\n")
    return(hvg)
})
names(hvg.list) <- names(decVar.list)

# plot top 10 HVGs
if(length(annoFactors)>2) {stop("\nno more than 2 categories allowed in 'anno'.")}
dotcol <- annoFactors[1]
if(length(annoFactors)==2) { 
    dotshape <- annoFactors[2]
} else {
    dotshape <- NULL
}

for(type in names(hvg.list)){
    hvg <- hvg.list[[type]]
    if(nrow(hvg) == 0) {
        cat("\nNo highly variable genes found passing the thresholds.\n")
        next
    }

    if(type=="all"){
      tmp.sce <- sce
    }else{
      tmp.sce <- sce[,sce$type==type] # if desired to subset this plot for subgroups of cells
    }
    rownames(tmp.sce) <- rowData(tmp.sce)$SYMBOL
    if(type=="all"){
        print(
          plotExpression(tmp.sce, 
                         na.omit(gtf$gene_name[match(rownames(hvg), gtf$gene_id)])[1:10],
                         colour_by=dotcol,
                         shape_by=dotshape) + 
            ggtitle("Top 10 HGVs for all cells") +
            scale_color_hue(c=40, l=50, name= dotcol) +
            facet_grid(~colour_by)
        )
    }else{ # if it desired later to subset this plot for subgroups of cells
        print(
          plotExpression(tmp.sce, 
                         na.omit(gtf$gene_name[match(rownames(hvg), gtf$gene_id)])[1:10],
                         colour_by=dotcol,
                         shape_by=dotshape) +
               scale_fill_hue(c=40, l=50, name=dotcol)+ 
               ggtitle(paste("10 highest HGVs for", type))
        )   
    }
}
```

## Identify correlated HVGs

Another useful procedure is to identify the HGVs that are highly correlated with one another. This distinguishes between HVGs caused by random noise and those involved in driving systematic differences between sub-populations. Correlations between genes are quantified by computing Spearman's rho, which accomodates non-linear relationships in the expression value. Gene pairs with significantly large positive or negative values of rho are identified using the correlatePairs function. 

```{r get_hvg_corr, echo=F, error=F, warning=F, message=F}
# Identify correlated HVGs
hvg.cor.list <- lapply(names(hvg.list), function(type){
  if(type=="all"){
    tmp.sce <- sce
  }else{
    tmp.sce <- sce[, sce$type == type]
  }
  set.seed(100)
  hvg.cor <- correlatePairs(tmp.sce, subset.row=rownames(hvg.list[[type]] ))

  hvg.cor$gene1SYMBOL <- gtf$gene_name[match(hvg.cor$gene1, gtf$gene_id)]
  hvg.cor$gene2SYMBOL <- gtf$gene_name[match(hvg.cor$gene2, gtf$gene_id)]
  hvg.cor$sig.cor = hvg.cor$FDR <= .05
  top.hvg    <- unique(c(hvg.cor$gene1[hvg.cor$sig.cor], hvg.cor$gene2[hvg.cor$sig.cor]))
  write.table(file=paste0("report_files/hvg_cor_", type, ".tsv"), hvg.cor, sep="\t", quote=FALSE, row.names=FALSE)
  cat(paste("number of genes part of min 1 significant correlation:", length(top.hvg), "\n"))

  return(list(hvg.cor=hvg.cor, sig.cor=hvg.cor$sig.cor, top.hvg=top.hvg))
})
names(hvg.cor.list) <- names(hvg.list)

```


# Using correlated HVGs for further data exploration

## PCA based on significantly correlated HVGs

```{r hvg_pca, echo=F, error=F, warning=F, message=F, fig.height=5, fig.width=10}
##### parameters to set:
# define up to 2 categories to be indicated in the plot by color and shape.
annoFactors <- SHINYREPS_COLORBYFACTOR  
ncomponents=2  
exprs_values="logcounts"
#####

# HVG 
 set.seed(100)
  sce <- scater::runPCA(sce, name = "PCA", ncomponents=ncomponents, feature_set=hvg.cor.list[["all"]]$top.hvg, exprs_values=exprs_values)

# all genes
 set.seed(100)
  sce <-scater::runPCA(sce, name = "PCA_allgenes", ncomponents=ncomponents, ntop = nrow(sce), exprs_values=exprs_values)
  
if(length(annoFactors)>2) {stop("\nno more than 2 categories allowed in 'annoFactors'.")}
  dotcol <- annoFactors[1]
  if(length(annoFactors)==2) { 
    dotshape <- annoFactors[2]} else {
      dotshape <- NULL}

# plot PCA
  multiplot(plotlist=mclapply(grep("PCA", reducedDimNames(sce), value=T), function(d) {
 
      scater::plotReducedDim(sce, dimred = d, 
        by_exprs_values = exprs_values,
        colour_by=dotcol, shape_by = dotshape, point_size=2) +  
        ggtitle(paste("PCA plot of", if(grepl("allgenes", d)) {"all genes"} else {"cor HVGs"}))

  }, mc.cores=1), cols=2)
  
```

## Identification of sub-populations with t-SNE 

Another widely used approach is the t-stochastic neighbour embedding (t-SNE) method (Van der Maaten & Hinton, 2008). t-SNE  tends  to  work  better  than  PCA  for  separating  cells  in  more  diverse  populations.  This  is  because  the former can directly capture non-linear relationships in high-dimensional space, whereas the latter must represent them (suboptimally) as linear components. However, this improvement comes at the cost of more computational effort and complexity. In particular, 
t-SNE is a stochastic method, so users should run the algorithm several times to ensure that the results are representative, and then set a seed to ensure that the chosen results are reproducible. It is also advisable to test different settings of the â€œperplexityâ€ parameter as this will affect the distribution of points in the low-dimensional space.

The perplexity can be interpreted as a smooth measure of the effective number of neighbors. The performance of SNE is fairly robust to changes in the perplexity, and typical values are between 5 and 50. 

A major weakness of t-SNE is that the cost function is not convex, as a result of which several optimization parameters need to be chosen. The constructed solutions depend on these choices of optimization parameters and may be different each time t-SNE is run from an initial random configuration of map points. But the developers of t-SNE have demonstrated that the same choice of optimization parameters can be used for a variety of different visualization tasks and found that the quality of the optima does not vary much from run to run. Thus, t-SNE should not be rejected in favor of methods that lead to convex optimization problems but produce noticeably worse visualizations. A local optimum of a cost function that accurately captures what is wanted in a visualization is often preferable to the global optimum of a cost function that fails to capture important aspects of what is wanted.

Identification of sub-populations with t-SNE, trying different perplexity values to identify structure based on the sig. correlated HVG genes:

```{r TSNE_plot_various_perplexities, fig.height=10, fig.width=10, echo=F, error=F, warning=F, message=F}
##### parameters to set:
# define up to 2 categories to be indicated in the plot by color and shape.
annoFactors <- SHINYREPS_COLORBYFACTOR
ncomponents=2  
perplexity=c(25, 50)
exprs_values="logcounts"
#####

if(length(annoFactors)>2) {stop("\nno more than 2 categories allowed in 'annoFactors'.")}
  dotcol <- annoFactors[1]
  if(length(annoFactors)==2) { 
    dotshape <- annoFactors[2]} else {
      dotshape <- NULL}

  # t-SNE without using pre-existing PCA results as input
    for (p in perplexity) {
    set.seed(100) # HWE
    sce <- scater::runTSNE(sce, name = paste0("TSNE_p", p), ncomponents=ncomponents, 
                           perplexity=p, dimred = NULL, feature_set=hvg.cor.list[["all"]]$top.hvg, exprs_values=exprs_values)
    set.seed(100) # all genes
    sce <- scater::runTSNE(sce, name = paste0("TSNE_p", p, "_allgenes"), ncomponents=ncomponents, 
                           perplexity=p, dimred = NULL, ntop = nrow(sce), exprs_values=exprs_values)
    }
 
  # plot TSNE
  multiplot(plotlist=mclapply(grep("TSNE", reducedDimNames(sce), value=T), function(d) {
 
      scater::plotReducedDim(sce, dimred = d, 
        by_exprs_values = exprs_values,
        colour_by=dotcol, shape_by = dotshape, point_size=2) +  
        ggtitle(paste("TSNE plot of", if(grepl("allgenes", d)) {"all genes"} else {"cor HVGs"}, "with perplexity", gsub("^.*p", "", d)))

  }, mc.cores=1), cols=2)
  
```


## Identification of sub-populations with UMAP 

Identification of sub-populations with UMAP plots (uniform manifold approximation and projection) to identify structure based on the sig. correlated HVG genes:

```{r UMAP_plot, fig.height=5, fig.width=10, echo=F, error=F, warning=F, message=F}
##### parameters to set:
# define up to 2 categories to be indicated in the plot by color and shape.
annoFactors <- SHINYREPS_COLORBYFACTOR
ncomponents=2  
n_neighbors = 15
exprs_values="logcounts"
#####

if(length(annoFactors)>2) {stop("\nno more than 2 categories allowed in 'annoFactors'.")}
  dotcol <- annoFactors[1]
  if(length(annoFactors)==2) { 
    dotshape <- annoFactors[2]} else {
      dotshape <- NULL}


# HVGs
 set.seed(100)
  sce <- scater::runUMAP(sce, name = "UMAP", ncomponents=ncomponents, n_neighbors = n_neighbors, 
                         dimred = NULL, feature_set=hvg.cor.list[["all"]]$top.hvg, exprs_values=exprs_values)  
# all genes  
 set.seed(100)
  sce <- scater::runUMAP(sce, name = "UMAP_allgenes", ncomponents=ncomponents, n_neighbors = n_neighbors, 
                         dimred = NULL, ntop = nrow(sce), exprs_values=exprs_values)

 # plot UMAP 
   multiplot(plotlist=mclapply(grep("UMAP", reducedDimNames(sce), value=T), function(d) {
 
      scater::plotReducedDim(sce, dimred = d, 
        by_exprs_values = exprs_values,
        colour_by=dotcol, shape_by = dotshape, point_size=2) +  
        ggtitle(paste("UMAP plot of", if(grepl("allgenes", d)) {"all genes"} else {"cor HVGs"}))

  }, mc.cores=1), cols=2)
 
```


# Clustering

## Defining clusters with SC3

Single-Cell Consensus Clustering (SC3) is a tool for unsupervised clustering of scRNA-seq data. SC3 achieves high accuracy and robustness by consistently integrating different clustering solutions through a consensus approach. Genes used for clustering are filtered based on gene dropout rates of the counts matrix. SC3 recommends a number of clusters to be used for clustering.

```{r sc3, results="asis", eval=TRUE}
library(SC3)
library(pkgmaker)

# uses by default logcounts?

if(!exists("sce.no.sc3")) {sce.no.sc3 <- sce}

rowData(sce)$feature_symbol <- rowData(sce)$SYMBOL # columns needed for SC3

  set.seed(100)
    sce <- sc3_prepare(sce, gene_filter=T, n_cores=8, rand_seed = 100) # 
  set.seed(100)
    sce <- sc3_estimate_k(sce) # metadata(sce)$sc3$k_estimation 12
    nr_clusters <- metadata(sce)$sc3$k_estimation
    cat("Number of selected clusters:", nr_clusters, "\n")
  set.seed(100)
    sce <- sc3_calc_dists(sce) # repeated runs not identical despite of setting rand_seed in sc3_prepare
  set.seed(100)
    sce <- sc3_calc_transfs(sce)
  set.seed(100)
    sce <- sc3_kmeans(sce, ks=nr_clusters) 
  set.seed(100)
    sce <- sc3_calc_consens(sce) # assigns clusters in colData(sce)
 
# metadata(sce)$sc3
# colData(sce)[ , grep("sc3_", colnames(colData(sce))), drop=F]   
# rowData(sce)[ , grep("sc3_", colnames(rowData(sce))), drop=F]

# print number of filtered genes
    cat("\n", sum(rowData(sce)$sc3_gene_filter), "of", nrow(sce), "genes were used for clustering")
        
# print cluster assignment
  knitr::kable(table(colData(sce)[, paste0("sc3_", nr_clusters, "_clusters")]), caption="cells per cluster", col.names=c("Cluster", "Freq")) %>% kable_styling() 

# store cluster assignments 
dir_sc3 <- file.path("report_files", "cluster", "SC3")
if (!file.exists(dir_sc3)) {dir.create(dir_sc3, recursive=T) }
write.table(colData(sce)[,c(group.vars, grep("sc3_", colnames(colData(sce)), value=T))], file =file.path(dir_sc3, paste0("SC3_", nr_clusters, "_cluster.txt")), sep="\t", quote = F, row.names = F)


### plot SC3 clustering
annoFactors <- c(paste0("sc3_", nr_clusters, "_clusters"), "group")  

if(length(annoFactors)>2) {stop("\nno more than 2 categories allowed in 'annoFactors'.")}
  dotcol <- annoFactors[1]
  if(length(annoFactors)==2) { 
    dotshape <- annoFactors[2]} else {
      dotshape <- NULL}


### plot clusterings (separated for PCA, TSNE and UMAP)
  for (d in c("PCA", "TSNE", "UMAP")) {
        plot_Items <- grep(d, reducedDimNames(sce), value=T)
        
      # dim reduction plots
        plot_cluster <- mclapply(plot_Items, function(i) {
                  scater::plotReducedDim(sce, 
                    dimred = i,
                    colour_by=dotcol, shape_by = dotshape, point_size=1.5) +
                    scale_color_discrete(name="cluster") +
                    guides(color=guide_legend(ncol=2)) +
                    ggtitle(paste(gsub("_", " ", i), "SC3")) + theme(aspect.ratio = 1)
              }, mc.cores=1)
      
      filenamePlot <- file.path(dir_sc3, paste0("SC3_", d, "_cluster_plots.png")) 
      ggsave(filename=filenamePlot,
                      plot=multiplot(plotlist=plot_cluster, cols=2), 
                      width = 200, height = 80*ceiling(length(plot_Items)/2), 
                      units = c("mm"),  dpi = 300, device="png")
    
      # include plots in report    
      cat(paste0("![plot of ", basename(filenamePlot), "](", filenamePlot, ")"))

  }

```



## Graph-based clustering with igraph

Graph-based clustering is a flexible and scalable technique for clustering large scRNA-seq datasets. We first build a graph where each node is a cell that is connected to its nearest neighbours in the high-dimensional space. Edges are weighted based on the similarity between the cells involved, with higher weight given to cells that are more closely related. We then apply algorithms to identify communities of cells that are more connected to cells in the same community than they are to cells of different communities. Each community represents a cluster that we can use for downstream interpretation. 

Graph construction avoids making strong assumptions about the shape of the clusters or the distribution of cells within each cluster, compared to other methods. k is the number of nearest neighbors used to construct the graph. This controls the resolution of the clustering where higher k yields a more inter-connected graph and broader clusters. Users can exploit this by experimenting with different values of k to obtain a satisfactory resolution.

Clustering is applied on the respective dimension reduction slot (i.e. PCA, TSNE or UMAP).

```{r clustering_graphbased, results='asis'}
library(igraph)
library(pheatmap)
exprs_values = "logcounts"
dotshape <- SHINYREPS_COLORBYFACTOR[1]
numberNearestNeighbors <- c(10, 20) # number of nearest neighbors k
# available clustering strategies in igraph
cluster_algo <- c("cluster_walktrap", "cluster_louvain", "cluster_infomap", "cluster_fast_greedy", "cluster_label_prop", "cluster_leading_eigen")

sce_buildgraph <- list()
sce_igraphclust <- list()
clmod <- list() # clusterModularity
cluster.gr <- list() # cluster interaction graph

for(d in reducedDimNames(sce)) {    

  for(k in numberNearestNeighbors) {
  
    set.seed(100)
    sce_buildgraph[[paste(d, paste0("k",k), sep="_")]] <- buildSNNGraph(sce, k=k, use.dimred = d, assay.type = exprs_values) # type= rank, number, jaccard
    
    for (clu in cluster_algo[1:2]) {
      
      clu <- gsub("^cluster_", "", clu)
      clusterfun <- switch(clu, 
                      "walktrap"=igraph::cluster_walktrap,
                      "louvain"= igraph::cluster_louvain,
                      "infomap"= igraph::cluster_infomap,
                      "fast_greedy"= igraph::cluster_fast_greedy,
                      "label_prop"= igraph::cluster_label_prop,
                      "leading_eigen"= igraph::cluster_leading_eigen)

      set.seed(100)
      sce_igraphclust[[paste(d, paste0("k",k), clu, sep="_")]] <- clusterfun(sce_buildgraph[[paste(d, paste0("k",k), sep="_")]])$membership
      colData(sce)[,paste("igraph", d, paste0("k",k), clu, sep="_")] <- factor(sce_igraphclust[[paste(d, paste0("k",k), clu, sep="_")]])
      
  # Assessing cluster separation
  set.seed(100)
  clmod[[paste(d, paste0("k",k), clu, sep="_")]] <- clusterModularity(sce_buildgraph[[paste(d, paste0("k",k), sep="_")]], sce_igraphclust[[paste(d, paste0("k",k), clu, sep="_")]], get.weights=F, as.ratio = TRUE)
  cluster.gr[[paste(d, paste0("k",k), clu, sep="_")]] <- igraph::graph_from_adjacency_matrix(clmod[[paste(d, paste0("k",k), clu, sep="_")]], mode="upper", weighted=TRUE, diag=FALSE)

    }
  }
}    

cat("\nOverview cluster assignments\n", fill=T)
lapply(names(sce_igraphclust), function(x) {kable(t(table(sce_igraphclust[[x]])), align = "l", caption=x) %>% kable_styling()})  

# store cluster assignments
dir_igraph <- file.path("report_files", "cluster", "igraph")
if (!file.exists(dir_igraph)) {dir.create(dir_igraph, recursive=T) }
      write.table(colData(sce)[,c(group.vars, grep("igraph", names(colData(sce)), value=T))], 
      file = file.path(dir_igraph, paste0("igraph_cluster_assignments.txt")), sep="\t", quote=F, row.names=F)



### plot clusterings (separated for PCA, TSNE and UMAP)
  for (d in c("PCA", "TSNE", "UMAP")) {
        plot_Items <- grep(d, names(sce_igraphclust), value=T)
        
      # dim reduction plots
        plot_cluster <- mclapply(plot_Items, function(i) {
                  scater::plotReducedDim(sce, 
                    dimred = sub("^([A-Z]*[_p0-9]*[_allgenes]*)_.*", "\\1", i),
                    by_exprs_values = exprs_values,
                    colour_by=paste0("igraph_", i), shape_by = dotshape, point_size=2) +
                    scale_color_discrete(name="cluster") +
                    guides(color=guide_legend(ncol=2)) + theme(aspect.ratio = 1) +
                    ggtitle(gsub("_", " ", i))
              }, mc.cores=1)
      
      filenamePlot <- file.path(dir_igraph, paste0("igraph_", d, "_cluster_plots.png"))
      ggsave(filename=filenamePlot,
                      plot=multiplot(plotlist=plot_cluster, cols=2), 
                      width = 200, height = 80*ceiling(length(plot_Items)/2), 
                      units = c("mm"),  dpi = 300, device="png")
  
  # include plots in report    
  cat(paste0("![plot of ", basename(filenamePlot), "](", filenamePlot, ")"))
  }


# heatmaps
for (d in c("PCA", "TSNE", "UMAP")) {
 plot_Items <- grep(d, names(clmod), value=T)

   cl_heatmap <-  mclapply(plot_Items, function(i) {
                      pheatmap(log2(clmod[[i]]+1), cluster_rows=FALSE, cluster_cols=FALSE,
                          color=colorRampPalette(c("white", "blue"))(100), silent=T,
                          main= paste("cl heatmap", gsub("_", " ", i)))[["gtable"]]
               }, mc.cores=1)

   ggsave(filename=file.path(dir_igraph, paste0("igraph_", d, "_cluster_heatmaps.png")),
          plot=plot(arrangeGrob(grobs=cl_heatmap, ncol=2)), 
          width = 200, height = 80*ceiling(length(plot_Items)/2), units = c("mm"),  dpi = 300, device="png")
}


# cluster graph plots
  for (d in c("PCA", "TSNE", "UMAP")) {
    plot_Items <- grep(d, names(cluster.gr), value=T)
    
      png(filename= file.path(dir_igraph, paste0("igraph_", d, "_cluster_graph.png")), units = "mm", res= 300,
          width = 200, height = 80*ceiling(length(plot_Items)/2))
       par(mfrow = c(ceiling(length(plot_Items)/2), 2))
      for (i in plot_Items) {
      set.seed(100)
      plot(cluster.gr[[i]], edge.width=igraph::E(cluster.gr[[i]])$weight*5, main=paste("cluster graph", gsub("_", " ", i)))
      }
       
  dev.off()
  par(mfrow = c(1, 1))
 }
      
```


## Hierarchical clustering

Hierarchical clustering aims to generate a dendrogram containing a hierarchy of samples. This is most commonly done by greedily agglomerating samples into clusters, then agglomerating those clusters into larger clusters, and so on until all samples belong to a single cluster. Variants of hierarchical clustering methods primarily differ in how they choose to perform the agglomerations.

The dendrogram describes the relationships between cells and subpopulations at various resolutions and in a quantitative manner based on the branch lengths. Users can cut the tree at different heights to define clusters with different granularity, where clusters defined at high resolution are guaranteed to be nested within those defined at a lower resolution. The dendrogram is also a natural representation of the data in situations where cells have descended from a relatively recent common ancestor. For larger datasets hierarchical clustering may be too slow.

Clustering is applied on the respective dimension reduction slot (i.e. PCA, TSNE or UMAP).

```{r clustering_hierarchical, warning=F, results='asis'}
library(dendextend)
library(dynamicTreeCut)
hclust_methods <- c("ward.D2")
minClusterSize <- c(10, 100)
exprs_values <- "logcounts"
dotshape <- SHINYREPS_COLORBYFACTOR[1]

dist_sce <- list()
tree_sce <- list()
dend <- list()
clust_sce <- list()

for(d in reducedDimNames(sce)) {    
    dist_sce[[d]] <- dist(reducedDim(sce, d))

  for(h in hclust_methods) {
    set.seed(100)
    tree_sce[[paste(d, h, sep="_")]] <- hclust(dist_sce[[d]], method=h)
    
    # Making dendrogram.
    tree_sce[[paste(d, h, sep="_")]]$labels <- seq_along(tree_sce[[paste(d, h, sep="_")]]$labels)

    for (cs in minClusterSize) {
      
      dend[[paste(d, h, paste0("cs", cs), sep="_")]] <- as.dendrogram(tree_sce[[paste(d, h, sep="_")]], hang=0.1)

      # minClusterSize needs to be turned down for small datasets.
      # deepSplit controls the resolution of the partitioning.
      set.seed(100)
      clust_sce[[paste(d, h, paste0("cs", cs), sep="_")]] <- cutreeDynamic(tree_sce[[paste(d, h, sep="_")]], 
                                                         distM=as.matrix(dist_sce[[d]]),
                                                         method = "hybrid", minClusterSize=cs, deepSplit=1, verbose = 0)

      labels_colors(dend[[paste(d, h, paste0("cs", cs), sep="_")]]) <- clust_sce[[paste(d, h, paste0("cs", cs), sep="_")]][order.dendrogram(dend[[paste(d, h, paste0("cs", cs), sep="_")]])]

      colData(sce)[,paste("hclust", d, h, paste0("cs",cs), sep="_")] <- factor(clust_sce[[paste(d, h, paste0("cs", cs), sep="_")]])
  
     }
  }
}    


cat("\nOverview cluster assignments\n", fill = T)
lapply(names(clust_sce), function(x) {kable(t(table(clust_sce[[x]])), align = "l", caption=x) %>% kable_styling()})  

# store cluster assignments
dir_hclust <- file.path("report_files", "cluster", "hclust")
if (!file.exists(dir_hclust)) {dir.create(dir_hclust, recursive=T) }
      write.table(colData(sce)[,c(group.vars, grep("hclust", names(colData(sce)), value=T))], 
      file = file.path(dir_hclust, paste0("hclust_cluster_assignments.txt")), sep="\t", quote=F, row.names=F)


### plot clusterings (separated for PCA, TSNE and UMAP)
  for (d in c("PCA", "TSNE", "UMAP")) {
    plot_Items <- grep(d, names(clust_sce), value=T)
    
      # dim reduction plots
        plot_cluster <- mclapply(plot_Items, function(i) {
                  scater::plotReducedDim(sce, 
                    dimred = sub("^([A-Z]*[_p0-9]*[_allgenes]*)_.*", "\\1", i),
                    by_exprs_values = exprs_values,
                    colour_by=paste0("hclust_", i), shape_by = dotshape, point_size=3) +
                    scale_color_discrete(name="cluster") +
                    guides(color=guide_legend(ncol=2)) +
                    ggtitle(gsub("_", " ", i)) + theme(aspect.ratio = 1)
              }, mc.cores=1)
  
        
      filenamePlot <- file.path(dir_hclust, paste0("hclust_", d, "_cluster_plots.png"))
      ggsave(filename=filenamePlot,
                      plot=multiplot(plotlist=plot_cluster, cols=2), 
                      width = 200, height = 80*ceiling(length(plot_Items)/2), 
                      units = c("mm"),  dpi = 300, device="png")
    
      # include plots in report    
      cat(paste0("![plot of ", basename(filenamePlot), "](", filenamePlot, ")"))
  }
  
## plot dendrograms
 for (d in c("PCA", "TSNE", "UMAP")) {
    plot_Items <- grep(d, names(clust_sce), value=T)
    
     png(filename= file.path(dir_hclust, paste0("hclust_", d, "_dendrograms.png")), units = "mm", res= 300,
          width = 200, height = 80*ceiling(length(plot_Items)/2))
      
      par(mfrow = c(ceiling(length(plot_Items)/2), 2))
      
      for (i in plot_Items) {
        plot(dend[[i]], main=paste("hclust dendrogram", gsub("_", " ", i)))
      }
      par(mfrow = c(1, 1))
      dev.off()
 }


```

## Clustering method selected for downstream processing

```{r select_clustering}
     
  selected_clustering_method <- "igraph_TSNE_p25_allgenes_k10_louvain"    
  name_dimred <- gsub("^.*(((TSNE)(_p[[:digit:]]+)*|(UMAP)|(PCA))(_allgenes)*).*$", "\\1",  selected_clustering_method) # get corresponding reducedDimName
  cat("Selected clustering:", selected_clustering_method, "\nreducedDimName:", name_dimred, "\n\n")

   plot_selected <- scater::plotReducedDim(sce, 
                      dimred = name_dimred,
                      by_exprs_values = exprs_values,
                      colour_by=selected_clustering_method, shape_by = dotshape, point_size=3) +
                      scale_color_discrete(name="cluster") +
                      guides(color=guide_legend(ncol=2)) +
                      ggtitle(gsub("_", " ", selected_clustering_method)) + theme(aspect.ratio = 1)
   
   plot_selected
   
   ggsave(plot=plot_selected, filename= file.path("report_files", "cluster", paste0(selected_clustering_method, "_selected_cluster_plot.png")),
          width = 120, height = 120, units = c("mm"),  dpi = 300)

```





# Detect marker genes for each cluster

Next we identify the genes that drive separation between clusters of the final clustering setting. These marker genes allow us to assign biological meaning to each cluster based on their functional annotation. In the most obvious case, the marker genes for each cluster are a priori associated with particular cell types, allowing us to treat the clustering as a proxy for cell type identity. We use a one-sided t-test to identify genes that are upregulated in each cluster compared to the others (test for log foldchange > 1).

Additionally, we use a cluster specific approach. Instead of identifiying genes upregulated compared to any of the other clusters, we only consider genes that are up-regulated in all pairwise comparisons involving the cluster of interest. To achieve this, we use an intersection-union test where the combined p-value for each gene is the maximum of the p-values from all pairwise comparisons (apply setting pval.type="all"). A gene will only achieve a low combined p-value if it is strongly DE in all comparisons to other clusters.

For each cluster, every gene is assigned with the highest rank which from comparisons with all other clusters. E.g. the set of genes with top rank <= n contains the top n genes from each comparison. These genes which distinguish that cluster from any other cluster are visualized in a heatmap across all clusters.


```{r findmarkers_per_cluster, fig.width=11, fig.height=20, results='asis'}

DE_var2adjust <- if(is.null(final_var2adjust)) {NULL} else {"final_var2adjust"} 
clustervar <- selected_clustering_method
groupingvar <- SHINYREPS_COLORBYFACTOR[1] # still to implement: use findMarkers for differential expression analysis within clusters
# unadjusted logcounts shall be used as assay.type for findMarkers (adjustment is done with DE_var2adjust if needed)
assay.type <- c("logcounts_not_adjusted", "logcounts", "counts")[c("logcounts_not_adjusted", "logcounts", "counts") %in% names(assays(sce))][1]
###############################################


if(nlevels(factor(colData(sce)[,clustervar])) >=2) {

  dir_marker <- file.path("report_files", "marker_per_cluster", clustervar)
  if (!file.exists(dir_marker)) {dir.create(dir_marker, recursive=T) }
  
  cat("Un-adjusted logcounts are used for marker detection. ")
  if(is.null(DE_var2adjust)) {
    cat("The analysis is not adjusted for any confounding variables.\n")
  } else {
      if(DE_var2adjust == "final_var2adjust") {
        cat("The analysis is adjusted for the same factors as described in 'Adjust for confounder variables'.\n")
      } else {
        cat("The analysis is adjusted for:", DE_var2adjust, "\n")
      } 
    }
    
    
  # find upregulated markers for each cluster and upregulated marker specific for each cluster
  # using unadjusted logcounts including previously applied blocking variable
  # If DE_var2adjust is NULL no blocking applied 
  markers <- findMarkers(sce, groups=colData(sce)[,clustervar], 
                         block= if (is.null(DE_var2adjust)) {NULL} else {colData(sce)[,DE_var2adjust]},  
                         assay.type = assay.type,
                         row.data=rowData(sce)[,"SYMBOL", drop=F], direction="up", lfc=1)
  markers_spec <- findMarkers(sce, groups=colData(sce)[,clustervar], 
                         block= if (is.null(DE_var2adjust)) {NULL} else {colData(sce)[,DE_var2adjust]},  
                         assay.type = assay.type,
                         row.data=rowData(sce)[,"SYMBOL", drop=F], direction="up", pval.type="all")
                         # use pval.type="some" if "all" is too stringent 
  
     list_heatmaps <- NULL
     list_heatmaps_spec <- NULL
   marker_heatmaps <- lapply(names(markers), function(cl) { 
    
         # all upregulated markers per cluster
        write.table(markers[[cl]], sep="\t", quote=F, row.names=F, 
                    file=file.path(dir_marker, paste0("gene_marker_upregulated_in_cluster_", cl, ".txt")) )
        best_set <- markers[[cl]][markers[[cl]]$Top <= 5,]
        logFCs <- as.matrix(best_set[,grepl("logFC", colnames(best_set))])
        colnames(logFCs) <- sub("logFC.", "", colnames(logFCs))
        logFCs <- logFCs[, colSums(is.na(logFCs)) != nrow(logFCs)] # remove columns with only NA (cannot be plotted in heatmap)
        
        # markers upregulated in 1 cluster only   
        write.table(markers_spec[[cl]], sep="\t", quote=F, row.names=F, 
                    file=file.path(dir_marker, paste0("gene_marker_upregulated_in_cluster_", cl, "_only.txt")))
        best_set_spec <- markers_spec[[cl]][1:nrow(best_set), ] # select same number of genes displayed as in best_set
        logFCs_spec <- as.matrix(best_set_spec[,grepl("logFC", colnames(best_set_spec))])
        colnames(logFCs_spec) <- sub("logFC.", "", colnames(logFCs_spec))
        logFCs_spec <- logFCs_spec[, colSums(is.na(logFCs_spec)) != nrow(logFCs_spec)] # remove columns with only NA 
        
        if(length(names(markers)) >=3) { # create heatmaps if at least 3 groups, i.e. at least 2 hetmap columns.
            list_heatmaps <- pheatmap(logFCs, labels_row=best_set$SYMBOL, silent=T, breaks=seq(-5, 5, length.out=101), 
                                  main=paste("Top marker upregulated in cluster", cl))[["gtable"]]
  
            list_heatmaps_spec <- pheatmap(logFCs_spec, labels_row=best_set_spec$SYMBOL, silent=T, breaks=seq(-5, 5, length.out=101), 
                                       main=paste("Top marker upregulated in cluster", cl, "only"))[["gtable"]]
        
            filenamePlot <- file.path(dir_marker, paste0("heatmap_of_upregulated_gene_marker_cluster_", cl, ".png"))
            ggsave(filename=filenamePlot,
                   plot=plot(arrangeGrob(grobs=list(list_heatmaps, list_heatmaps_spec), ncol=2)), 
                   width = 200, height = 150, units = c("mm"),  dpi = 300, device="png")
    
            # include plots in report    
            cat(paste0("![plot of ", basename(filenamePlot), "](", filenamePlot, ")"))
            cat("\n", fill = T)
        } else {cat("\nHeatmaps skipped because only 1 column to plot.")}
        
      return(list(all_up=list_heatmaps, cluster_spec=list_heatmaps_spec))
    })
  
} else {
  cat("\nNo marker genes to calculate because clustering variable has only 1 cluster\n")
}

```






```{r cell_type_annotation, results='asis', eval=org %in% c("human", "mouse")}
library(SingleR)

cat("# Cell type / GO annotation")

dir_anno_per_cluster <- file.path("report_files", "annotation_per_cluster", selected_clustering_method)
if (!file.exists(dir_anno_per_cluster)) {dir.create(dir_anno_per_cluster, recursive=T) }


# Assigning cell labels from reference data (available for human only)
if(org=="human") {
  cat("\nAssigning cell labels from built-in reference data constructed from Blueprint and ENCODE data (Martens and Stunnenberg 2013; The ENCODE Project Consortium 2012). This reference contains normalized expression values of 259 human bulk RNA-seq samples of pure stroma and immune cells\n")
  countdata <- logcounts(sce)
  rownames(countdata) <- rowData(sce)$SYMBOL
  ref <- BlueprintEncodeData() # human bulk RNA-seq data from Blueprint and ENCODE
  pred <- SingleR(test=sce, ref=ref, labels=ref$label.main) # alternative: ref$label.fine
  table(pred$labels)
  plotScoreHeatmap(pred)
  plotScoreDistribution(pred)
  
  tab <- table(Assigned=pred$pruned.labels, Cluster=colData(sce)[,selected_clustering_method])
  # Adding a pseudo-count of 10 to avoid strong color jumps with just 1 cell.
  pheatmap(log2(tab+10), color=colorRampPalette(c("white", "blue"))(101))
}

  switch(org,
         human={
            library(org.Hs.eg.db)
            orgdb <- org.Hs.eg.db
            species="Hs"
         },
         mouse={
            library(org.Mm.eg.db)
            orgdb <- org.Mm.eg.db
            species="Mm"
         }
  )
    
## Assigning cluster labels from markers
cat("
    Perform a gene set enrichment analysis on the marker genes defining each cluster. This identifies the pathways and processes that are (relatively) active in each cluster based on upregulation of the associated genes compared to other clusters. We will use gene sets defined by the Gene Ontology (GO) project, which describe a comprehensive range of biological processes and functions. We define our subset of relevant marker genes at a FDR of 5% and apply the goana function from the limma package. This performs a hypergeometric test to identify GO terms that are overrepresented in our marker subset. We keep only biological process terms that are not overly general (<=200 genes) and which are significantly enriched (p<0.05).\n\n")
 
anno.results <- list()

for (i in levels(colData(sce)[,selected_clustering_method])) {
 
    cat("\nProcessing cluster", i, "\n")

    cur.markers <- markers[[i]]
    is.de <- cur.markers$FDR <= 0.05 
    # summary(is.de)
    
    if(org=="human") {rownames(cur.markers) <- gsub("\\..*$", "", rownames(cur.markers))} # remove .version of human Ensembl IDs

    entrez.ids <- mapIds(orgdb, keys=rownames(cur.markers), 
        column="ENTREZID", keytype="ENSEMBL")
    go.out <- goana(unique(entrez.ids[is.de]), species=species, 
        universe=unique(entrez.ids))
    
    # Only keeping biological process terms that are not overly general and which are significantly enriched.
    # P.DE: p-value for over-representation of the GO term in the set.
    go.out <- go.out[order(go.out$P.DE),]
    go.useful <- go.out[go.out$Ont=="BP" & go.out$N <= 200 & go.out$P.DE < 0.05,]
    
    print(kable(go.useful[1:10,]) %>% kable_styling())
    write.table(data.frame(GOID=rownames(go.useful), go.useful), 
                file =file.path(dir_anno_per_cluster, paste0("GOannotation_cluster_", i, ".txt")),
                quote = F, row.names = F,  sep="\t")
    
    anno.results[[i]] <- go.useful
    
    # Identify genes associated with an interesting term.
    # adhesion <- unique(by.go[["GO:0022408"]])
    # head(cur.markers[rownames(cur.markers) %in% adhesion,1:3], 10)
    
  }

# Extract symbols for each GO term; done once.
    tab <- AnnotationDbi::select(orgdb, keytype="ENSEMBL", keys=gsub("\\..*$", "", rownames(sce)), columns="GOALL")
    by.go <- split(tab[,1], tab[,2])

```


# Differential expression analysis

Differential analyses of multi-condition scRNA-seq experiments can be broadly split into two categories: differential expression (DE) and differential abundance (DA) analyses. The former tests for changes in expression between conditions for cells of the same type that are present in both conditions, while the latter tests for changes in the composition of cell types (or states, etc.) between conditions.

## Differential expression between conditions using pseudo-bulk samples

Motivations behind the use of pseudo-bulking:

- Larger counts are more amenable to standard DE analysis pipelines designed for bulk RNA-seq data. Normalization is more straightforward and certain statistical approximations are more accurate for large counts.
- Collapsing cells into samples reflects the fact that our biological replication occurs at the sample level (Lun and Marioni 2017). Each sample is represented no more than once for each condition, avoiding problems from unmodelled correlations between samples. Supplying the per-cell counts directly to a DE analysis pipeline would imply that each cell is an independent biological replicate, which is not true from an experimental perspective. 
- Variance between cells within each sample is masked, provided it does not affect variance across (replicate) samples. This avoids penalizing DEGs that are not uniformly up- or down-regulated for all cells in all samples of one condition. Masking is generally desirable as DEGs - unlike marker genes - do not need to have low within-sample variance to be interesting, e.g., if the treatment effect is consistent across replicate populations but heterogeneous on a per-cell basis. 
    
The DE analysis will be performed using quasi-likelihood (QL) methods from the edgeR package (Robinson, McCarthy, and Smyth 2010; Chen, Lun, and Smyth 2016). This uses a negative binomial generalized linear model (NB GLM) to handle overdispersed count data in experiments with limited replication. We test for differences in expression using glmQLFTest(). DEGs are defined as those with non-zero log-fold changes at a false discovery rate of 5%.

```{r diff_expression, results='asis'}
library(edgeR)

DE_var2adjust <- if(is.null(final_var2adjust)) {NULL} else {"final_var2adjust"} 
varDiffGroups <- SHINYREPS_COLORBYFACTOR[1]
########################################

dir_DE_per_cluster <- file.path("report_files", "DE_per_cluster", selected_clustering_method)
if (!file.exists(dir_DE_per_cluster)) {dir.create(dir_DE_per_cluster, recursive=T) }

cat("Un-adjusted and un-normalized counts are used for differential expression analysis. Normalization is done after grouping of cells. ")
if(is.null(DE_var2adjust)) {
  cat("The analysis is not adjusted for any confounding variables.\n")
} else {
    if(DE_var2adjust == "final_var2adjust") {
      cat("The analysis is adjusted for the same factors as described in 'Adjust for confounder variables'.\n")
    } else {
      cat("The analysis is adjusted for:", DE_var2adjust, "\n")
    } 
  }


# Creating pseudo-bulk samples from clusters uses (unnormalised "counts" matrix)
  summed <- scater::aggregateAcrossCells(sce, ids=DataFrame(
                cluster=colData(sce)[,selected_clustering_method], 
                variable2adjust=colData(sce)[, DE_var2adjust],
                diffgroups=colData(sce)[, varDiffGroups]),
                exprs_values = "counts"
                )

de.results <- list()

for (i in levels(colData(sce)[,selected_clustering_method])) {
   
    cat("\nProcessing cluster", i, "\n")
  
    current <- summed[,colData(summed)[,selected_clustering_method] == i]
    
    # Creating up a DGEList object for use in edgeR:
    y <- DGEList(counts(current), samples=colData(current), genes=rowData(current)$SYMBOL)
    
    # preprocessing for bulk samples (is done here for each cluster separately)
        # # remove samples with very low library sizes
        # discarded <- isOutlier(y$samples$lib.size, log=TRUE, type="lower") # 
        # y <- y[,!discarded]
        # remove genes that are lowly expressed
        keep <- filterByExpr(y, group=current$diffgroups, min.count = 5, min.total.count = 10, large.n = 3, min.prop = 0.5)
        y <- y[keep,]
    
    
    # correct for composition biases by normalization factors (currently using unprocessed count data)
    y <- edgeR::calcNormFactors(y)
    
    # Statistical modelling
    design <- try(
        model.matrix(~factor(variable2adjust) + factor(diffgroups), y$samples),
        silent=TRUE
    )
    if (is(design, "try-error") || 
        qr(design)$rank==nrow(design) ||
        qr(design)$rank < ncol(design)) 
    {
        # Skipping labels without contrasts or without 
        # enough residual d.f. to estimate the dispersion.
        cat("\nskip cluster", i, "\n")
        next
    }

    # print filter metrics if cluster is processed further
     cat(paste0("\nFiltering genes for low expression for cluster ", i, "."))
    # cat("\nsamples discarded:", sum(discarded))
    cat("\nGenes kept:", sum(keep), "\n")

    # We estimate the negative binomial (NB) dispersions with estimateDisp(). 
    y <- estimateDisp(y, design)
    #summary(y$trended.dispersion)
    #plotBCV(y)
    
    # We also estimate the quasi-likelihood dispersions with glmQLFit() (Chen, Lun, and Smyth 2016). 
    # This fits a GLM to the counts for each gene and estimates the QL dispersion from the GLM deviance. 
    # We set robust=TRUE to avoid distortions from highly variable clusters (Phipson et al. 2016). 
    # The QL dispersion models the uncertainty and variability of the per-gene variance (Figure 14.3),
    # which is not well handled by the NB dispersions, so the two dispersion types complement each other 
    # in the final analysis.
    
    fit <- glmQLFit(y, design, robust=TRUE)
    #summary(fit$var.prior)
    #summary(fit$df.prior)
    #plotQLDisp(fit)
    
    # We test for differences in expression using glmQLFTest(). DEGs are defined as those with non-zero 
    # log-fold changes at a false discovery rate of 5%.
    
    cat("\nDifferential expression results for", varDiffGroups, "in cluster", i, "\n")
    res <- glmQLFTest(fit, coef=ncol(design))
    #summary(decideTests(res))
    print(kable(as.data.frame(topTags(res))) %>% kable_styling())
    write.table(data.frame(EnsemblID=rownames(topTags(res, n=nrow(res))), topTags(res, n=nrow(res))), 
                file =file.path(dir_DE_per_cluster, paste0("DEgenes_cluster_", i, ".txt")),
                quote = F, row.names = F,  sep="\t")
    de.results[[i]] <- res

}    

cat("\nSummary for all clusters\n")    
summaries <- lapply(de.results, FUN=function(x) summary(decideTests(x))[,1])
sum.tab <- do.call(rbind, summaries)
print(kable(sum.tab) %>% kable_styling())

```



```{r store_data, echo=F, results='hide', error=F, warning=F, message=F, eval=T}

# store count data (all and by pool)
dir_countdata <- file.path("report_files", "count_data")

cat("count data matrices are stored in", dir_countdata, "\n")

countslist <- lapply(names(assays(sce)), function(x) {

  if (!file.exists(file.path(dir_countdata, x))) {dir.create(file.path(dir_countdata, x), recursive = T)}

  write.table(data.frame(gene=rownames(sce), symbol=rowData(sce)$SYMBOL, assay(sce, x)), 
              file =file.path(dir_countdata, x, paste0(x, "_all.txt")), sep="\t", quote = F, row.names = F)

  lapply(unique(colData(sce)$pool), function(y) {
    sce.subset <- sce[, colData(sce)$pool == y]
    write.table(data.frame(gene=rownames(sce.subset), symbol=rowData(sce.subset)$SYMBOL, assay(sce.subset, x)), 
                file = file.path(dir_countdata, x, paste0(x, "_", y, ".txt")), sep="\t", quote = F, row.names = F)
  })
})

# save workspace
save.image("report_files/WS.RData")

# create files for shiny app 
library(shinydashboard)

dir_shinyapp <- file.path("report_files", "shinyapp")
if (!file.exists(dir_shinyapp)) {dir.create(dir_shinyapp) }
  top.hvg <- hvg.cor.list[["all"]]$top.hvg
  genes <- gene.names[gene.names$gene_id %in% rownames(sce), ]
  save(sce, genes, top.hvg, file=file.path(dir_shinyapp, "data4shiny.RData"))
  
```




# Used tools and versions for this analysis ##

Read mapping was performed with STAR using the following parameters:

```{r STAR_parameters_paragraph, echo=F, results='asis', error=F, warning=F, message=F}
cat(DEhelper.STARparms(), sep="\n")
``` 

The following tools were used for data processing:

```{r ToolVersions_paragraph, echo=F, results='asis', error=F, warning=F, message=F}
cat(Toolhelper.ToolVersions(), sep="\n")
```

R session info:

```{r R_sessionInfo, echo=F, results='asis', error=F, warning=F, message=F}
sessionInfo()
```

</div>

