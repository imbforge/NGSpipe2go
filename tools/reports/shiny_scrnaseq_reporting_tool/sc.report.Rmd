---
title: "SHINYREPS_PROJECT"
output:
  html_document:
    toc: true
    toc_float: true
    css: styles.css
---

<div class="contentbox">


# Description

Enter project description here


# Processing

Enter pipeline description here.


```{r setup, echo=F, result='hide', error=F, warning=F, message=F}

# source helper functions
source("sc.shinyrep.helpers.R")

# load required packages
not_yet_attached <- attach_packages(c("tidyverse", "AnnotationDbi", "batchelor", "Biobase", "BiocSingular", "bluster", "Cairo", "celldex", "circlize",
          "cluster", "ComplexHeatmap", "corrplot", "dendextend", "DESeq2", "DropletUtils", "dynamicTreeCut", "edgeR", "GeneOverlap", "ggbeeswarm",
          "ggrepel", "gplots", "grid", "gridExtra", "igraph", "knitr", "kableExtra", "limma", "M3Drop", "Matrix", "parallel", 
          "pheatmap", "pkgmaker", "plotly", "png", "RColorBrewer", "reshape2", "rmarkdown", "rtracklayer", "Rtsne","SC3", "scales", 
          "scater", "scDblFinder", "scran", "shinydashboard", "SingleR", "uwot", "VennDiagram", "vipor", "viridis"))

# load global variables
loadGlobalVars(f="shinyReports.txt")

# this file is for scRNA-seq only, set tenXmultiome pipeline to tenX
if (SHINYREPS_SEQTYPE == "tenXmultiome") {SHINYREPS_SEQTYPE <- "tenX"}

# create folder for output files
if (!file.exists(file.path("report_files"))) {dir.create(file.path("report_files"))}

# set options
options(stringsAsFactors=FALSE)
CORES <- 2
pal   <- brewer.pal(9, "Set1")
pal_rb <- colorRampPalette(c(pal[1], "white", pal[2]))(20)
pal_y  <- colorRampPalette(c("black", "yellow"))(100)
plot_pointsize <- 1 # point size for plotReducedDim plots

knitr::opts_chunk$set(cache=F,
                      echo=F,
                      warning=F,
                      message=F,
                      dev='CairoPNG')

theme_set(theme_bw() + theme(axis.text=element_text(colour="grey30",size=12),
									axis.title=element_text(colour="grey30",size=14),
									plot.title=element_text(size=14,hjust=0.5),
									plot.subtitle=element_text(size=12,hjust=0.5),
									legend.text=element_text(size=12,colour="grey30"),
									legend.title=element_text(size=12,colour="grey30")))

# load list of mitochondrial genes (if not given, we will use all genes starting with "MT-")
mito.genes <- if(file.exists(file.path(SHINYREPS_PROJECT,SHINYREPS_MTGENES))) {
  read.delim(file.path(SHINYREPS_PROJECT,SHINYREPS_MTGENES))[, 1]
} else {NA}

runCutadapt <- SHINYREPS_RUN_CUTADAPT=="true" # check for selected pipeline modules 
org <- SHINYREPS_ORG # organism name needed for cell cycle and GO enrichment 
selgenes <- c("") # selected genes (symbols) for expression plots


## load or create specific targets file for all cells. 
switch(SHINYREPS_SEQTYPE,
    MARSseq={
    # Columns required: "sample", "file", "barcode", "plate", "row", "col", "cells", "group", "pool"   
      targets <- read.delim(SHINYREPS_TARGET, sep="\t", comment.char = "#")
      group.vars <- colnames(targets)[!colnames(targets) %in% c("sample", "file", "barcode", "row", "col")] 
      targets[,group.vars] <- lapply(targets[,group.vars], factor)
      # pool-wise targets to annotate plots made before demultiplexing by cell barcode
      targets_pools <- targets[!duplicated(targets$pool),!colnames(targets) %in% c("sample", "file", "barcode", "row", "col", "cells")] 
      targets4plots <- targets_pools 
      group.vars <- group.vars[sapply(group.vars, function(x) {length(unique(targets[,x]))>1})] # remove group vars with single value
      colorByFactor <- "pool" # default for pipeline overview plots
      colorByFactor2 <- if(!is.na(SHINYREPS_COLORBYFACTOR)) {SHINYREPS_COLORBYFACTOR} else {"group"} # default for downstream plots (up to 2 categories)
   }, 
    SmartSeq2={
    # Columns required: "sample", "file", "plate", "row", "col", "cells", "group"
      targets <- read.delim(SHINYREPS_TARGET, sep="\t", comment.char = "#")
      group.vars <- colnames(targets)[!colnames(targets) %in% c("sample", "file", "barcode", "row", "col")] 
      targets[,group.vars] <- lapply(targets[,group.vars], factor)
      targets4plots <- targets
      group.vars <- group.vars[sapply(group.vars, function(x) {length(unique(targets[,x]))>1})] # remove group vars with single value
      colorByFactor <- "cells" # default for pipeline overview plots
      colorByFactor2 <- if(!is.na(SHINYREPS_COLORBYFACTOR)) {SHINYREPS_COLORBYFACTOR} else {"group"} # default for downstream plots (up to 2 categories)
    },
    tenX={
      aggrcsv <- read.delim(file.path(SHINYREPS_PROJECT, "aggr.csv"), sep=",") # aggr.csv contains the order of GEM wells
      # load targets.txt (data per fastq file). Columns required: "sample", "file", "group", "replicate"
      targets_pools <- read.delim(SHINYREPS_TARGET, sep="\t", comment.char = "#")
      # create targets on cell level
      matrix_dir = file.path(SHINYREPS_RES, SHINYREPS_CELLRANGERAGGR_ID, "outs/count/filtered_feature_bc_matrix/")
      if (!dir.exists(matrix_dir)) {matrix_dir = file.path(SHINYREPS_RES, SHINYREPS_CELLRANGERAGGR_ID, "outs/filtered_feature_bc_matrix/")}
      targets = read.delim(file.path(matrix_dir, "barcodes.tsv.gz"), header = FALSE, stringsAsFactors = FALSE)
      names(targets) = "cell_id"
      
      if(SHINYREPS_RUN_DEMUX) { # add HTO information if cell hashing applied
        demuxfiles <- lapply(1:nrow(aggrcsv), function(x) {
          read.delim(file.path(SHINYREPS_DEMUX_OUT, gsub("_S._L..._R._001", "",aggrcsv$sample_id)[x], "Seurat", "demux.txt"), sep="\t")
        })
        demuxfiles <- dplyr::bind_rows(demuxfiles, .id = "GEMwell")
        demuxfiles$cell_id <- paste(demuxfiles$cell_id, demuxfiles$GEMwell, sep="-")
        targets <- merge(targets, demuxfiles[, c("cell_id", "GEMwell", "HTO_maxID", "HTO_classification.global", 
                                                  "nCount_HTO", "nFeature_HTO", "HTO_secondID", "HTO_margin")], by="cell_id")
        targets$name_HTO <- gsub("-.*$", "", targets$HTO_maxID)
      }
     
      # merge targets with targets_pools and sort 
      # ensure that GEM wells merged in aggr module are assigned the sample names from targets.txt in correct order
      # the -digit suffix of the cell barcode reflects the order that the GEM wells were provided in the aggr.csv
      targets$file <- gsub("_S._L..._R._001", "", aggrcsv[as.numeric(gsub("^.*-", "", targets$cell_id)), "sample_id"])
      targets <- merge(targets, targets_pools) # if "name_HTO" column is present in targets, it is used for merging as well (unmapped cells discarded)
      rownames(targets) <- targets$cell_id
      StartingColumnNames <- c("cell_id", "sample", "file", "group", "replicate")
      targets <- targets[order(targets$sample, targets$cell_id), c(StartingColumnNames, colnames(targets)[!colnames(targets) %in% StartingColumnNames])]
      
      # define group.vars
      group.vars <- colnames(targets)[!colnames(targets) %in% c("cell_id", "sample", "file")] 
      targets[,group.vars] <- lapply(targets[,group.vars], factor)
      group.vars <- group.vars[sapply(group.vars, function(x) {length(unique(targets[,x]))>1})] # remove group vars with single value
      # in case of hto multiplexing targets_pools is collapsed to (not de-multiplexed) fastq files for the fastq-file level QC plots (targets4plots).
      # But 'group' and 'replicate' columns are not kept if they show multiple values per file (e.g. multiplexed samples belong to different groups). 
      # This is only relevant for the gene body coverage plots per group or per replicate, which would be misleading in that case.
      collapsedColumns <- sapply(c("sample", "group", "replicate"), function(x) {
          all(sapply(unique(targets_pools$file), function(y) {length(unique(targets_pools[targets_pools$file==y, x]))==1}))
          })
      targets4plots <- targets_pools[!duplicated(targets_pools$file), c("file", names(collapsedColumns)[collapsedColumns])] 
      if(!"sample" %in% colnames(targets4plots)) {targets4plots$sample <- targets4plots$file} # "sample" column is needed. Same as "file" if ambiguous.
      colorByFactor <- "sample" # default for pipeline overview plots on file level
      colorByFactor2 <- if(!is.na(SHINYREPS_COLORBYFACTOR)) {SHINYREPS_COLORBYFACTOR} else {"sample"} # default for downstream plots on cell level (up to 2 categories)
   },
    stop(c("Don't find seqtype:", SHINYREPS_SEQTYPE))   
)

# print targets file
DT::datatable(targets, caption="targets file (sample sheet) used for analysis")
```

```{r read_annotation}
# load gene annotation provided in essential.vars.groovy
gtf <- import.gff(SHINYREPS_GTF, format="gtf", feature.type="exon")
#gtf.flat <- unlist(reduce(split(gtf, elementMetadata(gtf)$gene_id)))
#gene.lengths <- tapply(width(gtf.flat), names(gtf.flat), sum)
gene.names <- unique(as.data.frame(gtf)[, c("gene_id", "gene_name")])

```


# Raw reads QC #

## FastQC of all reads

The raw sequence reads of all samples are analysed with the popular FastQC tool (http://www.bioinformatics.babraham.ac.uk/projects/fastqc/).

#### Read quality

The "read qualities" plot summarizes the mean quality score at each position.
The color on the graph shows the Phred quality scores, which are logarithmically related to the base-calling error probabilities. The higher the score the better the base call. The background of the graph divides the y axis into very good quality calls (green), calls of reasonable quality (yellow), and calls of poor quality (red). Typically, the majority of calls on all base positions fall into the green area. Due to binning of quality score in recent sequencing technologies (e.g. Illumina NextSeq), sequencing qualities are the same on many position and, thus, also samples appear to have very similar or almost identical sequencing qualities.


```{r FastQC_qual_summarized, echo=F,results='asis',error=F,warning=F,message=F,fig.align='center'}
##### parameters to set:
# Select samples for which you would like to include fastqc results in the report. For single cell RNA-Seq with many cells, 
# you may want to restrict the total number of plots. If you provide a regular expression in 'samplePattern' only those 
# filenames will be included which match this expression, e.g. setting samplePattern="R1" yields only those fastq 
# files containing read1 of a read pair. This is recommended e.g. for MARS-Seq, where Read2 contains barcode information only.
# If you want to exclude samples according to the given certain pattern, define it in excludePattern. 
# If you set argument 'maxno', the maximum sample number will be restricted accordingly to the first 'maxno' plots. 
##### 

if (SHINYREPS_FASTQC_SUMMARIZED) {
    fastqc_summary_plot <- DEhelper.Fastqc.custom(web=F,summarizedPlots=T, targetsdf=targets4plots, samplePattern=SHINYREPS_SAMPLEPATTERN1, excludePattern=SHINYREPS_SAMPLEPATTERN2, maxno=SHINYREPS_MAXNO)
    plot(fastqc_summary_plot$p.qual)
    cat("\n", fill = T)
} else {
    fastqc_summary_plot <- DEhelper.Fastqc.custom(web=F,summarizedPlots=F, targetsdf=targets4plots, samplePattern=SHINYREPS_SAMPLEPATTERN1, excludePattern=SHINYREPS_SAMPLEPATTERN2, maxno=SHINYREPS_MAXNO)
}

```

```{r FastQC_qual, echo=F,results='asis',error=F,warning=F,message=F,fig.align='center',fig.height=min(170, round((3*fastqc_summary_plot$no.of.samples)/4)+1)}
# REMARK: the maximal figure height is limited to 170 and will throw an error if exceeded ("Failed to create Cairo backend!"). For the not summarized plots you should limit the number of plots to 200 by using the maxno parameter. Otherwise the plots will be squeezed.

if (SHINYREPS_FASTQC_SUMMARIZED == FALSE) {
    plot(fastqc_summary_plot$p.qual)
    cat("\n", fill = T)
}

```

#### Sequence bias

The "sequence bias" plot shows the proportion of each base (% G, A, T and C) at each position. In a random library there would be little difference between the positions of a sequence run, so the lines in this plot should run parallel with each other. But most RNA-seq libraries show sequence imbalance in the first 10-12 read positions due to RT priming biases, which should however look fairly similar in all samples. In case the nucleotide content is summarized in a heatmap for all samples, gray colored fields represent a balanced nucleotide content.

```{r FastQC_nuc_content_summarized, echo=F,results='asis',error=F,warning=F,message=F,fig.align='center', fig.height=min(170, ceiling(0.12*fastqc_summary_plot$no.of.samples)+1)}

if (SHINYREPS_FASTQC_SUMMARIZED) {
    plot(fastqc_summary_plot$p.content)
    cat("\n", fill = T)
} 

```

```{r FastQC_nuc_content, echo=F,results='asis',error=F,warning=F,message=F,fig.align='center',fig.height=min(170, round((3*fastqc_summary_plot$no.of.samples)/4)+1)}

if (SHINYREPS_FASTQC_SUMMARIZED == F) {
    plot(fastqc_summary_plot$p.content)
    cat("\n", fill = T)
}
```

#### GC content

The "GC content" plot shows the GC% distribution of all reads and all samples. Typically, the lines should tightly overlap and look essentially the same in all samples. An unusually shaped distribution could indicate a contaminated library.

```{r FastQC_GC, echo=F,results='asis',error=F,warning=F,message=F,fig.align='center'}

fastqc_summary_plot$p.gc
cat("\n", fill = T)
```



```{r cutadapt, echo=F, results='asis', error=F, warning=F, message=F, eval=runCutadapt}
##### parameters to set:
# define subset of cutadapt log files if desired (samplePattern=NULL (default) includes all files).
# define categories of targets.txt to be used for dot color in the plot (one plot per element of colorByFactor will be created). 
# The function will try to map the data from targets$file to the cutadapt log file names (for this, the unique part of 
# targets$file must be a substring of file name). If you have cutadapt log files from pooled fastq files but cell-wise 
# information in targets.txt as in MARS-Seq, you may provide another customized targets object in 'targetsdf'. 
# Otherwise pruned file names will be used by default.
#####

cat("## Adapter trimming with Cutadapt\n")

cat("\nThe following plot shows the amount of reads trimmed for the selected adapter sequences including polyA/polyT sequences if specified. The column 'tooshort' gives the percentage of reads removed due to a length of less than 30 bases after trimming. Additionally to the column 'trimmed' which is for all adapters combined, there are columns for every single adapter giving the percentage of reads trimmed for this adapter.\n\n")

DEhelper.cutadapt(colorByFactor=colorByFactor, targetsdf=targets4plots)
```

```{r FastQC_qual_summarized_trimmed, echo=F,results='asis',error=F,warning=F,message=F,fig.align='center', eval=runCutadapt}
cat("### FastQC of trimmed reads QC\n\n")
cat("#### Read quality\n\n")
if (SHINYREPS_FASTQC_SUMMARIZED) {
    fastqc_summary_plot <- DEhelper.Fastqc.custom(web=F,summarizedPlots=T, subdir="trimmed", targetsdf=targets4plots, samplePattern=SHINYREPS_SAMPLEPATTERN1, excludePattern=SHINYREPS_SAMPLEPATTERN2, maxno=SHINYREPS_MAXNO)
    plot(fastqc_summary_plot$p.qual)
    cat("\n", fill = T)
} else {
    fastqc_summary_plot <- DEhelper.Fastqc.custom(web=F,summarizedPlots=F, subdir="trimmed", targetsdf=targets4plots, samplePattern=SHINYREPS_SAMPLEPATTERN1, excludePattern=SHINYREPS_SAMPLEPATTERN2, maxno=SHINYREPS_MAXNO)
}
```

```{r FastQC_qual_trimmed, echo=F,results='asis',error=F,warning=F,message=F,fig.align='center',fig.height=min(170, round((3*fastqc_summary_plot$no.of.samples)/4)+1), eval=runCutadapt}

if (SHINYREPS_FASTQC_SUMMARIZED == FALSE) {
    plot(fastqc_summary_plot$p.qual)
    cat("\n", fill = T)
}
```

```{r FastQC_nuc_content_summarized_trimmed, echo=F,results='asis',error=F,warning=F,message=F,fig.align='center', fig.height=min(170, ceiling(0.12*fastqc_summary_plot$no.of.samples)+1), eval=runCutadapt}
cat("#### Sequence bias of trimmed reads\n")

if (SHINYREPS_FASTQC_SUMMARIZED) {
    plot(fastqc_summary_plot$p.content)
    cat("\n", fill = T)
} 
```

```{r FastQC_nuc_content_trimmed, echo=F,results='asis',error=F,warning=F,message=F,fig.align='center',fig.height=min(170, round((3*fastqc_summary_plot$no.of.samples)/4)+1), eval=runCutadapt}

if (SHINYREPS_FASTQC_SUMMARIZED == F) {
    plot(fastqc_summary_plot$p.content)
    cat("\n", fill = T)
}
```

```{r FastQC_GC_trimmed, echo=F,results='asis',error=F,warning=F,message=F,fig.align='center', eval=runCutadapt}
cat("#### GC content of trimmed reads\n")
fastqc_summary_plot$p.gc
cat("\n", fill = T)
```


## Gene body coverage ##

The plots below show meta-gene profiles of 5' to 3' read coverage and can reveal biases due to RNA degradation or specific library prep protocol; polyA-selection protocols are particularly prone of producing 3' coverage bias upon RNA degradation.

```{r GeneBodyCoverage_paragraph, echo=F, error=F, warning=F, message=F, results='asis', fig.align="center"}

plot_list <- DEhelper.geneBodyCov2(web=F, targetsdf=targets4plots, samplePattern=SHINYREPS_SAMPLEPATTERN1, excludePattern=SHINYREPS_SAMPLEPATTERN2, maxno=SHINYREPS_MAXNO)

cat("#### Gene body coverage per sample: plotly")
cat("\n\n", fill=T)
plot_list$plotly %>%
    highlight(dynamic=T, selectize=T, persistent=T, selected = attrs_selected(showlegend = FALSE)) 
cat("\n\n")

if("p_per_sample" %in% names(plot_list)){
   cat("#### Gene body coverage per sample: \n", fill=T)
   if("p_per_cells" %in% names(plot_list)) {
     plot(plot_list$p_per_cells) # for plate-based approaches use plot colored by cell control type if available
   } else {
   plot(plot_list$p_per_sample)
   }
   cat("\n", fill=T)
}

if("num_groups" %in% names(plot_list)) {
   no.of.groups <- plot_list$num_groups
} else {
   no.of.groups <- 1
}

if("num_replicates" %in% names(plot_list)) {
   no.of.replicates <- plot_list$num_replicates
} else {
   no.of.replicates <- 1
}

```

```{r GeneBodyCoverage_splitByGroup, echo=F, error=F, warning=F, message=F, results='asis', fig.align="center", out.width="100%", fig.height=ceiling((no.of.groups)/2)*3+0.5+ifelse(no.of.groups<2,2,0)}

if(("p_per_sample_splitByGroup" %in% names(plot_list)) && !("p_per_cells_splitByGroup" %in% names(plot_list))){
   cat("#### Gene body coverage per sample split by group: \n", fill=T)
   plot(plot_list$p_per_sample_splitByGroup)
   cat("\n", fill=T)
}
if("p_per_replicate_splitByGroup" %in% names(plot_list)){
  cat("#### Gene body coverage per sample split by group, colored by replicate: \n", fill=T)
  plot(plot_list$p_per_replicate_splitByGroup)
  cat("\n", fill=T)
}
if("p_per_cells_splitByGroup" %in% names(plot_list)){
  cat("#### Gene body coverage per sample split by group, colored by cell control type: \n", fill=T)
  plot(plot_list$p_per_cells_splitByGroup)
  cat("\n", fill=T)
}
```


```{r GeneBodyCoverage_splitByReplicate, echo=F, error=F, warning=F, message=F, results='asis', fig.align="center", out.width="100%", fig.height=ceiling((no.of.replicates)/2)*3+0.5+ifelse(no.of.replicates<2,2,0)}
if("p_per_group_splitByReplicate" %in% names(plot_list)){
  cat("#### Gene body coverage per sample split by replicate, colored by group: \n", fill=T)
  plot(plot_list$p_per_group_splitByReplicate)
  cat("\n", fill=T)
}
```


```{r GeneBodyCoverage_splitByGroupReplicate, echo=F, error=F, warning=F, message=F, results='asis', fig.align="center", out.width="100%", fig.height=ceiling((no.of.replicates*no.of.groups)/2)*3+0.5+ifelse(no.of.replicates*no.of.groups<2,2,0)}
if("p_per_cells_splitByGroupReplicate" %in% names(plot_list)){
  cat("#### Gene body coverage per sample split by group and replicate, colored by cell control type: \n", fill=T)
  plot(plot_list$p_per_cells_splitByGroupReplicate)
  cat("\n", fill=T)
}
```



```{r strandspecificity,  echo=F, results='asis', error=F, warning=F, message=F}
cat("## Strand specificity\n")

cat("\nThe table below shows the fraction of reads mapped in sense or antisense to gene exons - around 0.5 for non-stranded library prep protocols and close to 0 or 1 for strand-specific RNA-seq protocols. The strandedness is calculated as the percentage of unambiguous reads that are on the strand, which is expected by the library preparation protocol.\n\n")

DEhelper.strandspecificity(samplePattern=SHINYREPS_SAMPLEPATTERN1, targetsdf=targets4plots, excludePattern=SHINYREPS_SAMPLEPATTERN2, maxno=SHINYREPS_MAXNO)
```


## Qualimap

Illustrate genomic origin of reads

```{r Qualimap, fig.height=min(170, ceiling(0.12*fastqc_summary_plot$no.of.samples)+1)}
DEhelper.Qualimap(targetsdf=targets4plots, samplePattern=SHINYREPS_SAMPLEPATTERN1, excludePattern=SHINYREPS_SAMPLEPATTERN2, maxno=SHINYREPS_MAXNO)
```


## RNA class representation ##

The following plot shows the fraction of reads assigned to various RNA classes. These plots help in determining if the sample prep protocol worked well and may reveal issues with rRNA contamination. 

```{r RNAtypes_paragraph, echo=F, results='asis', error=F, warning=F, message=F, fig.height=min(170, ceiling(0.12*fastqc_summary_plot$no.of.samples)+1)}
DEhelper.RNAtypes(targetsdf=targets4plots, samplePattern=SHINYREPS_SAMPLEPATTERN1, excludePattern=SHINYREPS_SAMPLEPATTERN2, maxno=SHINYREPS_MAXNO)
```


# Mapping Statistics

```{r STAR_paragraph_violin, results='asis', message=FALSE, warning=FALSE, eval=nrow(targets)>100}
##### parameters to set:
# If less then 100 sample files, the bar plot is used below.
#
# You can define columns of targets.txt to be used for dot color in the summary plot (one plot per element of colorByFactor will be created). 
# The function will try to map the data from targets$file to the STAR log file names (for this, the unique part of 
# targets$file must be a substring of the file name). If you have STAR log files from pooled fastq files but cell-wise 
# information in targets.txt as in MARS-Seq, you may provide another customized targets object in 'targetsdf'. 
# Otherwise pruned file names will be used by default.
#####
if (SHINYREPS_SEQTYPE=="tenX") {
  cat("\nMapping Statistics can be found in the cellranger web summary.\n")
} else {
  
  cat("\nMapping to the reference genome & transcriptome is performed with STAR (https://github.com/alexdobin/STAR). The program version, genome assembly and software parameters are described in the table at the end of the report.

The mapping statistics below show the number input raw reads and percentages of uniquely mapped reads, multi-mapped reads aligning equally well to multiple (up to 10) positions in the genome and overall mapped reads. For the multimapped reads, one random alignment among the best mapping positions is retained.\n\n")
  
  star_stats_violin <- DEhelper.STAR.violin(targetsdf=targets4plots, colorByFactor=colorByFactor)

  cat("#### Mapping summary\n", fill=T)
    for(i in 1:length(star_stats_violin$p_mapped)){
      grid.arrange(grobs=star_stats_violin$p_mapped[[i]], layout_matrix= matrix(c(1,2), nrow=1)) 
    }
  cat("\n", fill = T)

  cat("#### Mapping statistics\n", fill=T)
  star_stats_violin$stat
  
}
```


```{r STAR_paragraph_barplot, echo=F, results='asis', error=F, warning=F, message=F, fig.align="center", out.width="100%", fig.height=(nrow(targets)/5)+2, eval=nrow(targets)<=100}
# plot mapping statistics as bar plot (only done if less than 100 samples)
if (SHINYREPS_SEQTYPE=="tenX") {
  cat("\nMapping Statistics can be found in the cellranger web summary.\n")
} else {
star_stats <- DEhelper.STAR(targetsdf=targets4plots)

cat("Mapping to the reference genome & transcriptome is performed with STAR (https://github.com/alexdobin/STAR). The program version, genome assembly and software parameters are described in the table at the end of the report.

The mapping statistics below show the number and percentage of: (i) input raw reads, (ii) uniquely mapped reads, (iii) multi-mapped reads aligning equally well to multiple (up to 10) positions in the genome, (iv) reads that align to too many (>10) genome loci are discarded, (v) unmapped reads. For the multimapped reads, one random alignment among the best mapping positions is retained. 

While both unique and multi-mapped reads are included in the generation of browser coverage tracks, only uniquely mapped reads are typically taken into account in the DESeq2 differential expression analysis in order to avoid potential false-positives. However, this standard analysis approach may also exclude some recently duplicated genes or gene-pseudogene pairs, resulting in false-negatives. To investigate the potential expression changes in such genes, a side branch of DESeq2 is run using all mapped reads and the results of both branches are compared.\n\n")

cat("#### Mapping summary\n", fill=T)
plot(star_stats$p_perc_count)
cat("\n", fill = T)

cat("#### Mapping statistics\n", fill=T)
star_stats$stat
}
```


```{r umicount, echo=F, results='asis', error=F, warning=F, message=F, eval= (SHINYREPS_SEQTYPE=="MARSseq")}
##### parameters to set:
# define subset of umicount log files if desired (samplePattern=NULL (default) includes all files).
# define categories of targets.txt to be used for dot color in the plot (one plot per element of colorByFactor will be created). 
# The function will try to map the data from targets$file to the umicount log file names (for this, the unique part of 
# targets$file must be a substring of the file name). If you have umi-tools log files from pooled fastq files but cell-wise 
# information in targets.txt as in MARS-Seq, you may provide another customized targets object in 'targetsdf'. 
# Otherwise pruned file names will be used by default.
#####
if(SHINYREPS_SEQTYPE=="MARSseq") {
cat("## Read de-duplication and feature counting with UMI-tools

UMI-tools counts the mapped reads per feature obtained after de-duplication. UMI-tools uses the uniquely mapped reads from STAR as well as as the multimapped reads (but not those which are mapped to too many loci) as input reads.")

DEhelper.umicount(colorByFactor=colorByFactor, targetsdf=targets4plots)
}
```


## Competetive mapping to rRNAs and other contaminants:

A competitive mapping to the reference genome and known rRNAs (H. sapiens, M. musculus, C. elegans, D. melanogaster, D. rerio, X. tropicalis) as well as frequently occurring Mycoplasma species (M. arginini, M. fermentans, M. hominis, M. hyorhinis, M. orale, and Acholeplasma laidlawii), PhiX and ERCC spike-in sequences, E. coli and B. taurus (bovine) was performed with FastQScreen (http://www.bioinformatics.babraham.ac.uk/projects/fastq_screen/).

In the following plot only relevant contaminants are shown. A contaminant is considered relevant if it consumes `r SHINYREPS_FASTQSCREEN_PERC`% of reads or more in at least one sample. The "one genome" label refers to the amount of reads mapping to that genome only, no matter if they are mapping uniquely or multiple times. The "multiple genome" label refers to the amount of read mapping to this genome but also to other genomes in the screening regardless of whether they are uniquely or multi mapping. 

```{r fastqscreen_paragraph, echo=F, results='asis', fig.width=10, error=F, warning=F, message=F}
fastqscreen_plots <- DEhelper.fastqscreen(perc.to.plot=SHINYREPS_FASTQSCREEN_PERC, ncol=3, targetsdf=targets4plots, samplePattern=SHINYREPS_SAMPLEPATTERN1, excludePattern=SHINYREPS_SAMPLEPATTERN2, maxno=SHINYREPS_MAXNO)
```

```{r fastqscreen_paragraph2, echo=F,results='asis',error=F,warning=F,message=F,fig.align='center', fig.height=min(170, (round((1.5*fastqscreen_plots$no.of.rows))+1)*fastqscreen_plots$no.of.samples/8+1)}
  if(is.null(fastqscreen_plots$errortext)) {
    plot(fastqscreen_plots$p.category.wrap)
    } else {
      cat(fastqscreen_plots$errortext)
      }
```


## UCSC Genome Browser tracks ##

The browser tracks are generated using bamCoverage tool in [deepTools](https://deeptools.readthedocs.io/en/develop/) and they are normalized using the Counts Per Million mapped reads (CPM) method. Both unique and multi-mapped reads are included in the generation of browser coverage tracks.

To view the tracks in the UCSC Genome Browser, navigate to the [UCSC Genome Browser Track Hub page](https://genome-euro.ucsc.edu/cgi-bin/hgHubConnect) and enter the following line into the `URL` field:

```{r Trackhub, echo=F, results='asis', error=F, warning=F, message=F}
cat(DEhelper.Trackhub(), sep="\n")
```


## Library complexity assessment ##

Measuring the fraction of duplicated reads is commonly used to control for excessive PCR over-amplification during library preparation and/or for suboptimal amount of input material. However, in RNA-seq duplicated reads may also arise naturally due to the highly expressed (e.g. housekeeping) genes, which makes the overall read duplication rate useless. The IMB-developed tool dupRadar (http://bioconductor.org/packages/release/bioc/html/dupRadar.html) gives an informative insight into the duplication problem by graphically relating the gene expression levels and the observed duplication rates. Thus, problematic samples can be easily identified by visual inspection.

```{r Dupradar_paragraph, echo=F, results='asis', error=F, warning=F, message=F}
cat(DEhelper.dupRadar(web=F, targetsdf=targets4plots, samplePattern=SHINYREPS_SAMPLEPATTERN1, excludePattern=SHINYREPS_SAMPLEPATTERN2, maxno=SHINYREPS_MAXNO), sep="\n")
```


# Low-level analysis of single-cell RNAseq data

The analysis workflow is based on the Bioconductor packages *scater* and *scran* as well as the Bioconductor workflows by Lun ATL, McCarthy DJ, & Marioni JC *A step-by-step workflow for low-level analysis of single-cell RNA-seq data.* F1000Res. 2016 Aug 31 [revised 2016 Oct 31];5:2122 and Amezquita RA, Lun ATL et al. *Orchestrating Single-Cell Analysis with Bioconductor* Nat Methods. 2020 Feb;17(2):137-145.

```{r loading_counts, echo=F, message=FALSE, error=TRUE, warning=TRUE}

## load counts for MARSseq design (pooled count files) 
switch(SHINYREPS_SEQTYPE,
    MARSseq={
        # load counts
        f <- list.files(paste0(SHINYREPS_UMICOUNT), pattern="\\.tsv$", full.names=TRUE)
        counts <- mclapply(f, read.delim, head=T, row.names=1, mc.cores=CORES)
        names(counts) <- basename(f)

        # pool-wise replacement of barcodes by sample names. A pool consists of those cells which are amplified together 
        # in one pool (i.e. all reads have identical pool barcode) but which all have different unique cell barcodes.
        # The cell barcodes are re-used in other pools. For assignment of cell barcodes given in 'targets.txt' to the 
        # pooled count data, the pool ID given in 'targets.txt' must be a substring of the respective count data filename
        # (no matter at which position).
        for (i in unique(targets$pool)) {
            if(sum(grepl(i, names(counts), ignore.case = T)) !=1) {stop("\ncannot unambiguously assign pool ID given in targets.txt to count filename")}
            current_pool <- targets[targets$pool ==i, ]
            countname <- grep(i, names(counts), ignore.case = T, value=T )
            colnames(counts[[countname]]) <- current_pool[match(colnames(counts[[countname]]), current_pool$barcode), "sample"]
        }
        
        # merge pools into one dataset with union of genelists
        custom.merge <- function(x,y) { # custom merge function for Reduce
            z <- merge(x,y, all=T, by="row.names")
            rownames(z) <- z$Row.names
            return(z[,-1])
        }
        counts <- Reduce(custom.merge, counts)
        
        counts[is.na(counts)] <- 0 # set NAs in the dataset to zero counts
        targets <- targets[targets$sample %in% colnames(counts),] # adjust targets in case of cells have been lost during processing
        if (!all(colnames(counts) %in% targets$sample)) {stop("column names of count matrix and entries in targets$sample do not match!")}
        counts <- counts[,match(targets$sample, colnames(counts))] # sort counts according to target
        # table(colnames(counts) == targets$sample) # all TRUE
    },
    SmartSeq2={
      # load counts
        f <- list.files(paste0(SHINYREPS_SUBREAD), pattern="\\.readcounts\\.tsv$", full.names=TRUE) # don't select raw_readcounts.tsv
        f <- f[!grepl("Undetermined", f)] # don't use count file with unmapped reads if present
        
        counts <- mclapply(f, read.delim, header=F, row.names=1, comment.char = "#")
        stopifnot(all(sapply(counts, function(x) all(rownames(x) == rownames(counts[[1]])))))
        counts <- do.call(cbind, counts)
        colnames(counts) <- basename(f)

      # align with targets.txt
        if(any(duplicated(targets$file))) { # check for duplicated entries in targets.txt
          stop("Duplicated entries in targets.txt: ", paste(targets$file[duplicated(targets$file)], collapse=", "))
        }
     
        logindex_targetsfile <- sapply(targets$file, grep,  colnames(counts)) # grep targets in counts
        targets <- targets[sapply(logindex_targetsfile, length) ==1, ] # remove targets entries not (uniquely) found in counts

       if(!identical(sort(unname(unlist(logindex_targetsfile))), 1:length(colnames(counts)))) { 
         # check if all counts samples are contained in targets.txt
         stop("Sample names not included in targets.txt: ", paste(colnames(counts)[-logindex_targetsfile], collapse=", "))
       }
       counts <- counts[,unlist(logindex_targetsfile)] # cols in same order as rows in targets   
       colnames(counts) <- targets$sample
    },
    tenX={
      # matrix_dir already defined above 
      barcode.path <- file.path(matrix_dir, "barcodes.tsv.gz")
      barcode.names <- read.delim(barcode.path, header = FALSE, stringsAsFactors = FALSE)
      features.path <- file.path(matrix_dir, "features.tsv.gz")
      feature.names = read.delim(features.path, header = FALSE, stringsAsFactors = FALSE)
      matrix.path <- file.path(matrix_dir, "matrix.mtx.gz")
      counts <- Matrix::readMM(file = matrix.path)
      colnames(counts) = barcode.names$V1
      rownames(counts) = feature.names$V1
      counts <- counts[feature.names$V3 == "Gene Expression",]
      counts <- counts[,targets$cell_id] # subset cell barcodes by targets (e.g. for successful cell hashing)
    },
    stop(c("Don't find seqtype:", SHINYREPS_SEQTYPE))   
)


#### create SingleCellExperiment object
sce <- SingleCellExperiment(assays=list(counts=as.matrix(counts)), colData = targets)

# add gene symbols from annotation file to SingleCellExperiment object
rowData(sce)$ENSEMBL <- rownames(sce)
rowData(sce)$SYMBOL  <- gtf$gene_name[match(rownames(sce), gtf$gene_id)]
rowData(sce)$CHR  <- as.character(seqnames(gtf))[match(rownames(sce), gtf$gene_id)]

# get rid of NAs and duplicated names (missing SYMBOLS are replaced with ENSEMBL IDs)
new.names <- rowData(sce)$SYMBOL
missing.name <- is.na(rowData(sce)$SYMBOL)
new.names[missing.name] <- rowData(sce)$ENSEMBL[missing.name]

# duplicated names are extended with ENSEMBL IDs
dup.name <- new.names %in% new.names[duplicated(new.names)]
new.names[dup.name] <- paste0(new.names, "_", rowData(sce)$ENSEMBL)[dup.name]
rowData(sce)$SYMBOL <- new.names

# add gene length information (not needed)
# rowData(sce)$genelength <- gene.lengths[ match(rownames(sce),names(gene.lengths)) ]

```


```{r droplets_emptyDrops, echo=F, message=FALSE, results='asis', eval=(SHINYREPS_SEQTYPE == "tenX" && grepl("raw_feature_bc_matrix", matrix_dir))}

## ATTENTION: CellRanger version 3 automatically performs cell calling using an algorithm similar to emptyDrops().
# Since we load the FILTERED count matrix, we can go straight to computing other QC metrics and do
# not need to run this code chunk.
# However, sometimes it may be desirable to load the UNFILTERED (raw) matrix and apply emptyDrops() ourselves, 
# on occasions where more detailed inspection or control of the cell-calling statistics is desired.
# If intended to do so, specify the path to the raw matrix in 'matrix_dir' above.

cat("## Detect empty droplets\n\n")

cat("\nAn unique aspect of droplet-based data is that we have no prior knowledge about whether a particular library (i.e., cell barcode) corresponds to cell-containing or empty droplets. Thus, we need to call cells from empty droplets based on the observed expression profiles. This is not entirely straightforward as empty droplets can contain ambient (i.e., extracellular) RNA that can be captured and sequenced, resulting in non-zero counts for libraries that do not contain any cell.\n")

cat("\nknee plot: Total UMI count for each barcode in the PBMC dataset, plotted against its rank (in decreasing order of total counts). The inferred locations of the inflection and knee points are also shown.")

bcrank <- barcodeRanks(counts(sce))
# Only showing unique points for plotting speed.
uniq <- !duplicated(bcrank$rank)
plot(bcrank$rank[uniq], bcrank$total[uniq], log="xy",
    xlab="Rank", ylab="Total UMI count", cex.lab=1.2)

abline(h=metadata(bcrank)$inflection, col="darkgreen", lty=2)
abline(h=metadata(bcrank)$knee, col="dodgerblue", lty=2)

legend("bottomleft", legend=c("Inflection", "Knee"), 
        col=c("darkgreen", "dodgerblue"), lty=2, cex=1.2)

cat("\nTesting for empty droplets\n")
set.seed(100)
e.out <- emptyDrops(counts(sce))
summary(e.out$FDR <= 0.001)
table(Sig=e.out$FDR <= 0.001, Limited=e.out$Limited)

cat("\nemptyDrops() assumes that barcodes with low total UMI counts are empty droplets. Thus, the null hypothesis should be true for all of these barcodes. We can check whether the hypothesis testing procedure holds its size by examining the distribution of p-values for low-total barcodes with test.ambient=TRUE\n")

set.seed(100)
limit <- 100   
all.out <- emptyDrops(counts(sce), lower=limit, test.ambient=TRUE)
hist(all.out$PValue[all.out$Total <= limit & all.out$Total > 0],
    xlab="P-value", main="", col="grey80") 

cat("\nIdeally, the distribution should be close to uniform. Large peaks near zero indicate that barcodes with total counts below lower are not all ambient in origin. This can be resolved by decreasing lower further to ensure that barcodes corresponding to droplets with very small cells are not used to estimate the ambient profile. Once we are satisfied with the performance of emptyDrops(), we subset our SingleCellExperiment object to retain only the detected cells.\n")

sce <- sce[,which(e.out$FDR <= 0.001)]

# While emptyDrops() will distinguish cells from empty droplets, it makes no statement about the quality of the cells.
# Filtering on the mitochondrial proportion provides is still necessary.
# Since emptyDrops() already removes cells with very low library sizes or (by association) low numbers of expressed genes, 
# further filtering on these metrics is not strictly necessary. It may still be desirable to filter on both of these metrics
# to remove non-empty droplets containing cell fragments or stripped nuclei that were not caught by the mitochondrial filter. 
# However, this should be weighed against the risk of losing genuine cell types.

cat("\nFor routine analyses, there is usually no need to remove the ambient contamination from each library.\n")

```



## Quality control of cells and RNA sequenced

To assess if the sequenced libraries are usable and the RNA captured represents a meaningful fraction of the RNA present in the cell, we are focussing on the following factors:

* **Library size:** for cells with small library size the RNA was not efficiently captured.
* **Number of expressed genes:** few expressed genes suggest that a diverse transcript population was not captured.
* **Proportion of reads mapping to mitochondrial genes:** high proportion mean increased apoptosis and/or loss of cytoplasmatic RNA from lysed cells.

To remove outliers, these criteria may be filtered either for relative thresholds using the median absolute deviation (MAD) or by setting absolute threshold after inspecting the quality control plots.

```{r quality_plots_of_cells, results='asis'}
##### parameters to set:
# define set of categories used to annotate cells in the plots (e.g.: annoFactors <- c("group", "pool"))
annoFactors <- colorByFactor2
#####

# define mitochondrial genes as control features
if (!is.na(mito.genes)) {
 is.mito  <- row.names(sce) %in% mito.genes # use predefined list with mitochondrial genes
 cat("\n", sum(is.mito), "mitochondrial genes identified by pre-defined mitochondrial gene list.\n")
 } else {
 is.mito <- rowData(sce)$CHR %in% c("chrM", "chrMT", "M", "MT")   
 cat("\n", sum(is.mito), "genes identified on mitochrondrial chromosome.\n")
 #is.mito <- grepl("^MT-", rowData(sce)$SYMBOL, ignore.case = TRUE) # use all genes starting with "MT-"
 #cat("\n", sum(is.mito), "mitochondrial genes identified by gene name starting with 'MT-'.\n")
 }

# calculate QC metrics
sce <- addPerCellQC(sce, percent_top =2, subsets=list(Mito=is.mito))  

qc.frame <- colData(sce)
qc.frame$Lib.size <- qc.frame$sum/1e3 # library size in thousands
qc.frame <- as.data.frame(qc.frame)

# generate plots for Lib.size, detected features, subsets_Mito_percent
# histograms 
# todo: include 2nd lapply for separating by multiple annoFactors including no separation at all (need 2nd for loop for printing below).
# todo: Also include MAD lines separated by annoFactor but first remove all the cells which hardly have any counts to avoid bias for relative thresholds (as done in filtering below). Create temporary qc.frame for this.
qc.plots <- lapply(c("Lib.size", "detected", "subsets_Mito_percent"), function(to.plot){
      if(to.plot=="Lib.size"){
        xlabel="Lib. sizes in thousands"
         }else{
           xlabel=to.plot}
      # if(to.plot=="subsets_Mito_percent") { madOrient="high"} else {madOrient="low"}
      # plot.list <- lapply(annoFactors, function(separation){}) # 2nd lapply for annofactors
      p <- ggplot(qc.frame, aes_string(to.plot, fill=annoFactors[1]))+ 
            geom_histogram(position="identity", alpha=0.5, bins=100) + # , col="grey80"
            # geom_vline(aes(xintercept = median(na.omit(qc.frame[,to.plot])), col="median"), linetype=2)  +
            # geom_vline(aes(xintercept = MAD(na.omit(qc.frame[,to.plot]), as.numeric(SHINYREPS_NMADS))[[madOrient]], col=madOrient), linetype=2) +
            # scale_color_manual(name = "Statistics", values = c(median = "blue", high="red")) +
            scale_fill_brewer(palette = "Set1") +
            xlab(xlabel) +
            ylab(paste0("# cells")) + 
            theme(legend.position="top")
          return(p)
})


# violinplots
qc.plots.violin <- lapply(c("Lib.size", "detected", "subsets_Mito_percent"), function(to.plot){ 
  if(to.plot=="Lib.size"){
    ylabel <- "Lib. sizes in thousands"
  }else{
    ylabel <- to.plot
  }
  
  p <- ggplot(qc.frame, aes_string(annoFactors[1],to.plot,color=annoFactors[1]))+
      geom_violin() +
      geom_quasirandom() +
      scale_fill_hue(l=40, c=40) +
      ylab(ylabel) +
      xlab(annoFactors[1]) +
      theme(axis.text.x=element_text(angle=45, vjust=1, hjust=1)) + 
      theme(legend.position="top")
    return(p)
 })


for(i in 1:length(qc.plots)) {
  grid.arrange(qc.plots[[i]], qc.plots.violin[[i]], ncol=2)
}

```



**Top 2% biggest libraries (based on mapped reads on features)**

```{r top_1perc_cells}
# Output the cells with a library size in the top 2%
highest.lib.size <- colData(sce)[sce$sum > quantile(sce$sum, 0.98),
              c("sum",
               "detected", 
               "subsets_Mito_percent",
               group.vars)]
highest.lib.size <- highest.lib.size[order(highest.lib.size$sum, decreasing = T),]
highest.lib.size$subsets_Mito_percent <- round(highest.lib.size$subsets_Mito_percent, digits=2)

DT::datatable(as.data.frame(highest.lib.size), caption="2% cells with the highest library size")
```


```{r total_counts_per_plate_position, fig.width=8, fig.height= ceiling(length(unique(sce$plate))/2)*3, results='asis', eval= (SHINYREPS_SEQTYPE %in% c("MARSseq", "SmartSeq2"))}
##### parameters to set:
# define one category to be used for symbol shape in the plots.
annoFactors <- colorByFactor2
#####

cat("## Count distribution per plate position

The plots below visualize the count data distribution (not log-transformed) via plate position for the categories library size ('sum'), detected features ('detected') and percentage of mitochondrial reads ('subsets_Mito_percent') indicated by spot size. 0-cell and 10-cell controls are indicated in blue and orange, respectively. Symbol shape is defined by custom grouping.\n\n\n") 
# For each category the plates are plotted row-wise with plate number in increasing order

sce$plate_position <- paste0(sce$row, sce$col) # column "plate_position" needed for plotPlatePosition
for (size in c("sum", "detected", "subsets_Mito_percent")) {
  cat(paste("\nPlotting", size, "as spot size\n\n"))
  plates <- list()
  for (p in as.character(sort(unique(sce$plate)))) {
        plates[[p]] <- scater::plotPlatePosition(sce[, sce$plate==p], colour_by="cells",size_by=size, shape_by=annoFactors[1],
                            by_exprs_values = "counts", theme_size = 10,  point_alpha = 0.6, point_size = 3, add_legend = F) + 
          ggtitle(label=paste0("Plate ", p, ": ", size, " (range: ", paste(signif(range(colData(sce[, sce$plate==p])[,size]),2), collapse=" - "), ")")) +
          theme(plot.title = element_text(color="black", size=8))
  } # plotPlatePosition uses by_exprs_values = "logcounts" by default. But if not available, uses "counts" instead

  grid.arrange(grobs=plates, layout_matrix=matrix(c(1:ceiling(length(plates))), ncol=2, byrow=TRUE))
}

```


### Correlation plots for different features

```{r corr_plots}
##### parameters to set:
# define one category to be used for symbol color in the plots.
annoFactors <- colorByFactor2
#####

if(!is.null(annoFactors)) {annoFactors <- rlang::ensym(annoFactors)} 

lib.size.scatter <- ggplot(qc.frame, aes(x=detected, 
                                        y=Lib.size,
                                        color=!!annoFactors)) +
                             geom_point() +
                             scale_color_hue(l=40, c=60) +
                             ylab("Lib. sizes in thousands") +
                             xlab("Number of expressed genes")

mit.perc.scatter <- lib.size.scatter + aes(y=subsets_Mito_percent) + ylab("% mitochondrial reads")
grid.arrange(lib.size.scatter,  mit.perc.scatter)

```

## Filtering out low quality cells

Filter criteria (see below) are selected according to quality control plots.

Summary of cells that don't pass the QC:

```{r dropout_cells, results='asis', fig.height=8, fig.width=10}
##### parameter to set:
# define filtering criteria based on previous QC plots
type_of_threshold = SHINYREPS_TYPE_OF_THRESHOLD # either "absolute" or "relative" (i.e. using MAD)
threshold_total_counts_min = as.numeric(SHINYREPS_THRESHOLD_TOTAL_COUNTS_MIN)
threshold_total_counts_max = as.numeric(SHINYREPS_THRESHOLD_TOTAL_COUNTS_MAX)
threshold_total_detected = as.numeric(SHINYREPS_THRESHOLD_TOTAL_FEATURES_DETECTED)
threshold_pct_counts_Mt = as.numeric(SHINYREPS_THRESHOLD_PCT_COUNTS_MT)
NMADS = as.numeric(SHINYREPS_NMADS) # number of absolute deviations from median. Only relevant if type_of_threshold = "relative".
apply_QCfilter_by_factor = if(gtools::invalid(SHINYREPS_APPLY_QCFILTER_BY_FACTOR) || toupper(SHINYREPS_APPLY_QCFILTER_BY_FACTOR) == "NULL") {
  NULL} else {colData(sce)[,SHINYREPS_APPLY_QCFILTER_BY_FACTOR]}  # apply relative threshold separated by factor
# define QC metrics to be used for PCA
qcmetrics = c("sum", "detected", "subsets_Mito_percent")
# define one category to be used for symbol shape in the plot.
annoFactors <- colorByFactor2
# name individual samples failed in raw data QC
rawfail <- NULL
#####

# if(!exists("sce.beforeFilt")) {sce.beforeFilt <- sce}

## visualize selected thresholds

if(type_of_threshold=="absolute") { ### use absolute thresholds
  
  qc.drop <-  data.frame(row.names = colnames(sce)) # initialize
  qc.drop$libsize <- (sce$sum < threshold_total_counts_min) | (sce$sum > min(threshold_total_counts_max, max(sce$sum), na.rm=T))
  qc.drop$features <- sce$detected < threshold_total_detected
  qc.drop$mito <- sce$subsets_Mito_percent > threshold_pct_counts_Mt
  qc.drop$controls <- if("cells" %in% colnames(colData(sce))) {colData(sce)$cells != "1c"} else {F}
  qc.drop$rawQC <- if(!is.null(rawfail)) {!apply(qc.drop, 1, any) & colnames(sce) %in% unique(rawfail)} else {F} # consider rawQC only if no other QC failed
  qc.drop$pass <- !apply(qc.drop, 1, any)
  qc.drop <- qc.drop[match(colnames(sce), rownames(qc.drop)), ]  # sort to match the 'sce' cell order

  if(!gtools::invalid(threshold_total_counts_max)) {
    threshold_total_counts_max <- paste("or >", threshold_total_counts_max)
        } else {threshold_total_counts_max <- NULL}   # prepare string for output
  
 
  if(!is.null(annoFactors)) { # prepare cell counts by factor if present
    data_by_factor <- cbind(qc.drop, tempfactor=factor(colData(sce)[,annoFactors])) %>%
      dplyr::group_by(tempfactor) %>%
      dplyr::summarize(counts_unfiltered = paste0(length(libsize), " (", sprintf("%1.1f%%", 100*length(libsize)/ncol(sce)), ")"),
                       libsize = paste0(sum(libsize), " (", sprintf("%1.1f%%", 100*sum(libsize)/length(libsize)), ")"),
                       features = paste0(sum(features), " (", sprintf("%1.1f%%", 100*sum(features)/length(features)), ")"),
                       mito = paste0(sum(mito), " (", sprintf("%1.1f%%", 100*sum(mito)/length(mito)), ")"),
                       rawQC = paste0(sum(rawQC), " (", sprintf("%1.1f%%", 100*sum(rawQC)/length(rawQC)), ")"),
                       controls = paste0(sum(controls), " (", sprintf("%1.1f%%", 100*sum(controls)/length(controls)), ")"),
                       pass = paste0(sum(pass), " (", sprintf("%1.1f%%", 100*sum(pass)/length(pass)), ")")) %>%
      dplyr::ungroup() %>%
      column_to_rownames('tempfactor') %>%
      t()
  } else {data_by_factor <- NULL}
  
  qcfailed <- cbind(
      criterion=c(counts_unfiltered = "counts unfiltered", 
                  libsize = paste("total counts <", threshold_total_counts_min, threshold_total_counts_max),
                  features = paste("total features <", threshold_total_detected),
                  mito = paste0("mitochondrial counts > ", threshold_pct_counts_Mt, "%"),
                  rawQC = "cells failed in raw data QC",
                  controls = "control wells (0c, 10c)",
                  pass = paste("remaining")),
      data_by_factor, 
      cell_count_total=c(ncol(sce),
                         paste0(sum(qc.drop$libsize), " (", sprintf("%1.1f%%", 100*sum(qc.drop$libsize)/ncol(sce)), ")"),
                         paste0(sum(qc.drop$features), " (", sprintf("%1.1f%%", 100*sum(qc.drop$features)/ncol(sce)), ")"),
                         paste0(sum(qc.drop$mito), " (", sprintf("%1.1f%%", 100*sum(qc.drop$mito)/ncol(sce)), ")"),
                         paste0(sum(qc.drop$rawQC), " (", sprintf("%1.1f%%", 100*sum(qc.drop$rawQC)/ncol(sce)), ")"),
                         paste0(sum(qc.drop$controls), " (", sprintf("%1.1f%%", 100*sum(qc.drop$controls)/ncol(sce)), ")"),
                         paste0(sum(qc.drop$pass), " (", sprintf("%1.1f%%", 100*sum(qc.drop$pass)/ncol(sce)), ")")
                         )
      )

  
  } else {
    if(type_of_threshold=="relative") { ### use relative thresholds
      
     # Before we apply relative QC filters we remove all the cells which hardly have any counts to avoid bias for relative thresholds
     min_readcount <- 100
     count.drop <- sce$sum < min_readcount
     cat("We drop ", sum(count.drop), " cells with less than", min_readcount, "reads counted, because cells which hardly have any counts would bias the relative thresholding\n") 
     sce <- sce[,!count.drop]
  
     qc.drop <-  data.frame(row.names = colnames(sce)) # initialize
     qc.drop$libsize <- isOutlier(sce$sum, nmads=NMADS, type="lower", log=TRUE, batch=apply_QCfilter_by_factor)
     qc.drop$features <- isOutlier(sce$detected, nmads=NMADS, type="lower", log=TRUE, batch=apply_QCfilter_by_factor) 
     qc.drop$mito <- isOutlier(sce$subsets_Mito_percent, nmads=NMADS, type="higher", batch=apply_QCfilter_by_factor)
     qc.drop$controls <- if("cells" %in% colnames(colData(sce))) {colData(sce)$cells != "1c"} else {F}
     qc.drop$rawQC <- if(!is.null(rawfail)) {!apply(qc.drop, 1, any) & colnames(sce) %in% unique(rawfail)} else {F} # consider rawQC only if no other QC failed
     qc.drop$pass <- !apply(qc.drop, 1, any)
     qc.drop <- qc.drop[match(colnames(sce), rownames(qc.drop)), ]  # sort to match the 'sce' cell order

     if(!is.null(annoFactors)) { # prepare cell counts by factor if present
      data_by_factor <- cbind(qc.drop, tempfactor=factor(colData(sce)[,annoFactors])) %>%
        dplyr::group_by(tempfactor) %>%
        dplyr::summarize(libsize = paste0(sum(libsize), " (", sprintf("%1.1f%%", 100*sum(libsize)/length(libsize)), ")"),
                         features = paste0(sum(features), " (", sprintf("%1.1f%%", 100*sum(features)/length(features)), ")"),
                         mito = paste0(sum(mito), " (", sprintf("%1.1f%%", 100*sum(mito)/length(mito)), ")"),
                         rawQC = paste0(sum(rawQC), " (", sprintf("%1.1f%%", 100*sum(rawQC)/length(rawQC)), ")"),
                         controls = paste0(sum(controls), " (", sprintf("%1.1f%%", 100*sum(controls)/length(controls)), ")"),
                         pass = paste0(sum(pass), " (", sprintf("%1.1f%%", 100*sum(pass)/length(pass)), ")")) %>%
        dplyr::ungroup() %>%
        column_to_rownames('tempfactor') %>%
        t()
      } else {data_by_factor <- NULL}
     
      qcfailed <- cbind(
        criterion=c(paste("total counts <", NMADS, "MAD"),
                    paste("total features <", NMADS, "MAD"),
                    paste("% mitochondrial counts >", NMADS, "MAD"),
                    rawQC = "cells failed in raw data QC",
                    controls = "control wells (0c, 10c)",
                    paste("remaining")), 
       data_by_factor, 
       cell_count_total=c(paste0(sum(qc.drop$libsize), " (", sprintf("%1.1f%%", 100*sum(qc.drop$libsize)/ncol(sce)), ")"),
                          paste0(sum(qc.drop$features), " (", sprintf("%1.1f%%", 100*sum(qc.drop$features)/ncol(sce)), ")"),
                          paste0(sum(qc.drop$mito), " (", sprintf("%1.1f%%", 100*sum(qc.drop$mito)/ncol(sce)), ")"),
                          paste0(sum(qc.drop$rawQC), " (", sprintf("%1.1f%%", 100*sum(qc.drop$rawQC)/ncol(sce)), ")"),
                          paste0(sum(qc.drop$controls), " (", sprintf("%1.1f%%", 100*sum(qc.drop$controls)/ncol(sce)), ")"),
                          paste0(sum(qc.drop$pass), " (", sprintf("%1.1f%%", 100*sum(qc.drop$pass)/ncol(sce)), ")")
                          )
       ) 

  } else {stop("\ntype_of_threshold must be either 'absolute' or 'relative'")}
}

if(!"cells" %in% colnames(colData(sce))) { # exclude control wells if not applicable
  qcfailed <- qcfailed[!rownames(qcfailed) %in% c("controls"), ]
  qc.drop$controls <- NULL
}
if(!any(qc.drop$rawQC)) { # exclude rawQC if not applicable
  qcfailed <- qcfailed[!rownames(qcfailed) %in% c("rawQC"), ]
  qc.drop$rawQC <- NULL
}

kbl(qcfailed, caption="QC filtering", row.names=F) %>% kable_styling()

# plot violin plots with indicated thresholds
cat("\nVioline plots with applied thresholds indicated\n")
qc.drop.mod <- qc.drop[,!colnames(qc.drop) %in% c("pass", "controls")]
qc.drop.mod <- sapply(qc.drop.mod, function(x) ifelse(x==TRUE, "removed", "kept"))
qc.plots.violin.thresholds <- lapply(qcmetrics, function(to.plot){ 
  color_threshold <- switch(to.plot, "sum" = "libsize", "detected" = "features", "subsets_Mito_percent" = "mito")
  p <- plotColData(sce, x=annoFactors[1], y=to.plot, colour_by=I(qc.drop.mod[,color_threshold])) + 
    theme(legend.position = "top", legend.text = element_text(size = 14))  
   return(p)
  })

  gridExtra::grid.arrange(grobs=qc.plots.violin.thresholds, ncol=if(is.null(annoFactors) || nlevels(factor(colData(sce)[,annoFactors])) <=3) {3} else {1})


# plot QC metrics as PCA
cat("\nThe quality metrics are summarized in a PCA\n")
plotPCAfromQCmetrics(sce, metrics=qcmetrics, anno=annoFactors, qc.drop=qc.drop) 

```




Removed cells:

```{r remove_low_qual_cells_and_control_wells, results='asis'}
##### parameters to set:
# define up to 2 categories to be used for output tables.
annoFactors <- colorByFactor2
#####

# output overview of remaining cells and table of removed cells
j <- unique(c("sum", "detected", "subsets_Mito_percent", "cells", annoFactors))
j <- j[j %in% names(colData(sce))]
x <- cbind(qc.drop, colData(sce)[match(rownames(qc.drop), rownames(colData(sce))), j])
x$subsets_Mito_percent   <- round(x$subsets_Mito_percent, digits=2) 
colnames(x) <- gsub("subsets_", "", colnames(x))
DT::datatable(x[!x$pass, ])

# filter out cells failing QC
if(length(qc.drop$pass) == ncol(sce)) { # if chunk is executed multiple times
    sce <- sce[, qc.drop$pass]
}

```


### Replotting the PCA after filtering

```{r pca2, fig.height=8, fig.width=10}
##### parameters to set:
# define up to 2 categories to be indicated in the PCA plot by color and shape.
annoFactors <- colorByFactor2
#####

try(plotPCAfromQCmetrics(sce, qcmetrics, anno=annoFactors))
```



## Top most highly expressed genes

The plot below shows the most highly expressed genes (based on un-normalized mean counts). This should generally be dominated by constitutively expressed transcripts, such as those for ribosomal or mitochondrial proteins. All mitochondrial genes are marked as "Feature control". The color represents the total number of expressed genes in the respective sample/cell.

```{r highly_expr_genes, fig.height=6, fig.width=6}
# use tmp.sce for plotting with different rownames
tmp.sce <- sce
rownames(tmp.sce) <- rowData(sce)$SYMBOL
fontsize <- theme(axis.text=element_text(size=6), axis.title=element_text(size=10))
plotHighestExprs(tmp.sce, n=50) + fontsize
rm(tmp.sce)
```


## Filtering out low abundance genes

Average counts per gene should correlate with number of cells expressing it (see plot below). 
Low abundance genes are likely to be dominated by drop-out events (Poisson noise in different cells). They do not contain enough information for statistical inference, and may compromise accuracy of continuous approximations when fitting the data (edgeR biological coefficient of variation (BCV) estimation). Here, we filter out genes with no expression (i.e. zero count) in 95% of the cells.

```{r remove_low_abundance_genes, results='asis'}
rowData(sce)$ave.count <- calculateAverage(sce) 
expressed.cells <- nexprs(sce, byrow=TRUE)   # number of cells expressing the gene
smoothScatter(log10(rowData(sce)$ave.count), expressed.cells,
              xlab=expression("Log10 average count"), ylab= "Number of expressing cells")
# is.ercc <- isSpike(sce, type="ERCC")
# points(log10(ave.counts[is.ercc]), numcells[is.ercc], col="red", pch=16, cex=0.5)

# filtering for low abundance genes
genes2keep <- expressed.cells > ceiling(.05 * ncol(sce))
cat("\n\n", sum(genes2keep), "of", length(genes2keep), "genes remain after filtering for low abundance genes.\n")
sce <- sce[genes2keep, ]

# # remove feature controls
# cat("Remove", sum(rowData(sce)$is_feature_control), "feature controls from dataset.\n")
# sce <- sce[!rowData(sce)$is_feature_control,]

```



# Normalization of cell-specific biases
Normalization is required to eliminate these cell-specific differences in capture efficiency, prior to downstream quantitative analyses.

## Normalization for library size

Size factors can be calculated with DESeq2 or edgeR, but these methods do not work well with single-cell data due to the dominance of low and zero counts. To overcome this, we use the method from Lun et al. (2016) implemented in the *cran* package, which pools counts from many cells to estimate the size factors and to finally deconvolute them to cell-specific factors. Finally, normalized log2-expression values are computed for each endogenous gene using the appropriate size factors.

```{r size_factors_and_normalize, warning=FALSE, message=FALSE, results='asis'}
##### parameters to set:
# define up to 2 categories to be indicated in the plot by color and shape.
annoFactors <- colorByFactor2
#####

cat("\nSummary of size factors by deconvolution:\n")
set.seed(100)
qclust <- quickCluster(sce) 
sce <- computeSumFactors(sce, clusters=qclust)
#lib.sf <- librarySizeFactors(sce)
summary_sf <- matrix(summary(sizeFactors(sce)))
rownames(summary_sf) <- names(summary(sizeFactors(sce)))
kbl(t(summary_sf)) %>% kable_styling(full_width = F, position = "left")
cat("\n\n")

# cat("\nSummary size factors for ERCC spike-ins\n")
# sce <- computeSpikeFactors(sce, spikes="ERCC") 
# summary(sizeFactors(sce, "ERCC"))

# plot size factors 
dotcol <- annoFactors[1]
dotcol <- rlang::ensym(dotcol)
if(length(annoFactors)>=2) { 
  dotshape <- annoFactors[2]
  dotshape <- rlang::ensym(dotshape)
} else {
  dotshape <- NULL
}

to.plot <- data.frame(sizeFactors=sizeFactors(sce),
                 total_counts=sce$sum/1e6,
                 colData(sce)[,annoFactors, drop=F])
ggplot(to.plot, aes(x=sizeFactors, 
                    y=total_counts,
                    color=!!dotcol, shape=!!dotshape))+
  geom_point()+ ylab("Library size (millions)") + scale_color_hue(l=40, c=40)

# normalise
sce <- scater::logNormCounts(sce) # adds normalized logcounts matrix
```



```{r plot_violin_top50, fig.width=12, fig.height=10, results='asis'}
##### parameters to set:
# define up to 2 categories to be indicated in the plot by color and shape.
annoFactors <- colorByFactor2
#####

cat("\n")
cat("### Normalized log2-expression of top50 genes with highest average expression\n")

# use tmp.sce to change rownames to SYMBOL for the plot
tmp.sce <- sce
rownames(tmp.sce) <- rowData(sce)$SYMBOL

# re-calculate averages, using newly determined size factors
ave.counts.new <- calculateAverage(tmp.sce)
rowData(tmp.sce)$ave.count.new <- ave.counts.new

top.sce.aver.size <- head(rowData(tmp.sce)[order(rowData(tmp.sce)$ave.count.new,decreasing=TRUE),],50)

dotcolour <- annoFactors[1]
if(length(annoFactors)>=2) { 
  dotshape <- annoFactors[2]
} else {
  dotshape <- NULL
}

scater::plotExpression(tmp.sce, features=top.sce.aver.size$SYMBOL[1:25], colour_by=dotcolour, shape_by = dotshape) + ggtitle("highest avg. expression 1:25")
scater::plotExpression(tmp.sce, features=top.sce.aver.size$SYMBOL[26:50], colour_by=dotcolour, shape_by = dotshape) + ggtitle("highest avg. expression 26:50")
```


## Checking for confounding factors 

### Classification and normalization of cell cycle phase

We use the prediction method described by Scialdone et al. (2015) to classify cells into cell cycle phases based on the gene expression data. Pre-trained classifiers are available in scran for human and mouse data. Cells are classified as being in G1 phase if the G1 score is above 0.5 and greater than the G2/M score, in G2/M phase if the G2/M score is above 0.5 and greater than the G1 score, and in S phase if neither score is above 0.5.

Remark for adjustment: in routine scRNA-seq analyses it is not recommended to adjust for cell cycle. Compared to other differences between cell types it is a minor factor of variation and any attempt at removal would also need to assume that the cell cycle effect is orthogonal to other biological processes. For example, regression would potentially remove interesting signal if cell cycle activity varied across clusters or conditions. Cell cycle adjustment may be performed on an as-needed basis in populations with clear cell cycle effects.

```{r find_cell_cycle, results='asis', fig.width=8, fig.height=8, eval=(org %in% c("human", "mouse"))}
##### parameters to set:
# define one category to be used for symbol color in the plot.
annoFactors <- colorByFactor2
#####

# load gene.pairs: "mouse_cycle_markers.rds" or "human_cycle_markers.rds"
set.seed(100)
gene.pairs <- readRDS(system.file("exdata", paste0(org, "_cycle_markers.rds"), package="scran")) # 

# determine cell cycle phase:
# in "human_cycle_markers.rds" ensembl names are without the ".number" at the very end, but 
# the rownames of our sce object include this. ---> remove .number
assignments <- cyclone(sce, pairs=gene.pairs, gene.names=sub("\\..*$","",rowData(sce)$ENSEMBL), assay.type="logcounts") 
sce$phases <- assignments$phases
cat(sum(is.na(sce$phases)), "out of", length(sce$phases),
    "cells could not be assigned to a cell cycle phase.", fill=TRUE)

if(any(!is.na(sce$phases))) {
  
  # print cell cycle table
   ccptable <- data.frame(tempfactor=factor(colData(sce)[, c(annoFactors[1])]), phases=sce$phases) %>%
     dplyr::group_by(tempfactor) %>%
     dplyr::summarize(G1 = paste0(sum(phases=="G1"), " (", sprintf("%1.1f%%", 100*sum(phases=="G1")/length(phases)), ")"),
                      G2M = paste0(sum(phases=="G2M"), " (", sprintf("%1.1f%%", 100*sum(phases=="G2M")/length(phases)), ")"),
                      S = paste0(sum(phases=="S"), " (", sprintf("%1.1f%%", 100*sum(phases=="S")/length(phases)), ")")
                      ) %>%
     dplyr::ungroup() %>%
     column_to_rownames('tempfactor') %>%
     t() %>%
     as.data.frame()
   
     ccptable$total <- c(paste0(sum(sce$phases=="G1"), " (", sprintf("%1.1f%%", 100*sum(sce$phases=="G1")/ncol(sce)), ")"),
                         paste0(sum(sce$phases=="G2M"), " (", sprintf("%1.1f%%", 100*sum(sce$phases=="G2M")/ncol(sce)), ")"),
                         paste0(sum(sce$phases=="S"), " (", sprintf("%1.1f%%", 100*sum(sce$phases=="S")/ncol(sce)), ")")
                         )
   
    print(kbl(ccptable, caption="Cell cycle phases", align = "l", format="simple") %>% kable_styling()) 
    cat("\n")
    
    # plot cell cycle phases
    plot(0, xlim=c(0, 1), ylim=c(0, 1), type="n", xlab="G1 score", ylab="G2/M score")
    points(assignments$scores$G1, assignments$scores$G2M, col=scales::alpha(pal[factor(colData(sce)[,annoFactors[1]])], .5) )   # pch=16
    # abline(h=.5, v=.5, lty=2, col="red")
    arrows(0.5,0.5,1,1, length=0, col="red", lty=2)
    arrows(0,0.5,0.5,0.5, length=0, col="red", lty=2)
    arrows(0.5,0,0.5,0.5, length=0, col="red", lty=2)

    text(x=c(.25, .75, .25, .75), y=c(.75, .75, .25, .25), labels=c("G2", " ", "S", "G1"))
    legend("bottomleft", col=c(pal[1:4]), pch=16, legend=c(levels(factor(colData(sce)[,annoFactors[1]]))))
}

## Info: adjustment for cell cycle phases (https://osca.bioconductor.org/cell-cycle-assignment.html)
  # In routine scRNA-seq analyses it is not recommended to adjust for cell cycle. Compared to other differences 
  # between cell types it is a minor factor of variation and any attempt at removal would also need to assume 
  # that the cell cycle effect is orthogonal to other biological processes. For example, regression would 
  # potentially remove interesting signal if cell cycle activity varied across clusters or conditions. 
  # Cell cycle adjustment may be performed on an as-needed basis in populations with clear cell cycle effects.
```




### Identify explanatory variables 

We check whether there are technical factors that contribute substantially to the heterogeneity of gene expression. If so, the factor may need to be regressed out to ensure that it does not inflate the variances or introduce spurious correlations.

```{r explanatory_variables, results='asis', echo=F, error=F, warning=F, message=F}
##### parameters to set:
# define potential explanatory variables to be tested for their effect
explanatoryVariables <- c(group.vars, if("phases" %in% colnames(colData(sce))){"phases"} else {NULL})
explanatoryVariables <- explanatoryVariables[!explanatoryVariables %in% c("cells", "pos")]
#####

explVar <- plotExplanatoryVariables(sce, variables=explanatoryVariables, exprs_values = "logcounts") +    
    ggtitle("Explanatory variables")

cat("\nPCA plots of top 500 genes colored by potential confounder variables\n\n")  
  sce <-scater::runPCA(sce, name = "PCA")
  explVarScatter <- lapply(explanatoryVariables, function(x) {
                    scater::plotPCASCE(sce, ncomponents = 4, point_size=1, point_alpha=0.3, colour_by=x) + 
                      ggtitle(paste("PCA plots colored by", x))
                    })

  explVar2plot <- c(list(explVar), explVarScatter)
  
```

```{r plot_explanatory_variables, results='asis', echo=F, error=F, warning=F, message=F, fig.height=6*length(explVar2plot)}
  gridExtra::grid.arrange(grobs=explVar2plot, ncol=1)
```



```{r limma_blocking_confounder, results='asis', eval=T}
# use limma for adjustment of cell cycle phase and confounder variable
library(limma)
var2adjust <- NULL
adjust_ccp <- F # adjust for cell cycle phase 
######

if(adjust_ccp | !is.null(var2adjust)) {
  
cat("## Adjust for confounder variables\n\n")


if(adjust_ccp) {
cat("We can account for possible cell cycle effect on downstream analysis, using the G1 and G2M assignment scores as a continuous blocking factor to estimate the variance. This is more graduated than using a strict assignment of each cell to a specific phase, as the magnitude of the score considers the uncertainty of the assignment. The phase covariates in the design matrix will absorb any phase-related effects on expression such that they will not affect estimation of the effects of other experimental factors. Any additionial batch variable can be included via the 'batch'-argument of the 'removeBatchEffect' function.")
    
    
    # filter out cells with undetermined cell cycle
     scoresFilteredNA <- assignments$scores[!is.na(assignments$phases), ] # NA entries are not used
     cat(sum(is.na(sce$phases)), "cells not assigned to a cell cycle phase are removed from the dataset.\n")
     sce <- sce[,!is.na(sce$phases)] 
    
    sce.no_block <- sce
    sce.no_block$G1score <- sce$G1score <- scoresFilteredNA$G1
    sce.no_block$G2Mscore <- sce$G2Mscore <- scoresFilteredNA$G2M
    
    # adjust for cell cycle phase by modelling G scores
    design <- model.matrix(~ G1 + G2M, scoresFilteredNA)
}


  if(!is.null(var2adjust)) { # includes batch effect
    
        if(adjust_ccp) {# includes cell cycle phase
          
            cat("\nVariable to adjust for (additional to cell cycle phase):", var2adjust)
            set.seed(100)
            assay(sce, "corrected") <- removeBatchEffect(logcounts(sce), batch=colData(sce)[, var2adjust], covariates=design[,-1]) 
            
            # create final_var2adjust (factor composed from var2adjust and optionally cell cycle phase)
            sce$final_var2adjust <- as.factor(paste(colData(sce)[,var2adjust], sce$phases, sep="_"))
            
             } else {
                  cat("\nNo adjustment for cell cycle phase applied.\n")
                  cat("\nVariable to adjust for:", var2adjust)
                set.seed(100)
                assay(sce, "corrected") <- removeBatchEffect(logcounts(sce), batch=colData(sce)[, var2adjust]) 
                
                # create final_var2adjust 
                sce$final_var2adjust <- as.factor(colData(sce)[,var2adjust])
            }
    
  } else { # no batch var
    cat("\nAdjustment for cell cycle scores only (no additional batch variable).\n")
    set.seed(100)
    assay(sce, "corrected") <- removeBatchEffect(logcounts(sce), covariates=design[,-1]) # scores as covariates
    
    # create final_var2adjust 
    sce$final_var2adjust <- as.factor(sce$phases)
  }


## switch assay channel for downstream processing
assay(sce, "logcounts_not_adjusted") <- assay(sce, "logcounts")
assay(sce, "logcounts") <- assay(sce, "corrected")
assay(sce, "corrected") <- NULL

# plotting 
  # if(adjust_ccp) {
      # cat("\nPCA plot of top 500 genes colored by G1 and G2M score\n")
      # set.seed(100)
      # sce.no_block <- scater::runPCA(sce.no_block, name = "PCA", exprs_values = "logcounts")
      # out1 <- plotPCASCE(sce.no_block, ncomponents=2, # dimred="PCA", 
      #          colour_by="G1score", size_by="G2Mscore") + ggtitle("Before ccp adjustment")
      # # After blocking on the phase scores
      # set.seed(100)
      # scater::runPCA(sce, name = "PCA", exprs_values = "logcounts")
      # out2 <- plotPCASCE(sce, ncomponents=2, # dimred="PCA", 
      #           colour_by="G1score", size_by="G2Mscore") + ggtitle("After ccp adjustment")
      # gridExtra::grid.arrange(out1, out2, ncol=2)
  # }

} else {
    cat("\nno adjustment applied")
}
```


```{r batchelor_blocking_confounder, results='asis', echo=F, error=F, warning=F, message=F, eval=F}
library(batchelor)

##### parameters to set:
var2adjust <- NULL
adjust_ccp <- F # adjust for cell cycle phase (default = FALSE)
#####

cat("Counfounder variables like batch effects can be adjusted using the mutual nearest neighbors method (MNN). The effect from cell cycle phase can be removed from the dataset (if necessary) via linear regression treating each phase as a separate batch. If both a confounder variable and cell cycle shall be adjusted for, a new categorical variable is composed from both factors and used for MNN correction.")

## Info: Batchelor functions for batch correction
# (http://bioconductor.org/packages/devel/bioc/vignettes/batchelor/inst/doc/correction.html#3_mutual_nearest_neighbors)
# correctExperiments() # Apply a correction to multiple SingleCellExperiment objects, while also combining the 
# assay data and column metadata for easy use. batchCorrect does the correction inside this function
# batchCorrect() # A common interface for single-cell batch correction methods
# fastMNN() # For scRNA-seq data, fastMNN() tends to be both faster and better at achieving a satisfactory merge than mnnCorrect()
# mnnCorrect() # The original method described by Haghverdi et al. (2018), is mainly provided here for posteritys sake
#     The MNN-corrected values can be used for further correction with mnnCorrect().
#     This is useful in nested experimental designs involving multiple batches within each of multiple studies. 
#     Users should set cos.norm.in=FALSE and cos.norm.out=FALSE when supplying mnnCorrect() with MNN-corrected values. 
#     This ensures that the cosine normalization is only applied once, during the first round of MNN correction.
#     (https://bioc.ism.ac.jp/packages/3.7/workflows/vignettes/simpleSingleCell/inst/doc/work-5-mnn.html).
#     MNN-corrected values are generally not suitable for differential expression (DE) analyses.
# rescaleBatches() # conceptually equivalent to running removeBatchEffect() from limma with no covariates other than the batch. 
#     While this method is fast and simple, it makes the strong assumption that the population composition of each batch is the same. 
#     This is usually not the case for scRNA-seq experiments in real systems that exhibit biological variation. 
#     Thus, rescaleBatches() is best suited for merging technical replicates of the same sample, e.g., that have 
#     been sequenced separately.
# regressBatches() # Alternative to rescaleBatches(), a more direct linear regression of the batch effect. 
#     This does not preserve sparsity but uses a different set of tricks to avoid explicitly creating a dense matrix, 
#     specifically by using the ResidualMatrix class from the BiocSingular package.
# multiBatchNorm() # Differences in sequencing depth between batches are an obvious cause for batch-to-batch differences. 
#     These can be removed by multiBatchNorm(), which downscales all batches to match the coverage of the least-sequenced batch. 


# store unmodified logcounts channel for downstream processing
 assay(sce, "logcounts_not_adjusted") <- assay(sce, "logcounts")

if(adjust_ccp) {
 # filter out cells with undetermined cell cycle
 cat(sum(is.na(sce$phases)), "cells not assigned to a cell cycle phase are removed from the dataset.\n")
 sce <- sce[,!is.na(sce$phases)] 
}
 
 
if(is.null(var2adjust) & adjust_ccp) {
  
  # adjust cell cycle phase as batch effect (linear regression)
  cat("\nadjusting for cell cycle phase using linear regression.")
  sce$final_var2adjust <- as.factor(sce$phases)
  set.seed(100)
  sce.adjust <- batchelor::rescaleBatches(sce, batch=sce$final_var2adjust, assay.type = "logcounts")
  assay(sce, "logcounts") <- assay(sce.adjust, "corrected")

  } else {
  
  if(!is.null(var2adjust)) {
    
      # filter out cells with missing value for var2adjust
      cat(sum(is.na(colData(sce)[,var2adjust])), "cells with missing value for", var2adjust, "are removed from the dataset.\n")
      sce <- sce[,!is.na(colData(sce)[,var2adjust])] 
    
      # create final_var2adjust (factor composed from var2adjust and optionally cell cycle phase)
      sce$final_var2adjust <- if(adjust_ccp) {paste(colData(sce)[,var2adjust], sce$phases, sep="_")} else {colData(sce)[,var2adjust]}
      sce$final_var2adjust <- as.factor(sce$final_var2adjust)
      
      # adjust for final_var2adjust as batch effect (mutual nearest neighbours, MNN)
      cat("\nadjusting for", var2adjust, if(adjust_ccp) {"and cell cycle phase"}, "using MNN")
      set.seed(100)
      sce.adjust <- batchelor::fastMNN(sce, batch=sce$final_var2adjust, assay.type = "logcounts")
      reducedDim(sce, "corrected") <- reducedDim(sce.adjust, "corrected") # will be compared with PCA slot
      assay(sce, "logcounts") <- assay(sce.adjust, "reconstructed") # class LowRankMatrix (not compatible with trendVar) 
      
  } else {
    cat("\nno adjustment applied")
  }
}

```


```{r explanatory_variables_2, echo=F, error=F, warning=F, message=F, fig.width=8, fig.height=6*length(explVar2plot), results='asis', eval=(adjust_ccp | !is.null(var2adjust))}

if(adjust_ccp | !is.null(var2adjust)) {

  cat("\nRe-plot explanatory variables after adjustment\n\n")
  explVarAdj <- plotExplanatoryVariables(sce, variables=explanatoryVariables, exprs_values = "logcounts") +
                ggtitle("Explanatory variables after adjustment")

cat("\nPCA plots of top 500 genes colored by potential confounder variables after correction\n\n")
  sce <-scater::runPCA(sce, name = "PCA")
  explVarScaterAdj <- lapply(explanatoryVariables, function(x) {
                    scater::plotPCASCE(sce, ncomponents = 4, point_size=1, point_alpha=0.3, colour_by=x) +
                          ggtitle(paste("PCA plots after adjustment colored by", x))
                    })

  explVar2plotAdj <- c(list(explVarAdj), explVarScaterAdj)
  gridExtra::grid.arrange(grobs=explVar2plotAdj, ncol=1)
}
```



# Identifying highly variable genes (HVGs)

The per-gene variation is quantified by computing the variance of the log-normalized expression values for each gene across all cells in the population (A. T. L. Lun, McCarthy, and Marioni 2016). This has an advantage in that the feature selection is based on the same log-values that are used for later downstream steps. In particular, genes with the largest variances in log-values will contribute the most to the Euclidean distances between cells. By using log-values here, we ensure that our quantitative definition of heterogeneity is consistent throughout the entire analysis.

Calculation of the per-gene variance is simple but feature selection requires modelling of the mean-variance relationship. The log-transformation does not achieve perfect variance stabilization, which means that the variance of a gene is driven more by its abundance than its underlying biological heterogeneity. To account for this effect, we use the modelGeneVar() function to fit a trend to the variance with respect to abundance across all genes (see scatter plot below).

```{r trendVar, echo=F, error=F, warning=F, message=F}
# estimate highly variable genes
decVar <- modelGeneVar(sce)
var.fit <- metadata(decVar)

plot(var.fit$mean, var.fit$var, xlab="Mean of log-expression",
    ylab="Variance of log-expression")
curve(var.fit$trend(x), col="dodgerblue", add=TRUE, lwd=2)
```


At any given abundance, we assume that the expression profiles of most genes are dominated by random technical noise. Under this assumption, our trend represents an estimate of the technical noise as a function of abundance. We then break down the total variance of each gene into the technical component, i.e., the fitted value of the trend at that genes abundance; and the biological component, defined as the difference between the total variance and the technical component. This biological component represents the interesting variation for each gene and can be used as the metric for HVG selection.

Once we have quantified the per-gene variation, the next step is to select the subset of HVGs to use in downstream analyses. A larger subset will reduce the risk of discarding interesting biological signal by retaining more potentially relevant genes, at the cost of increasing noise from irrelevant genes that might obscure said signal. Here, we consider the top 10% of genes with the highest biological components as HVGs for downstream analysis.

```{r get_hvg, results='asis', fig.width=11, echo=F, error=F, warning=F, message=F}
annoFactors <- colorByFactor2

# Ordering by most interesting genes for inspection.
decVar_ordered <- decVar[order(decVar$bio, decreasing=TRUE),] 
decVar_ordered <- apply(decVar_ordered, 2, signif, digits=4)
decVar_ordered <- data.frame(SYMBOL=rowData(sce)$SYMBOL[match(rownames(decVar_ordered), rownames(sce))], decVar_ordered)
hvg <- getTopHVGs(decVar, prop=0.1)
rowSubset(sce, "HVGs") <- hvg

cat(paste(length(hvg), "genes are considered as HVGs.\n"))
DT::datatable(as.data.frame(decVar_ordered)[hvg,])


# plot top 10 HVGs
if(length(annoFactors)>2) {stop("\nno more than 2 categories allowed in 'anno'.")}
dotcol <- annoFactors[1]
if(length(annoFactors)==2) { 
    dotshape <- annoFactors[2]
} else {
    dotshape <- NULL
}

if(length(hvg) == 0) {
    cat("\nNo highly variable genes found.\n")
} else {
  
  # # print violinplots summarized by sample
  # tmp.sce <- sce    
  # rownames(tmp.sce) <- rowData(tmp.sce)$SYMBOL
  # print(
  #         plotExpression(tmp.sce, 
  #                        features = rownames(tmp.sce[rowData(tmp.sce)$ENSEMBL %in% hvg[1:10], ]),
  #                        colour_by=dotcol,
  #                        shape_by=dotshape) +
  #           geom_boxplot(color = "darkgrey", alpha = 0.2, outlier.shape = NA) +
  #           ggtitle("Top 10 HGVs for all cells") +
  #           scale_color_hue(c=40, l=50, name= dotcol) +
  #           facet_grid(~colour_by)
  #       )


  # print violinplots summarized by gene
  hvg2plot <- t(logcounts(sce)[hvg[1:10],])
  hvg2plot <- data.frame(hvg2plot, colData(sce)[,annoFactors[1]])
  colnames(hvg2plot)[length(colnames(hvg2plot))] <- annoFactors[1]
  HVGplot <- lapply(hvg[1:10], function(h) {
        ggplot(hvg2plot, aes_string(annoFactors[1],h,color=annoFactors[1]))+
        geom_violin() +
        geom_quasirandom() +
        #scale_fill_hue(l=40, c=40) +
        geom_boxplot(color = "darkgrey", alpha = 0.2, outlier.shape = NA) +
        ylab("expression (logcounts)") +
        xlab(annoFactors[1]) +
        ggtitle(rowData(sce)$SYMBOL[match(h, rownames(sce))]) +
        theme(axis.text.x=element_text(angle=45, vjust=1, hjust=1), legend.position = "none")
  })
  maxViolPerRow = 10 # i.e. #genes x #groups
  nGridCol <- floor(maxViolPerRow/nlevels(factor(colData(sce)[,annoFactors[1]])))
  splitGridList <- split(HVGplot, ceiling(seq_along(HVGplot)/nGridCol))
  
  plot_splitGridList <- lapply(splitGridList, function(gl) { 
  gridExtra::grid.arrange(grobs=gl, ncol=nGridCol, 
               top=paste0("Top 10 HGVs for all cells (", parent.frame()$i[], "/", length(splitGridList), ")"))
  
  })

} 

```



# Using HVGs for further data exploration

## PCA based on significantly HVGs

```{r hvg_pca, results='asis', echo=F, error=F, warning=F, message=F, fig.height=5, fig.width=10}
##### parameters to set:
# define up to 2 categories to be indicated in the plot by color and shape.
annoFactors <- colorByFactor2 
ncomponents=50  # number of principal components to obtain  
exprs_values="logcounts"
#####

cat(paste0("\nPCA is calculated either based on the highly variable genes (HVGs) or on all genes (see suffix 'allgenes').\n"))

# HVG 
 set.seed(100)
  sce <- scater::runPCA(sce, name = "PCA", ncomponents=ncomponents, subset_row=hvg, exprs_values=exprs_values)

# all genes
 set.seed(100)
  sce <-scater::runPCA(sce, name = "PCA_allgenes", ncomponents=ncomponents, ntop = nrow(sce), exprs_values=exprs_values)
  
if(length(annoFactors)>2) {stop("\nno more than 2 categories allowed in 'annoFactors'.")}
  dotcol <- annoFactors[1]
  if(length(annoFactors)==2) { 
    dotshape <- annoFactors[2]} else {
      dotshape <- NULL}

# plot PCA
  gridExtra::grid.arrange(grobs=lapply(grep("PCA", reducedDimNames(sce), value=T), function(d) {

      scater::plotReducedDim(sce, dimred = d, 
        by_exprs_values = exprs_values,
        colour_by=dotcol, shape_by = dotshape, point_size=plot_pointsize) +  
        ggtitle(paste("PCA plot of", if(grepl("allgenes", d)) {"all genes"} else {"HVGs"}))

  }), ncol=2)

```

## Identification of sub-populations with t-SNE 

Another widely used approach is the t-stochastic neighbour embedding (t-SNE) method (Van der Maaten & Hinton, 2008). t-SNE  tends  to  work  better  than  PCA  for  separating  cells  in  more  diverse  populations.  This  is  because  the former can directly capture non-linear relationships in high-dimensional space, whereas the latter must represent them (suboptimally) as linear components. However, this improvement comes at the cost of more computational effort and complexity. In particular, t-SNE is a stochastic method, so users should run the algorithm several times to ensure that the results are representative, and then set a seed to ensure that the chosen results are reproducible. It is also advisable to test different settings of the perplexity parameter as this will affect the distribution of points in the low-dimensional space.

The perplexity can be interpreted as a smooth measure of the effective number of neighbors. The performance of SNE is fairly robust to changes in the perplexity, and typical values are between 5 and 50. 

A major weakness of t-SNE is that the cost function is not convex, as a result of which several optimization parameters need to be chosen. The constructed solutions depend on these choices of optimization parameters and may be different each time t-SNE is run from an initial random configuration of map points. But the developers of t-SNE have demonstrated that the same choice of optimization parameters can be used for a variety of different visualization tasks and found that the quality of the optima does not vary much from run to run. Thus, t-SNE should not be rejected in favor of methods that lead to convex optimization problems but produce noticeably worse visualizations. A local optimum of a cost function that accurately captures what is wanted in a visualization is often preferable to the global optimum of a cost function that fails to capture important aspects of what is wanted.

```{r TSNE_plot_various_perplexities, results='asis', fig.height=10, fig.width=10, echo=F, error=F, warning=F, message=F}
##### parameters to set:
# define up to 2 categories to be indicated in the plot by color and shape.
annoFactors <- colorByFactor2
ncomponents=2 # for tSNE, dims should be either 1, 2 or 3 
perplexity=c(30)
exprs_values="logcounts"
#####

cat(paste0("\nHere, t-SNE dimensions (with perplexity ", paste(paste0("p=",perplexity), collapse=", "), ") are calculated either based on the highly variable genes (HVGs) or on all genes (see suffix 'allgenes').\n\n"))

if(length(annoFactors)>2) {stop("\nno more than 2 categories allowed in 'annoFactors'.")}
  dotcol <- annoFactors[1]
  if(length(annoFactors)==2) { 
    dotshape <- annoFactors[2]} else {
      dotshape <- NULL}

  # t-SNE without using pre-existing PCA results as input
    for (p in perplexity) {
    set.seed(100) # hvg
    sce <- scater::runTSNE(sce, name = paste0("TSNE_p", p), ncomponents=ncomponents, 
                           perplexity=p, dimred = NULL, subset_row=hvg, exprs_values=exprs_values)
    set.seed(100) # all genes
    sce <- scater::runTSNE(sce, name = paste0("TSNE_p", p, "_allgenes"), ncomponents=ncomponents, 
                           perplexity=p, dimred = NULL, ntop = nrow(sce), exprs_values=exprs_values)
    }
 
```


```{r TSNE_plot_various_perplexities2, results='asis', fig.height=5*length(grep("TSNE", reducedDimNames(sce)))/2, fig.width=10, echo=F, error=F, warning=F, message=F}
  # plot TSNE
  # Since multiple perplexities can be selected resulting in multiple plots, 
  # the figure height must be set accordingly (5 inch x number of plots / ncol)
  gridExtra::grid.arrange(grobs=lapply(grep("TSNE", reducedDimNames(sce), value=T), function(d) {

      scater::plotReducedDim(sce, dimred = d, 
        by_exprs_values = exprs_values,
        colour_by=dotcol, shape_by = dotshape, point_size=plot_pointsize) +  
        ggtitle(paste("TSNE plot of", if(grepl("allgenes", d)) {"all genes"} else {"HVGs"}, "with perplexity", gsub("^.*p", "", d)))

  }), ncol=2)

```



## Identification of sub-populations with UMAP 

Uniform manifold approximation and projection (UMAP) is another nonlinear dimensionality reduction technique to identify sub-populations in expression data.

```{r UMAP_plot, results='asis', fig.height=5, fig.width=10, echo=F, error=F, warning=F, message=F}
##### parameters to set:
# define up to 2 categories to be indicated in the plot by color and shape.
annoFactors <- colorByFactor2
ncomponents=2  
n_neighbors = 15
exprs_values="logcounts"
#####

cat(paste0("\nUMAP dimensions (with ", paste(paste0("n_neighbors=",n_neighbors), collapse=", "), ") are calculated either based on the highly variable genes (HVGs) or on all genes (see suffix 'allgenes').\n\n"))


if(length(annoFactors)>2) {stop("\nno more than 2 categories allowed in 'annoFactors'.")}
  dotcol <- annoFactors[1]
  if(length(annoFactors)==2) { 
    dotshape <- annoFactors[2]} else {
      dotshape <- NULL}


# HVGs
 set.seed(100)
  sce <- scater::runUMAP(sce, name = "UMAP", ncomponents=ncomponents, n_neighbors = n_neighbors, 
                         dimred = NULL, subset_row=hvg, exprs_values=exprs_values)  
# all genes  
 set.seed(100)
  sce <- scater::runUMAP(sce, name = "UMAP_allgenes", ncomponents=ncomponents, n_neighbors = n_neighbors, 
                         dimred = NULL, ntop = nrow(sce), exprs_values=exprs_values)

 # plot UMAP 
   gridExtra::grid.arrange(grobs=lapply(grep("UMAP", reducedDimNames(sce), value=T), function(d) {

      scater::plotReducedDim(sce, dimred = d, 
        by_exprs_values = exprs_values,
        colour_by=dotcol, shape_by = dotshape, point_size=plot_pointsize) +  
        ggtitle(paste("UMAP plot of", if(grepl("allgenes", d)) {"all genes"} else {"HVGs"}))

  }), ncol=2)

```

# {-}

# Clustering {.tabset .tabset-pills}

## Defining clusters with SC3

Single-Cell Consensus Clustering (SC3) is a tool for unsupervised clustering of scRNA-seq data. SC3 achieves high accuracy and robustness by consistently integrating different clustering solutions through a consensus approach. Genes used for clustering are filtered based on gene dropout rates of the counts matrix. SC3 recommends a number of clusters to be used for clustering.

```{r sc3, results="asis", eval=TRUE}
library(SC3)
library(pkgmaker)

rowData(sce)$feature_symbol <- rowData(sce)$SYMBOL # column feature_symbol needed for SC3

set.seed(100)
sce <- sc3_prepare(sce, gene_filter=T, n_cores=8, rand_seed = 100) # 
set.seed(100)
sce <- sc3_estimate_k(sce) # metadata(sce)$sc3$k_estimation 12
nr_clusters <- metadata(sce)$sc3$k_estimation
cat("Number of selected clusters:", nr_clusters, "\n")
set.seed(100)
sce <- sc3_calc_dists(sce) # repeated runs not identical despite of setting rand_seed in sc3_prepare
set.seed(100)
sce <- sc3_calc_transfs(sce)
set.seed(100)
sce <- sc3_kmeans(sce, ks=nr_clusters) 
set.seed(100)
sce <- sc3_calc_consens(sce) # assigns clusters in colData(sce)

# metadata(sce)$sc3
# colData(sce)[ , grep("sc3_", colnames(colData(sce))), drop=F]   
# rowData(sce)[ , grep("sc3_", colnames(rowData(sce))), drop=F]

# print cluster assignment
kbl(t(table(colData(sce)[, paste0("sc3_", nr_clusters, "_clusters")])), align = "l", format="simple", caption="cells per cluster") %>% kable_styling(full_width = F) 

# store cluster assignments (csv file for compatibility with Loupe import)
dir_sc3 <- file.path("report_files", "cluster", "SC3")
if (!file.exists(dir_sc3)) {
  dir.create(dir_sc3, recursive=T) 
  }
write.table(data.frame(barcode=colnames(sce), colData(sce)[,c(group.vars, grep("sc3_", colnames(colData(sce)), value=T))]), 
            file =file.path(dir_sc3, paste0("SC3_", nr_clusters, "_cluster_assignments.csv")), sep=",", quote = F, row.names = F)


### plot SC3 clustering
annoFactors <- c(paste0("sc3_", nr_clusters, "_clusters"), colorByFactor2[1])  

if(length(annoFactors)>2) {stop("\nno more than 2 categories allowed in 'annoFactors'.")}
  dotcol <- annoFactors[1]
  if(length(annoFactors)==2) { 
    dotshape <- annoFactors[2]} else {
      dotshape <- NULL}

cat(paste0("\nClustering is applied on the gene list filtered by SC3 (", sum(rowData(sce)$sc3_gene_filter), " of ", nrow(sce), " genes were used for clustering). The obtained cluster assingment is illustrated with PCA, TSNE (with perplexity ", paste(paste0("p=",perplexity), collapse=", "), ") and UMAP plots (with n_neighbors=", paste(n_neighbors, collapse = ", "), "), which were calculated based on either the highly variable genes (HVGs) or on the SC3 filtered genes (see suffix 'filtgenes').\n\n"))

 sceSC3 <- sce # create temp object for dim reduction with SC3 filtered gene list.
 reducedDims(sceSC3)[grepl("allgenes", reducedDimNames(sceSC3))] <- NULL # replace "allgenes" with SC3 gene filtered list
 set.seed(100) # PCA
 sceSC3 <- scater::runPCA(sceSC3, name = "PCA_filtgenes", ncomponents=ncomponents, subset_row=rowData(sceSC3)$sc3_gene_filter, exprs_values=exprs_values)
 for(p in perplexity) { # TSNE
    set.seed(100)
    sceSC3 <- scater::runTSNE(sceSC3, name = paste0("TSNE_p", p, "_filtgenes"), ncomponents=ncomponents, 
                           perplexity=p, dimred = NULL, subset_row=rowData(sceSC3)$sc3_gene_filter, exprs_values=exprs_values)
  }
 set.seed(100) # UMAP
 sceSC3 <- scater::runUMAP(sceSC3, name = "UMAP_filtgenes", ncomponents=ncomponents, n_neighbors = n_neighbors, 
                         dimred = NULL, subset_row=rowData(sceSC3)$sc3_gene_filter, exprs_values=exprs_values)  


### plot clustering separated for PCA, TSNE and UMAP
  for (d in c("PCA", "TSNE", "UMAP")) {
        plot_Items <- grep(d, reducedDimNames(sceSC3), value=T) # use sceSC3 instead of sce
        
      # dim reduction plots
        plot_cluster <- mclapply(plot_Items, function(i) {
                  scater::plotReducedDim(sceSC3,  # use sceSC3 instead of sce
                    dimred = i,
                    colour_by=dotcol, shape_by = dotshape, point_size=plot_pointsize) +
                    scale_color_discrete(name="cluster") +
                    guides(color=guide_legend(ncol=2)) +
                    ggtitle(paste(gsub("_", " ", i), "SC3")) + theme(aspect.ratio = 1)
              }, mc.cores=1)
      
      filenamePlot <- file.path(dir_sc3, paste0("SC3_", d, "_cluster_plots.png")) 
      ggsave(filename=filenamePlot,
                      plot=gridExtra::grid.arrange(grobs=plot_cluster, ncol=2),
                      width = 200, height = 80*ceiling(length(plot_Items)/2),
                      units = c("mm"),  dpi = 300, device="png")
    
      # include plots in report    
      cat(paste0("![plot of ", basename(filenamePlot), "](", filenamePlot, ")"))
  }

 rm(sceSC3)
```


## Graph-based clustering with igraph

Graph-based clustering is a flexible and scalable technique for clustering large scRNA-seq datasets. We first build a graph where each node is a cell that is connected to its nearest neighbours in the high-dimensional space. Edges are weighted based on the similarity between the cells involved, with higher weight given to cells that are more closely related. We then apply algorithms to identify communities of cells that are more connected to cells in the same community than they are to cells of different communities. Each community represents a cluster that we can use for downstream interpretation. 

Graph construction avoids making strong assumptions about the shape of the clusters or the distribution of cells within each cluster, compared to other methods. k is the number of nearest neighbors used to construct the graph. This controls the resolution of the clustering where higher k yields a more inter-connected graph and broader clusters. Users can exploit this by experimenting with different values of k to obtain a satisfactory resolution.


```{r clustering_graphbased, results='asis'}
exprs_values = "logcounts"
data2clust <- c(exprs_values, "PCA") # use full expression matrix and/or reduced dimension matrix ("PCA", "TSNE", "UMAP") for clustering
dotshape <- colorByFactor2[1]

numberNearestNeighbors <- c(10, 20) # number of nearest neighbors k
typeOfWeighting <- "rank"  # type of weighting scheme to use for shared neighbors ("rank", "number", "jaccard")
# available clustering strategies in igraph
cluster_algo <- c("cluster_walktrap", "cluster_louvain", "cluster_infomap", "cluster_fast_greedy", "cluster_label_prop", "cluster_leading_eigen")[1:2]

cat(paste0("\nClustering is applied on ", 
    if(exprs_values %in% data2clust) {paste0("the expression matrix (", exprs_values, ")", 
    if(length(data2clust)>1) {" and "} else {".\n"})}, 
    if(any(c("PCA", "TSNE", "UMAP") %in% data2clust)) {paste0("the respective dimension reduction slots (", 
    if("PCA" %in% data2clust) {paste0("PCA", if(any(c("TSNE", "UMAP") %in% data2clust)) {", "})}, 
    if("TSNE" %in% data2clust) {paste0("TSNE (with perplexity ", paste(paste0("p=",perplexity), collapse=", "), ")", if("UMAP" %in% data2clust) {", "})}, 
    if("UMAP" %in% data2clust) {paste0("UMAP (with n_neighbors=", paste(n_neighbors, collapse = ", "), ")")}, 
      "), which were calculated either based on highly variable genes (HVGs) or on all genes (see suffix 'allgenes').\n")}))
cat("\nGraph-based clustering algorithms applied:", paste(cluster_algo, collapse = ", "), "\n")
cat("\nNumber of nearest neighbors k to consider during graph construction:", paste(numberNearestNeighbors, collapse = ", "), "\n")

sce_buildgraph <- list()
sce_igraphclust <- list()
clmod <- list() # clusterModularity
cluster.gr <- list() # cluster interaction graph
data2clustext <- c(exprs_values, unlist(lapply(data2clust, grep, reducedDimNames(sce), value=T))) # catch all corresponding red dim names 


dir_igraph <- file.path("report_files", "cluster", "igraph")
if (!file.exists(dir_igraph)) {dir.create(dir_igraph, recursive=T) }


for(d in data2clustext) {    
  
  for(k in numberNearestNeighbors) {
  
    set.seed(100)
    sce_buildgraph[[paste(d, paste0("k",k), sep="_")]] <- buildSNNGraph(sce, k=k, type=typeOfWeighting, use.dimred = if(d==exprs_values){NULL} else {d}) 
    # assay.type = exprs_values not used anymore in buildSNNGraph
    
    for (clu in cluster_algo) {
      
      clu <- gsub("^cluster_", "", clu)
      clusterfun <- switch(clu, 
                      "walktrap"=igraph::cluster_walktrap,
                      "louvain"= igraph::cluster_louvain,
                      "infomap"= igraph::cluster_infomap,
                      "fast_greedy"= igraph::cluster_fast_greedy,
                      "label_prop"= igraph::cluster_label_prop,
                      "leading_eigen"= igraph::cluster_leading_eigen)

      set.seed(100)
      sce_igraphclust[[paste(d, paste0("k",k), clu, sep="_")]] <- clusterfun(sce_buildgraph[[paste(d, paste0("k",k), sep="_")]])$membership
      colData(sce)[,paste("igraph", d, paste0("k",k), clu, sep="_")] <- factor(sce_igraphclust[[paste(d, paste0("k",k), clu, sep="_")]])
      
  # Assessing cluster separation
  set.seed(100)
  clmod[[paste(d, paste0("k",k), clu, sep="_")]] <- bluster::pairwiseModularity(sce_buildgraph[[paste(d, paste0("k",k), sep="_")]], sce_igraphclust[[paste(d, paste0("k",k), clu, sep="_")]], get.weights=F, as.ratio = TRUE)
  cluster.gr[[paste(d, paste0("k",k), clu, sep="_")]] <- igraph::graph_from_adjacency_matrix(clmod[[paste(d, paste0("k",k), clu, sep="_")]], mode="upper", weighted=TRUE, diag=FALSE)

    }
  }
}    


### plot graph-based clusterings (separated for PCA, TSNE and UMAP)
  plottypes <- c("PCA", "TSNE", "UMAP")
  for (d in plottypes) { # save one plot per plottype
      plot_Items <- grep(d, reducedDimNames(sce), value=T)
      plot_cluster <- list()
      for (p in plot_Items) { # reduced dims on HVGs and all genes 
        for (cl in names(sce_igraphclust)) { # cluster results to color
          # skip elements with clustering done on red dim on HVGs but illustrated on red dim allgenes and vice versa
          # to reduce number of plots and remove unnecessary ones
          if(any(sapply(plottypes, grepl, cl)) & grepl("_allgenes", cl) & !grepl("_allgenes", p)) {next}  
          if(any(sapply(plottypes, grepl, cl)) & !grepl("_allgenes", cl) & grepl("_allgenes", p)) {next} 
          # dim reduction plots
            plot_cluster[[paste(cl, p, sep="_")]] <- invisible(scater::plotReducedDim(sce, 
                        dimred = p,
                        by_exprs_values = exprs_values,
                        colour_by= paste0("igraph_", cl), 
                        shape_by = dotshape, point_size=plot_pointsize) +
              scale_color_discrete(name="cluster") +
              guides(color=guide_legend(ncol=2)) + theme(aspect.ratio = 1) +
              ggtitle(paste("Graph-based clustering on", gsub("_", " ", cl), "\nillustrated by", gsub("_", " ", p))) +
              theme(plot.title = element_text(size = 9)))
        } # end cl-loop
      } # end p-loop
      filenamePlot <- file.path(dir_igraph, paste0("igraph_", d, "_cluster_plots.png"))
      ggsave(filename=filenamePlot,
             plot=gridExtra::grid.arrange(grobs=plot_cluster, ncol=2), 
             width = 200, height = 80*ceiling(length(plot_cluster)/2), 
             units = c("mm"),  dpi = 300, device="png",
             limitsize = FALSE)
  
  # include plots in report    
  cat(paste0("![plot of ", basename(filenamePlot), "](", filenamePlot, ")"))
  } # end d-loop


# heatmaps
for (d in data2clust) {
 plot_Items <- grep(d, names(clmod), value=T)

   cl_heatmap <-  mclapply(plot_Items, function(i) {
                      invisible(pheatmap::pheatmap(log2(clmod[[i]]+1), cluster_rows=FALSE, cluster_cols=FALSE,
                          color=colorRampPalette(c("white", "blue"))(100), silent=T,
                          main= paste("cl heatmap", gsub("_", " ", i)))[["gtable"]])
               }, mc.cores=1)

   ggsave(filename=file.path(dir_igraph, paste0("igraph_", d, "_cluster_heatmaps.png")),
          plot=gridExtra::grid.arrange(grobs=cl_heatmap, ncol=2), 
          width = 200, height = 80*ceiling(length(plot_Items)/2), units = c("mm"),  dpi = 300, device="png",
          limitsize = FALSE)
}


# cluster graph plots
  for (d in data2clust) {
    plot_Items <- grep(d, names(cluster.gr), value=T)
    
      png(filename= file.path(dir_igraph, paste0("igraph_", d, "_cluster_graph.png")), units = "mm", res= 300,
          width = 200, height = 80*ceiling(length(plot_Items)/2))
       par(mfrow = c(ceiling(length(plot_Items)/2), 2))
      for (i in plot_Items) {
      set.seed(100)
      plot(cluster.gr[[i]], edge.width=igraph::E(cluster.gr[[i]])$weight*5, main=paste("cluster graph", gsub("_", " ", i)))
      }
       
  dev.off()
  par(mfrow = c(1, 1))
 }

```


```{r clustering_graphbased_assignment_heatmap, results='asis', fig.height=3+0.5*max(sapply(sce_igraphclust, function(x) {length(unique(x))}), na.rm=T), fig.width=12}

# prepare single table with cell numbers per cluster for each clustering setting to display as heatmap
igraph_clusterassign <- map_dfr(sce_igraphclust, table) %>%
  t()
colnames(igraph_clusterassign) <- names(sce_igraphclust)


# plot table as heatmap
cat("\nOverview of cluster assignments (number of cells per cluster; note that the cluster IDs of the different cluster method settings do not relate to each other):\n\n", fill=T)
Heatmap(igraph_clusterassign, cluster_rows = F, cluster_columns = F, name="#cells",
        #column_title="Overview cluster assignment for all applied clustering settings", column_title_side = "bottom",
        column_names_side="top", column_names_max_height = unit(8, "cm"), 
        row_names_side="left", row_title = "Cluster ID", row_title_gp = gpar(fontface = "bold"), row_names_gp =  gpar(fontface = "bold"),
        col= colorRamp2(c(0, max(igraph_clusterassign, na.rm = T)), c("white", "orange")), # rev(heat.colors(255)),
        column_order= order(apply(igraph_clusterassign, 2, function(x) length(na.omit(x))), decreasing=T),
        # heatmap_legend_param=list(title="#cells"),
        cell_fun = function(j, i, x, y, width, height, fill) {
          grid.text(replace(igraph_clusterassign[i, j], is.na(igraph_clusterassign[i, j]), ""), x, y) # , gp = gpar(fontsize = 10)
          }
        )

# store cluster assignments
  write.table(data.frame(barcode=colnames(sce), colData(sce)[,c(group.vars, grep("igraph", names(colData(sce)), value=T))]), 
  file = file.path(dir_igraph, paste0("igraph_cluster_assignments.csv")), sep=",", quote=F, row.names=F)

```



## Hierarchical clustering

Hierarchical clustering aims to generate a dendrogram containing a hierarchy of samples. This is most commonly done by greedily agglomerating samples into clusters, then agglomerating those clusters into larger clusters, and so on until all samples belong to a single cluster. Variants of hierarchical clustering methods primarily differ in how they choose to perform the agglomerations.

The dendrogram describes the relationships between cells and subpopulations at various resolutions and in a quantitative manner based on the branch lengths. Users can cut the tree at different heights to define clusters with different granularity, where clusters defined at high resolution are guaranteed to be nested within those defined at a lower resolution. If cells cannot be assigned to any cluster e.g. because there is no neighboring cluster exceeding the defined minimum cluster size, they are labeled as cluster 0. The dendrogram is also a natural representation of the data in situations where cells have descended from a relatively recent common ancestor. For larger datasets hierarchical clustering may be too slow.

```{r clustering_hierarchical, warning=F, results='asis'}
exprs_values <- "logcounts"
data2clust <- c(exprs_values, "PCA") # use full expression matrix and/or reduced dimension matrix ("PCA", "TSNE", "UMAP") for clustering
hclust_methods <- c("ward.D2") 
# methods for hierarchical clustering: one or combination of "ward.D", "ward.D2", "single", "complete", 
# "average" (= UPGMA), "mcquitty" (= WPGMA), "median" (= WPGMC) or "centroid" (= UPGMC)
minClusterSize <- c(20)
deepSplit <- c(1,2) 
dotshape <- colorByFactor2[1]

cat(paste0("\nClustering is applied on ", 
    if(exprs_values %in% data2clust) {paste0("the expression matrix (", exprs_values, ")", 
    if(length(data2clust)>1) {" and "} else {".\n"})}, 
    if(any(c("PCA", "TSNE", "UMAP") %in% data2clust)) {paste0("the respective dimension reduction slots (", 
    if("PCA" %in% data2clust) {paste0("PCA", if(any(c("TSNE", "UMAP") %in% data2clust)) {", "})}, 
    if("TSNE" %in% data2clust) {paste0("TSNE (with perplexity ", paste(paste0("p=",perplexity), collapse=", "), ")", if("UMAP" %in% data2clust) {", "})}, 
    if("UMAP" %in% data2clust) {paste0("UMAP (with n_neighbors=", paste(n_neighbors, collapse = ", "), ")")}, 
      "), which were calculated either based on highly variable genes (HVGs) or on all genes (see suffix 'allgenes').\n")}))
cat("\nHierarchical clustering algorithms applied:", paste(hclust_methods, collapse = ", "), "\n")
cat("\nMinimum cluster size cs:", paste(minClusterSize, collapse = ", "), "\n")
cat(paste0("\ndeepSplit ds: ", paste(deepSplit, collapse = ", "), "."), "This parameter provides control over sensitivity to cluster splitting. The higher the value, the more and smaller clusters will be produced.\n\n")

dist_sce <- list()
tree_sce <- list()
dend <- list()
clust_sce <- list()
data2clustext <- c(exprs_values, unlist(lapply(data2clust, grep, reducedDimNames(sce), value=T))) # catch all corresponding red dim names 

dir_hclust <- file.path("report_files", "cluster", "hclust")
if (!file.exists(dir_hclust)) {dir.create(dir_hclust, recursive=T) }

for(d in data2clustext) {    
  if(d %in% assayNames(sce)) {
    dist_sce[[d]] <- dist(assay(sce, d))
  } else {
    dist_sce[[d]] <- dist(reducedDim(sce, d))
  }

  for(h in hclust_methods) {
    set.seed(100)
    tree_sce[[paste(d, h, sep="_")]] <- hclust(dist_sce[[d]], method=h)
    
    # Making dendrogram.
    tree_sce[[paste(d, h, sep="_")]]$labels <- seq_along(tree_sce[[paste(d, h, sep="_")]]$labels)

    for (cs in minClusterSize) {
        for (ds in deepSplit) {
          
          dend[[paste(d, h, paste0("cs", cs), paste0("ds", ds), sep="_")]] <- as.dendrogram(tree_sce[[paste(d, h, sep="_")]], hang=0.1)
    
          # minClusterSize needs to be turned down for small datasets.
          # deepSplit controls the resolution of the partitioning.
          set.seed(100)
          clust_sce[[paste(d, h, paste0("cs", cs), paste0("ds", ds), sep="_")]] <- cutreeDynamic(tree_sce[[paste(d, h, sep="_")]], 
                                                             distM=as.matrix(dist_sce[[d]]),
                                                             method = "hybrid", minClusterSize=cs, deepSplit=ds, verbose = 0)
    
          labels_colors(dend[[paste(d, h, paste0("cs", cs), paste0("ds", ds), sep="_")]]) <- clust_sce[[paste(d, h, paste0("cs", cs), paste0("ds", ds), sep="_")]][order.dendrogram(dend[[paste(d, h, paste0("cs", cs), paste0("ds", ds), sep="_")]])]
    
          colData(sce)[,paste("hclust", d, h, paste0("cs", cs), paste0("ds", ds), sep="_")] <- factor(as.vector(clust_sce[[paste(d, h, paste0("cs", cs), paste0("ds", ds), sep="_")]])) 
          # as.vector removes vector names
        }  
     }
  }
}    


### plot clusterings (separated for PCA, TSNE and UMAP)
plottypes <- c("PCA", "TSNE", "UMAP")
  for (d in plottypes) {
    plot_Items <- grep(d, reducedDimNames(sce), value=T)
    plot_cluster <- list()
    for (p in plot_Items) { # reduced dims on HVGs and all genes 
      for (cl in names(clust_sce)) { # cluster results to color
      # skip elements with clustering done on red dim on HVGs but illustrated on red dim allgenes and vice versa
      # to reduce number of plots and remove unnecessary ones
      if(any(sapply(plottypes, grepl, cl)) & grepl("_allgenes", cl) & !grepl("_allgenes", p)) {next}  
      if(any(sapply(plottypes, grepl, cl)) & !grepl("_allgenes", cl) & grepl("_allgenes", p)) {next} 
   
      # dim reduction plots
        plot_cluster[[paste(cl, p, sep="_")]] <- invisible(scater::plotReducedDim(sce, 
                    dimred =p,
                    by_exprs_values = exprs_values,
                    colour_by=paste0("hclust_", cl), 
                    shape_by = dotshape, point_size=plot_pointsize) +
                    scale_color_discrete(name="cluster") +
                    guides(color=guide_legend(ncol=2)) +
                    ggtitle(paste("Hierarchical clustering on", gsub("_", " ", cl), "\nillustrated by", gsub("_", " ", p))) +
        theme(plot.title = element_text(size = 9)))

      } # end cl-loop
  } # end p-loop
      filenamePlot <- file.path(dir_hclust, paste0("hclust_", d, "_cluster_plots.png"))
      ggsave(filename=filenamePlot,
                      plot=gridExtra::grid.arrange(grobs=plot_cluster, ncol=2), 
                      width = 200, height = 80*ceiling(length(plot_cluster)/2), 
                      units = c("mm"),  dpi = 300, device="png",
         limitsize = FALSE)

    # include plots in report    
    cat(paste0("![plot of ", basename(filenamePlot), "](", filenamePlot, ")"))
}
  
## plot dendrograms
 for (d in data2clust) {
    plot_Items <- grep(d, names(clust_sce), value=T)
    
     png(filename= file.path(dir_hclust, paste0("hclust_", d, "_dendrograms.png")), units = "mm", res= 300,
          width = 200, height = 80*ceiling(length(plot_Items)/2))
      
      par(mfrow = c(ceiling(length(plot_Items)/2), 2))
      
      for (i in plot_Items) {
        plot(dend[[i]], main=paste("hclust dendrogram", gsub("_", " ", i)))
      }
      par(mfrow = c(1, 1))
      dev.off()
 }

```


```{r clustering_hierarchical_assignment_heatmap, results='asis', fig.height=3+0.5*max(sapply(clust_sce, function(x) {length(unique(x))}), na.rm=T), fig.width=12}

# prepare single table with cell numbers per cluster for each clustering setting to display as heatmap
hclust_clusterassign <- map_dfr(clust_sce, table) %>%
  t()  
colnames(hclust_clusterassign) <- names(clust_sce)


# plot table as heatmap
cat("\nOverview of cluster assignments (number of cells per cluster; note that the cluster IDs of the different cluster method settings do not relate to each other):\n\n", fill=T)
Heatmap(hclust_clusterassign, cluster_rows = F, cluster_columns = F, name="#cells",
        #column_title="Overview cluster assignment for all applied clustering settings", column_title_side = "bottom",
        column_names_side="top", column_names_max_height = unit(8, "cm"), 
        row_names_side="left", row_title = "Cluster ID", row_title_gp = gpar(fontface = "bold"), row_names_gp =  gpar(fontface = "bold"),
        col= colorRamp2(c(0, max(hclust_clusterassign, na.rm = T)), c("white", "orange")), # rev(heat.colors(255)),
        column_order= order(apply(hclust_clusterassign, 2, function(x) length(na.omit(x))), decreasing=T),
        # heatmap_legend_param=list(title="#cells"),
        cell_fun = function(j, i, x, y, width, height, fill) {
          grid.text(replace(hclust_clusterassign[i, j], is.na(hclust_clusterassign[i, j]), ""), x, y) # , gp = gpar(fontsize = 10)
          }
        )

# store cluster assignments
  write.table(data.frame(barcode=colnames(sce), colData(sce)[,c(group.vars, grep("hclust", names(colData(sce)), value=T))]), 
  file = file.path(dir_hclust, paste0("hclust_cluster_assignments.csv")), sep=",", quote=F, row.names=F)

```



## Clustering method selected for downstream processing

```{r select_clustering, results='asis'}
annoFactors <- colorByFactor2
######################
     
  selected_clustering_method <- "igraph_PCA_allgenes_k10_louvain"    
  name_dimred <- "PCA_allgenes"
  cat("Selected clustering:", selected_clustering_method, "\n\nillustrated by:", name_dimred, "\n\n")
  
  colLabels(sce) <- factor(colData(sce)[, selected_clustering_method])
  
  plot_selected <- scater::plotReducedDim(sce, 
                      dimred = name_dimred,
                      by_exprs_values = exprs_values,
                      colour_by=selected_clustering_method, point_size=plot_pointsize) + # shape_by = dotshape, 
                      scale_color_discrete(name="cluster") +
                      #guides(color=guide_legend(ncol=2)) +
                      #ggtitle(gsub("_", " ", selected_clustering_method)) + 
                      theme(legend.position = "top", aspect.ratio = 1)
   
  plot_selected_annofactor <- scater::plotReducedDim(sce, 
                      dimred = name_dimred,
                      by_exprs_values = exprs_values,
                      colour_by=annoFactors[1], point_size=plot_pointsize) +
                      #ggtitle(paste(gsub("_", " ", name_dimred), "colored by", annoFactors[1])) + 
                      theme(legend.position = "top", aspect.ratio = 1)
   
   ggsave(plot=plot_selected, filename= file.path("report_files", "cluster", paste0(selected_clustering_method, "_selected_cluster_plot.png")),
          width = 120, height = 120, units = c("mm"),  dpi = 300)

gridExtra::grid.arrange(plot_selected, plot_selected_annofactor, ncol=2, top=gsub("_", " ", selected_clustering_method))

cat("\n")

# store table with mean logcounts per cluster
meanLogcountsByCluster <- data.frame(cbind(cluster=colLabels(sce), t(logcounts(sce)))) %>%
      dplyr::group_by(cluster) %>%
      dplyr::summarize_all(mean, na.rm = T) %>%
      dplyr::ungroup() %>%
      column_to_rownames('cluster') %>%
      t()

write.table(data.frame(gene=rownames(meanLogcountsByCluster), symbol=rowData(sce)$SYMBOL, meanLogcountsByCluster, check.names = F), 
            file=file.path("report_files", "cluster", paste0(selected_clustering_method, "_mean_logcounts.txt")), 
            sep="\t", quote=F, row.names=F)

```




```{r plot_selected_genes_per_cluster, results='asis', eval=(!is.null(selgenes) && selgenes !="" && length(selgenes)>0)}
annoFactors <- colorByFactor2
######################

cat("## Expression plots of selected genes\n\n")

cat(paste0("\nGenes selected for expression plot: ", paste(selgenes, collapse=", "), ".\n"))
selgenes_absent <- selgenes[!selgenes %in% rowData(sce)$SYMBOL]
if(length(selgenes_absent)>0) {
  cat(paste0("\nGenes not in dataset: ", paste(selgenes_absent, collapse=", "), ".\n"))
  selgenes <- selgenes[selgenes %in% rowData(sce)$SYMBOL]
}
selgenes_ensembl <- rowData(sce)$ENSEMBL[match(selgenes, rowData(sce)$SYMBOL)]

  # print boxplots per cluser separated by group
  genesplot <- t(logcounts(sce)[selgenes_ensembl,])
  genesplot <- data.frame(genesplot, colData(sce)[,annoFactors[1]])
  colnames(genesplot)[length(colnames(genesplot))] <- annoFactors[1]
  genesplot <- data.frame(genesplot, colData(sce)[,selected_clustering_method])
  colnames(genesplot)[length(colnames(genesplot))] <- "cluster"
  genesplotlist <- lapply(selgenes_ensembl, function(h) {
        ggplot(genesplot, aes_string("cluster",h,color=annoFactors[1]))+
        geom_boxplot() +
        ylab("expression (logcounts)") +
        xlab("cluster") +
        theme(axis.title= element_text(size = 12), axis.text=element_text(size=8), 
              legend.title=element_text(size=10), legend.text=element_text(size=10)) +  
        theme(legend.position="top", aspect.ratio = 1)
  })
  
  # expression plot per gene
  plot_expression_list <- lapply(selgenes_ensembl, function(h) {
              scater::plotReducedDim(sce, 
              dimred = name_dimred,
              by_exprs_values = exprs_values,
              colour_by=h, point_size=plot_pointsize) + 
              guides(color=guide_legend(title=paste(rowData(sce)$SYMBOL[match(h, rownames(sce))], exprs_values))) +
              theme(legend.position="top", aspect.ratio = 1)
  })
  
  for (i in 1:length(selgenes)) { 
  gridExtra::grid.arrange(grobs=list(genesplotlist[[i]], plot_expression_list[[i]]), ncol=2, top=paste(selgenes[i], "expression"))
  }

  # # write table of un-filtered and un-normalized counts of selected gene list
  # selgenes_ensembl_unfilt <- na.omit(rowData(sce.beforeFilt)$ENSEMBL[match(c(selgenes_absent,selgenes), rowData(sce.beforeFilt)$SYMBOL)])
  # sel_unfilt_counts <- data.frame(SYMBOL=c(selgenes_absent,selgenes), ENSEMBL=selgenes_ensembl_unfilt, 
  #                                 counts(sce.beforeFilt)[selgenes_ensembl_unfilt,], check.names =F)
  # write.table(sel_unfilt_counts, file=file.path("report_files", "cluster", paste0("selected_genes_unfilt_raw_counts.txt")), 
  #           sep="\t", quote=F, row.names=F)

```



```{r doublet_detection_with_clusters, results='asis', eval=SHINYREPS_SEQTYPE %in% c("tenX")}
library(scDblFinder)
cat("# Doublet detection\n\n")

cat("\nThe methods applied here are complementary to doublets identified via cell hashes and SNPs in multiplexed samples: while hashing/genotypes can identify doublets formed by cells of the same type (homotypic doublets) from two samples loaded to the same GEM well, which are often nearly undistinguishable from real cells transcriptionally (and hence generally unidentifiable through the present package), it cannot identify doublets made by cells of the same sample, even if they are heterotypic (formed by different cell types). Instead, the methods presented here are primarily geared towards the identification of heterotypic doublets, which for most purposes are also the most critical ones.\n\n")

cat("## Doublet detection with clusters\n")

cat("\n\nThe findDoubletClusters function from the scDblFinder package identifies clusters with expression profiles lying between two other clusters (Bach et al. 2017, https://doi.org/10.1038/s41467-017-02001-5). We consider every possible triplet of clusters consisting of a query cluster and two putative source clusters. Under the null hypothesis that the query consists of doublets from the two sources, we compute the number of genes (num.de) that are differentially expressed in the same direction in the query cluster compared to both of the source clusters. Such genes would be unique markers for the query cluster and provide evidence against the null hypothesis (for a doublet we would expect an intermediate gene expression respective to the 2 source clusters). For each query cluster, the best pair of putative sources is identified based on the lowest num.de. 
    \nClusters are then ranked by num.de where those with the few unique genes are more likely to be composed of doublets. Further characteristics indicating potential doublet clusters comprise a reasonable proportion of cells in the cluster (given in 'prop') with respect to the experimental protocol. The function also reports the ratio of the median library size in each source to the median library size in the query (lib.size fields). For a cluster made of doublets, the library sizes of the source clusters should be below that of the query cluster, i.e. lib.size values below unity. This assumes that the doublet cluster will contain more RNA and have more counts than either of the two source clusters. The top gene with the lowest adjusted p-value against the doublet hypothesis is given in the 'best' column. The reported p.value is of little use in a statistical sense, and is only provided for inspection.\n")

# Technically, the p.value could be treated as the Simes combined p-value against the doublet hypothesis for the query cluster. However, this does not account for the multiple testing across all pairs of clusters for each chosen cluster, especially as we are choosing the pair that is most concordant with the doublet null hypothesis. 

dbl.out <- findDoubletClusters(sce, clusters=colData(sce)[,selected_clustering_method])
dbl.out <- DataFrame(cluster=rownames(dbl.out), dbl.out[,1:which(colnames(dbl.out)=="best")], 
                      symbol=rowData(sce)$SYMBOL[match(dbl.out$best, rownames(sce))],
                      dbl.out[,(1+which(colnames(dbl.out)=="best")):ncol(dbl.out)])
dbl.out[,c("p.value", "lib.size1", "lib.size2", "prop")] <- sapply(dbl.out[,c("p.value", "lib.size1", "lib.size2", "prop")], signif, digits=3)

kbl(dbl.out, caption="Doublet detection with clusters") %>% kable_styling()

```


```{r doublet_detection_by_simulation, results='asis', eval=SHINYREPS_SEQTYPE %in% c("tenX")}
annoFactors <- colorByFactor2
######################

cat("## Doublet detection by simulation\n")

cat("\nThis doublet detection strategy involves in silico simulation of doublets from the single-cell expression profiles (Dahlin et al. 2018, 
https://doi.org/10.1182/blood-2017-12-821413). This is performed using the computeDoubletDensity() function from the scDblFinder package. This function simulates doublets by adding the count vectors for two randomly chosen cells in the dataset. For each original cell, we compute the density of neighboring simulated doublets and compare it to the density of neighboring original cells. Genuine doublets should have a high density of simulated doublets relative to the density of its neighbourhood. Thus, the doublet score for each cell is defined as the ratio of densities of simulated doublets to the density of the original cells.\n")


library(BiocSingular)

set.seed(100)

# Setting up the parameters for consistency with denoisePCA();
# this can be changed depending on your feature selection scheme.
dbl.dens <- computeDoubletDensity(sce, subset.row=hvg, d=ncol(reducedDim(sce)))
#summary(dbl.dens)

sce$DoubletScore <- dbl.dens

TSNE_DoubletScore <- plotReducedDim(sce, dimred=name_dimred, colour_by="DoubletScore", shape_by = annoFactors)
TSNE_DoubletScore

plotCluster_DoubletScore <- plotColData(sce, x=selected_clustering_method, y="DoubletScore", colour_by = annoFactors, shape_by = annoFactors)
plotCluster_DoubletScore

cat("\n\nSimply removing cells with high doublet scores will not be sufficient to eliminate real doublets from the data set. In some cases, only a subset of the cells in the putative doublet cluster actually have high scores, and removing these would still leave enough cells in that cluster to mislead downstream analyses. In fact, even defining a threshold on the doublet score is difficult as the interpretation of the score is relative. There is no general definition for a fixed threshold above which libraries are to be considered doublets. We recommend interpreting the computeDoubletDensity() scores in the context of cluster annotation. All cells from a cluster with a large average doublet score should be considered suspect, and close neighbors of problematic clusters should also be treated with caution.\n")

```




# Detect marker genes for each cluster

Next we identify the genes that drive separation between clusters of the final clustering setting. These marker genes allow us to assign biological meaning to each cluster based on their functional annotation. In the most obvious case, the marker genes for each cluster are a priori associated with particular cell types, allowing us to treat the clustering as a proxy for cell type identity. Identification of marker genes is usually based around the retrospective detection of differential expression between clusters. Genes that are more strongly differentially expressed are more likely to have caused separate clustering of cells in the first place. 

Several different statistical tests are available to quantify the differences in expression profiles, and different approaches can be used to consolidate test results into a single ranking of genes for each cluster. We apply pairwise comparisons of clusters rather than comparing each cluster to the average of all other cells to avoid influence by population composition. Pairwise comparisons naturally provide more information to interpret the utility of a marker, e.g., by providing log-fold changes to indicate which clusters are distinguished by each gene. Here, we use the findMarkers function from the scran package to apply a one-sided t-test to identify genes that are up-regulated in each cluster compared to other clusters (test for log foldchange > 1). The idea of this function is to identify a combination of marker genes that - together - uniquely define one cluster against the rest. To this end, we collect the top DE genes from each pairwise comparison involving a particular cluster to assemble a set of candidate markers for that cluster. For each cluster, we obtain a list of ranked marker candidates. Genes with a rank of 1 comprise the genes with the lowest p-value from each comparison. These ranks are provided in the supplementary marker tables provided with this report (column 'top'). Additionally, these tables contain the log2-fold changes of expression for this cluster over each other cluster (as plotted in the heatmaps below), along with a combined p-value across the pairwise comparisons involving the respective cluster obtained by applying Simes method (Simes, R. J. 1986. An Improved Bonferroni Procedure for Multiple Tests of Significance. Biometrika 73 (3): 75154.). The summary.logFC column serves as summary of the direction and effect size for each gene and is defined as the log-fold change from the comparison with the lowest p-value.

In addition to that, we use a more stringent cluster specific approach. Instead of identifying genes up-regulated compared to any of the other clusters, we only consider genes that are up-regulated in all pairwise comparisons involving the cluster of interest. To achieve this, we use an intersection-union test where the combined p-value for each gene is the maximum of the p-values from all pairwise comparisons (apply setting pval.type="all"). A gene will only achieve a low combined p-value if it is strongly differentially expressed in all comparisons to other clusters.


```{r findmarkers_per_cluster, fig.width=11, fig.height=20, results='asis'}
DE_var2adjust <- "final_var2adjust" # it will be checked if DE_var2adjust exists in sce. Can also be NULL.
clustervar <- selected_clustering_method
groupingvar <- colorByFactor2 
assay.type <- if(adjust_ccp | !is.null(var2adjust)) {"logcounts_not_adjusted"} else {"logcounts"}
rank_threshold <- 5
###############################################


if(nlevels(factor(colData(sce)[,clustervar])) >=2) {

  dir_marker <- file.path("report_files", "marker_per_cluster", clustervar)
  if (!file.exists(dir_marker)) {dir.create(dir_marker, recursive=T) }
  
  cat("Un-adjusted logcounts are used for marker detection. ")
  if(is.null(DE_var2adjust) || !any(colnames(colData(sce)) %in% DE_var2adjust)) {
    cat("The analysis is not adjusted for any confounding variables.\n")
    DE_var2adjust <- NULL
  } else {
      if(DE_var2adjust == "final_var2adjust") {
        cat("The analysis is adjusted for the same factors as described in 'Adjust for confounder variables'.\n\n")
      } else {
        cat("The analysis is adjusted for:", DE_var2adjust, "\n\n")
      } 
  }
    
    
  # find upregulated markers for each cluster and upregulated marker specific for each cluster
  # using unadjusted logcounts including previously applied blocking variable
  # If DE_var2adjust is NULL no blocking applied 
  markers <- findMarkers(sce, groups=colData(sce)[,clustervar], 
                         block= if (is.null(DE_var2adjust)) {NULL} else {colData(sce)[,DE_var2adjust]},  
                         assay.type = assay.type,
                         row.data=rowData(sce)[,"SYMBOL", drop=F], direction="up", lfc=1)
  markers_spec <- findMarkers(sce, groups=colData(sce)[,clustervar], 
                         block= if (is.null(DE_var2adjust)) {NULL} else {colData(sce)[,DE_var2adjust]},  
                         assay.type = assay.type,
                         row.data=rowData(sce)[,"SYMBOL", drop=F], direction="up", pval.type="all")
                         # use pval.type="some" if "all" is too stringent 
  

cat("\nFor each cluster heatmap, we include all genes with an assigned rank <=", rank_threshold, ", i.e. the top", rank_threshold, "genes from each comparison. These genes distinguish that cluster from the other clusters. For the heatmaps showing genes which are up-regulated in a specific cluster only, we use include the same number of top genes as defined for the previous heatmap.\n\n")
   
   list_heatmaps <- NULL
   list_heatmaps_spec <- NULL
  
   marker_heatmaps <- lapply(names(markers), function(cl) { 
    
         # all upregulated markers per cluster
        write.table(markers[[cl]], sep="\t", quote=F, row.names=F, 
                    file=file.path(dir_marker, paste0("gene_marker_upregulated_in_cluster_", cl, ".txt")) )
        best_set <- markers[[cl]][markers[[cl]]$Top <= rank_threshold,]
        logFCs <- getMarkerEffects(best_set, prefix = "logFC")
        #logFCs <- as.matrix(best_set[,grepl("logFC", colnames(best_set))])
        #colnames(logFCs) <- sub("logFC.", "", colnames(logFCs))
        logFCs <- logFCs[, colSums(is.na(logFCs)) != nrow(logFCs)] # remove columns with only NA (cannot be plotted in heatmap)
        
        # markers upregulated in 1 cluster only   
        write.table(markers_spec[[cl]], sep="\t", quote=F, row.names=F, 
                    file=file.path(dir_marker, paste0("gene_marker_upregulated_in_cluster_", cl, "_only.txt")))
        best_set_spec <- markers_spec[[cl]][1:nrow(best_set), ] # select same number of genes displayed as in best_set
        logFCs_spec <- getMarkerEffects(best_set_spec, prefix = "logFC")
        #logFCs_spec <- as.matrix(best_set_spec[,grepl("logFC", colnames(best_set_spec))])
        #colnames(logFCs_spec) <- sub("logFC.", "", colnames(logFCs_spec))
        logFCs_spec <- logFCs_spec[, colSums(is.na(logFCs_spec)) != nrow(logFCs_spec)] # remove columns with only NA 
        
        if(length(names(markers)) >=3) { # create heatmaps if at least 3 groups, i.e. at least 2 hetmap columns.
            list_heatmaps <- pheatmap::pheatmap(logFCs, labels_row=best_set$SYMBOL, silent=T, breaks=seq(-5, 5, length.out=101), 
                                  main=paste("Top marker upregulated in cluster", cl))[["gtable"]]
  
            list_heatmaps_spec <- pheatmap::pheatmap(logFCs_spec, labels_row=best_set_spec$SYMBOL, silent=T, breaks=seq(-5, 5, length.out=101), 
                                       main=paste("Top marker upregulated in cluster", cl, "only"))[["gtable"]]
        
            filenamePlot <- file.path(dir_marker, paste0("heatmap_of_upregulated_gene_marker_cluster_", cl, ".png"))
            ggsave(filename=filenamePlot,
                   plot=gridExtra::grid.arrange(grobs=list(list_heatmaps, list_heatmaps_spec), ncol=2), 
                   width = 200, height = 150, units = c("mm"),  dpi = 300, device="png")


        # include plots in report    
        cat(paste0("![plot of ", basename(filenamePlot), "](", filenamePlot, ")"))
        cat("\n", fill = T)
    } else {
      cat("\nHeatmaps skipped because only 1 column to plot.\n")
    }
    
    return(list(all_up=list_heatmaps, cluster_spec=list_heatmaps_spec))
  })
  
} else {
  cat("\nNo marker genes to calculate because clustering variable has only 1 cluster\n")
}
```



```{r go_annotation, results='asis', message=F,  eval=(org %in% c("human", "mouse"))}
# for human and mouse only
cat("# GO annotation of maker genes per cluster\n")

cat("\nPerform a gene set enrichment analysis on the marker genes defining each cluster. This identifies the pathways and processes that are (relatively) active in each cluster based on upregulation of the associated genes compared to other clusters. We will use gene sets defined by the Gene Ontology (GO) project, which describe a comprehensive range of biological processes and functions. We define our subset of relevant marker genes at a FDR of 5% and apply the goana function from the limma package. This performs a hypergeometric test to identify GO terms that are overrepresented in our marker subset. We keep only biological process terms that are not overly general (<=200 genes) and which are significantly enriched (p<0.05).\n\n")
 
dir_anno_per_cluster <- file.path("report_files", "annotation_per_cluster", selected_clustering_method)
if (!file.exists(dir_anno_per_cluster)) {dir.create(dir_anno_per_cluster, recursive=T) }

  switch(org,
           human={
              library(org.Hs.eg.db)
              orgdb <- org.Hs.eg.db
              species="Hs"
           },
           mouse={
              library(org.Mm.eg.db)
              orgdb <- org.Mm.eg.db
              species="Mm"
           }
  )
    
anno.results <- list()

# Extract symbols for each GO term for individual lookup; done once.
    tab.go <- AnnotationDbi::select(orgdb, keytype="ENSEMBL", keys=gsub("\\..*$", "", rownames(sce)), columns="GOALL")
    by.go <- split(tab.go[,1], tab.go[,2])

for (i in levels(colData(sce)[,selected_clustering_method])) {
 
    cat("\nProcessing cluster", i, "\n")

    cur.markers <- markers[[i]]
    is.de <- cur.markers$FDR <= 0.05 
    # summary(is.de)
    
    if(org %in% c("human", "mouse")) {
      rownames(cur.markers) <- gsub("\\..*$", "", rownames(cur.markers))
      } # remove .version of Ensembl IDs

    entrez.ids <- mapIds(orgdb, keys=rownames(cur.markers), 
        column="ENTREZID", keytype="ENSEMBL")
    go.out <- goana(unique(entrez.ids[is.de]), species=species, 
        universe=unique(entrez.ids))
    
    # Only keeping biological process terms that are not overly general and which are significantly enriched.
    # P.DE: p-value for over-representation of the GO term in the set.
    go.out <- go.out[order(go.out$P.DE),]
    go.useful <- go.out[go.out$Ont=="BP" & go.out$N <= 200 & go.out$P.DE < 0.05,]
    
    print(kbl(go.useful[1:10,], align = "l", format="simple") %>% kable_styling(full_width = F))
    write.table(data.frame(GOID=rownames(go.useful), go.useful), 
                file =file.path(dir_anno_per_cluster, paste0("GOannotation_cluster_", i, ".txt")),
                quote = F, row.names = F,  sep="\t")
    
    anno.results[[i]] <- go.useful
    
    # Identify genes associated with an interesting term.
    # adhesion <- unique(by.go[["GO:0022408"]])
    # head(cur.markers[rownames(cur.markers) %in% adhesion,1:3], 10)
}
```



```{r cell_type_annotation, results='asis', message=F,  eval=(org %in% c("human"))}
# for human only
library(celldex)
library(SingleR)

cat("# Cell type annotation\n")

# Assigning cell labels from reference data (available for human only)
  cat("\nAssigning cell labels from built-in reference data constructed from Blueprint and ENCODE data (Martens and Stunnenberg 2013; The ENCODE Project Consortium 2012). This reference contains normalized expression values of 259 human bulk RNA-seq samples of pure stroma and immune cells. The Heatmap below shows the assignment score for each cell (column) and label (row). Scores are normalized to [0, 1] within each cell\n\n")
  logcountdata <- logcounts(sce)
  rownames(logcountdata) <- rowData(sce)$SYMBOL
  ref <- celldex::BlueprintEncodeData() # human bulk RNA-seq data from Blueprint and ENCODE
  pred <- SingleR(test=logcountdata, ref=ref, labels=ref$label.main) # alternative: ref$label.fine 
  plotScoreHeatmap(pred)

  #pred.cluster <- SingleR(test=logcountdata, ref=ref, labels=ref$label.main, clusters=colData(sce)[,selected_clustering_method])
  #plotScoreHeatmap(pred.cluster, show_colnames=T) # plot prediction per cluster instead per cell
  #table(pred$labels)
  #plotScoreDistribution(pred)
  
  cat("\n\nAssignments are compared with the clustering results to determine the identity of each cluster. The heatmap below shows the distribution of cells across labels and clusters in the dataset. Color scale is reported in the log10-number of cells for each cluster-label combination.\n\n")
  tab <- table(Assigned=pred$pruned.labels, Cluster=colData(sce)[,selected_clustering_method])
  # Adding a pseudo-count of 10 to avoid strong color jumps with just 1 cell.
  pheatmap::pheatmap(log2(tab+10), color=colorRampPalette(c("white", "blue"))(101))
  cat("\n")

```




```{r diff_expression, results='asis', eval=("replicate" %in% colnames(colData(sce)) && length(unique(sce$replicate)) >1)}
DE_var2adjust <- "final_var2adjust" # it will be checked if DE_var2adjust exists in sce. Can also be NULL.
varDiffGroups <- colorByFactor2[1]
########################################

library(edgeR)

cat("# Multi-sample comparisons

Differential analyses of replicated multi-condition scRNA-seq experiments can be broadly split into two categories: differential expression (DE) and differential abundance (DA) analyses. The former tests for changes in expression between conditions for cells of the same type that are present in both conditions, while the latter tests for changes in the composition of cell types (or states, etc.) between conditions.

## Differential expression between conditions using pseudo-bulk samples

Motivations behind the use of pseudo-bulking:

- Larger counts are more amenable to standard DE analysis pipelines designed for bulk RNA-seq data. Normalization is more straightforward and certain statistical approximations are more accurate for large counts.
- Collapsing cells into samples reflects the fact that our biological replication occurs at the sample level (Lun and Marioni 2017). Each sample is represented no more than once for each condition, avoiding problems from unmodelled correlations between samples. Supplying the per-cell counts directly to a DE analysis pipeline would imply that each cell is an independent biological replicate, which is not true from an experimental perspective. 
- Variance between cells within each sample is masked, provided it does not affect variance across (replicate) samples. This avoids penalizing DEGs that are not uniformly up- or down-regulated for all cells in all samples of one condition. Masking is generally desirable as DEGs - unlike marker genes - do not need to have low within-sample variance to be interesting, e.g., if the treatment effect is consistent across replicate populations but heterogeneous on a per-cell basis. 
    
The DE analysis will be performed using quasi-likelihood (QL) methods from the edgeR package (Robinson, McCarthy, and Smyth 2010; Chen, Lun, and Smyth 2016). This uses a negative binomial generalized linear model (NB GLM) to handle overdispersed count data in experiments with limited replication. We test for differences in expression using glmQLFTest(). DEGs are defined as those with non-zero log-fold changes at a false discovery rate of 5%.

As in bulk RNA-seq, we remove samples with very low library sizes as step of pre-processing. For the pseudo-bulk samples, this is equivalent to removing label-sample combinations that have very few (<5) cells. We also remove genes that are lowly expressed using the filterByExpr function from edgeR. This reduces computational work, improves the accuracy of mean-variance trend modelling and decreases the severity of the multiple testing correction.
\n\n")


dir_DE_per_cluster <- file.path("report_files", "DE_per_cluster", selected_clustering_method)
if (!file.exists(dir_DE_per_cluster)) {dir.create(dir_DE_per_cluster, recursive=T) }

sce$diffgroups <- colData(sce)[, varDiffGroups] 

cat("Un-adjusted and un-normalized counts are used for differential expression analysis. Normalization is done after pseudo-bulking cells per sample. ")
if(!DE_var2adjust %in% colnames(colData(sce))) {
  cat("The differential expression analysis is not adjusted for any confounding variables.\n")
  DE_var2adjust <- NULL
} else {
    sce$DE_var2adjust <- colData(sce)[,DE_var2adjust]
    if(DE_var2adjust == "final_var2adjust") {
      cat("The differential expression analysis is adjusted for the same factors as described in section 'Adjust for confounder variables'.\n")
    } else {
      cat("The differential expression analysis is adjusted for:", DE_var2adjust, "\n")
    } 
  }

# Creating pseudo-bulk samples from clusters (uses un-normalised "counts" matrix)
  vars4aggregation <- c("diffgroups", "replicate", selected_clustering_method, "DE_var2adjust")
  vars4aggregation <- vars4aggregation[vars4aggregation %in% names(colData(sce))] # in case there is no replicate variable
  summed <- aggregateAcrossCells(sce, id=colData(sce)[,vars4aggregation],
                use.assay.type = "counts") 
  
 de.results <- list()

for (i in unique(colData(summed)[,selected_clustering_method])) {
   
    cat("\n#### Processing cluster", i, "\n\n")
  
    current <- summed[,colData(summed)[,selected_clustering_method] == i]
    
    # Creating up a DGEList object for use in edgeR:
    y <- DGEList(counts(current), samples=colData(current), genes=rowData(current)$SYMBOL)
    
    # preprocessing for bulk samples (is done here for each cluster separately)
        # remove samples with very low library sizes
        # discarded <- isOutlier(y$samples$lib.size, log=TRUE, type="lower") # relative outlier detection 
        discarded <- current$ncells < 5
        y <- y[,!discarded]
        # remove genes that are lowly expressed
        keep <- filterByExpr(y, group=current$diffgroups, min.count = 5, min.total.count = 10, large.n = 3, min.prop = 0.5)
        y <- y[keep,]

    # print filter metrics 
     cat(paste0("\nFiltering samples for low cell number (<5) and genes for low expression.\n"))
     cat("\n(pseudo-bulk) samples discarded:", sum(discarded), "of", length(discarded), "\n")
     cat("\nGenes removed:", sum(!keep), "of", length(keep), "\n")

    # correct for composition biases by normalization factors (currently using unprocessed count data)
    y <- edgeR::calcNormFactors(y)
    
    # MDS plot for the pseudo-bulk profiles
    #limma::plotMDS(cpm(y, log=TRUE), col=y$samples$diffgroups, main=paste("MDS plot for cluster", i))
    
    # Statistical modeling
    design <- try(
        if(!is.null(DE_var2adjust)) { 
            model.matrix(~ factor(DE_var2adjust) + factor(diffgroups), y$samples)
            } else {
              model.matrix(~ factor(diffgroups), y$samples)
            },  
        silent=TRUE)
    
    if (is(design, "try-error") || 
        qr(design)$rank==nrow(design) ||
        qr(design)$rank < ncol(design) ||
        nrow(y$genes)==0) {
            # Skipping labels without contrasts or without 
            # enough residual d.f. to estimate the dispersion
            # or if all genes had been filtered out.
            cat("\ncluster", i, "skipped\n")
            next
        }

    # We estimate the negative binomial (NB) dispersions with estimateDisp(). 
    y <- estimateDisp(y, design)
    #summary(y$trended.dispersion)
    #plotBCV(y)
    
    # We also estimate the quasi-likelihood dispersions with glmQLFit() (Chen, Lun, and Smyth 2016). 
    # This fits a GLM to the counts for each gene and estimates the QL dispersion from the GLM deviance. 
    # We set robust=TRUE to avoid distortions from highly variable clusters (Phipson et al. 2016). 
    # The QL dispersion models the uncertainty and variability of the per-gene variance (Figure 14.3),
    # which is not well handled by the NB dispersions, so the two dispersion types complement each other 
    # in the final analysis.
    fit <- glmQLFit(y, design, robust=TRUE)
    #summary(fit$var.prior)
    #summary(fit$df.prior)
    #plotQLDisp(fit)
    
    # We test for differences in expression using glmQLFTest(). DEGs are defined as those with non-zero 
    # log-fold changes at a false discovery rate of 5%.
    
    cat("\nDifferential expression results for", varDiffGroups, "in cluster", i, "\n")
    res <- glmQLFTest(fit, coef=ncol(design))
    #summary(decideTests(res))
    print(kbl(as.data.frame(topTags(res)), align = "l", format="simple") %>% kable_styling(full_width = F)) 
    write.table(data.frame(EnsemblID=rownames(topTags(res, n=nrow(res))), topTags(res, n=nrow(res))), 
                file =file.path(dir_DE_per_cluster, paste0("DEgenes_cluster_", i, ".txt")),
                quote = F, row.names = F,  sep="\t")
    de.results[[i]] <- res
    names(de.results)[[i]] <- i
}    

cat("\n#### Summary for all clusters\n\n")    
de.results <- de.results[lengths(de.results) != 0] # remove skipped clusters
summaries <- lapply(de.results, FUN=function(x) {summary(decideTests(x))[,1]})
sum.tab <- do.call(rbind, summaries)
sum.tab <- data.frame(cluster=rownames(sum.tab), sum.tab)
print(kable(sum.tab, align = "l", row.names=F, format="simple") %>% kable_styling(full_width = F))

```





```{r store_data, echo=F, results='hide', error=F, warning=F, message=F, eval=T}

# store count data (all and by pool)
dir_countdata <- file.path("report_files", "count_data")

cat("count data matrices are stored in", dir_countdata, "\n")

countslist <- lapply(names(assays(sce)), function(x) {

  if (!file.exists(file.path(dir_countdata, x))) {dir.create(file.path(dir_countdata, x), recursive = T)}

  table2write <- data.frame(gene=rownames(sce), symbol=rowData(sce)$SYMBOL, assay(sce, x))
  #table2write <- rbind(cluster=c("cluster", " ", colLabels(sce)), table2write)
  write.table(table2write, file =file.path(dir_countdata, x, paste0(x, ".txt")), sep="\t", quote = F, row.names = F)

  if(!gtools::invalid(colorByFactor) && length(unique(colData(sce)[,colorByFactor])) >1) {
      lapply(unique(colData(sce)[,colorByFactor]), function(y) {
        sce.subset <- sce[, colData(sce)[,colorByFactor] == y]
        write.table(data.frame(gene=rownames(sce.subset), symbol=rowData(sce.subset)$SYMBOL, assay(sce.subset, x)), 
                    file = file.path(dir_countdata, x, paste0(x, "_", y, ".txt")), sep="\t", quote = F, row.names = F)
      })
  }
})

# store projections (csv file for compatibility with Loupe import)
dir_projections <- file.path("report_files", "projections")
if (!file.exists(dir_projections)) {dir.create(dir_projections, recursive = T)}

for (p in (reducedDimNames(sce))) {
  projection <- data.frame(
    Barcode=rownames(reducedDim(sce)),
    X=reducedDim(sce, p)[,1],
    Y=reducedDim(sce, p)[,2]
  )
  write.table(projection, file =file.path(dir_projections, paste0(p, ".csv")), sep=",", quote = F, row.names = F)
  }

# save workspace
save.image("report_files/WS.RData")

# create files for shiny app 
library(shinydashboard)
library(iSEE)

dir_shinyapp <- file.path("report_files/shinyapp")
if (!file.exists(dir_shinyapp)) {dir.create(dir_shinyapp) }
  app <- iSEE(sce)
  top.hvg <- hvg
  genes <- gene.names[gene.names$gene_id %in% rownames(sce), ]
  save(sce, genes, top.hvg, file=file.path(dir_shinyapp, "data4shiny.RData"))
  save(sce, file=file.path(dir_shinyapp, "sce.RData"))
  save(app, file=file.path(dir_shinyapp, "app.R"))
   
```

# Used tools and versions for this analysis

Read mapping was performed with STAR using the following parameters:

```{r STAR_parameters_paragraph, echo=F, results='asis', error=F, warning=F, message=F}
cat(DEhelper.STARparms(), sep="\n")
``` 

The following tools were used for data processing:

```{r ToolVersions_paragraph, echo=F, results='asis', error=F, warning=F, message=F}
cat(Toolhelper.ToolVersions(), sep="\n")
```

R session info:

```{r R_sessionInfo, echo=F, results='asis', error=F, warning=F, message=F}
sessionInfo()
```

</div>

